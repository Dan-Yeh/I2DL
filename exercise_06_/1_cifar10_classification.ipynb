{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cifar10 Classification\n",
    "\n",
    "Until now, we have implemented several pieces of a deep learning pipeline and even trained a two-layer neural network, but all the hyperparameters were already set to some values yielding reasonable results. In real-life problems, however, much of the work in a deep learning project will be geared towards finding the best hyperparameters for a certain problem. In this notebook we will explore some good practices for network debugging and hyperparameters search, as well as extending our previous binary classification neural network to a multi-class one.\n",
    "\n",
    "Let's go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some lengthy setup.\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from exercise_code.networks.layer import (\n",
    "    Sigmoid, \n",
    "    Relu, \n",
    "    LeakyRelu, \n",
    "    Tanh,\n",
    ")\n",
    "from exercise_code.data import (\n",
    "    DataLoader,\n",
    "    ImageFolderDataset,\n",
    "    RescaleTransform,\n",
    "    NormalizeTransform,\n",
    "    FlattenTransform,\n",
    "    ComposeTransform    \n",
    ")\n",
    "from exercise_code.data.image_folder_dataset import RandomHorizontalFlip\n",
    "from exercise_code.networks import (\n",
    "    ClassificationNet,\n",
    "    BCE,\n",
    "    CrossEntropyFromLogits\n",
    ")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Quick recap (and some new things)\n",
    "\n",
    "Until now, in the previous exercises, we focused on building and understanding all the necessary modules for training a simple model. We followed the Pytorch implementations closely, as this is the framework we will use later and we want you to have a smoother transition to its APIs. \n",
    "\n",
    "In the figure below you can see the main components in Pytorch. Before starting the actual exercise, we begin with a quick recap of **our implementation** of these components. \n",
    "\n",
    "Everything is already implemented for this part, but we **strongly** encourage you to check out the respective source files in order to have a better understanding.\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/800/1*uZrS4KjAuSJQIJPgOiaJUg.png\" style=\"width: 500px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Dataset and Dataloader\n",
    "\n",
    "Data preparation plays an important role in deep learning projects. The data comes from different sources and in different formats and is prepared differently from application to application. One part, however, is clear: because entire datasets are usually too large for us to handle at once, we train our models on smaller batches of data. \n",
    "\n",
    "The goal of the ```Dataset``` class is to encapsulate all the 'dirty' data processing: loading and cleaning the data, storing features (or names of files where features can be found) and labels, as well as providing the means for accessing individual (transformed) items of the data using the ```__getitem__()``` function and an index. You already implemented an ```ImageFolderDataset``` (in ```exercise_code/data/image_folder_dataset.py```) class in Exercise 3. We we will reuse this class here.\n",
    "\n",
    "For processing the data, you implemented several transforms in Exercise 3 (```RescaleTransform```, ```NormalizeTransform```, ```ComposeTransform```). In this exercise we are working with images, which are multidimensional arrays, but we are using simple feedforward neural network which takes a one dimensional array as an input, so it is necessary to reshape the images before feeding them into the model. We implemented this reshape operation for you in the ```FlattenTransform``` class, also found in ```exercise_code/data/image_folder_dataset.py```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_url = \"https://cdn3.vision.in.tum.de/~dl4cv/cifar10.zip\"\n",
    "i2dl_exercises_path = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "cifar_root = os.path.join(i2dl_exercises_path, \"datasets\", \"cifar10\")\n",
    "\n",
    "# Use the Cifar10 mean and standard deviation computed in Exercise 3.\n",
    "cifar_mean = np.array([0.49191375, 0.48235852, 0.44673872])\n",
    "cifar_std  = np.array([0.24706447, 0.24346213, 0.26147554])\n",
    "\n",
    "# Define all the transforms we will apply on the images when \n",
    "# retrieving them.\n",
    "rescale_transform = RescaleTransform()\n",
    "normalize_transform = NormalizeTransform(\n",
    "    mean=cifar_mean,\n",
    "    std=cifar_std\n",
    ")\n",
    "flatten_transform = FlattenTransform()\n",
    "compose_transform = ComposeTransform([rescale_transform, \n",
    "                                      normalize_transform,\n",
    "                                      flatten_transform])\n",
    "\n",
    "# Create a train, validation and test dataset.\n",
    "datasets = {}\n",
    "for mode in ['train', 'val', 'test']:\n",
    "    crt_dataset = ImageFolderDataset(\n",
    "        mode=mode,\n",
    "        root=cifar_root, \n",
    "        download_url=download_url,\n",
    "        transform=compose_transform,\n",
    "        split={'train': 0.6, 'val': 0.2, 'test': 0.2}\n",
    "    )\n",
    "    datasets[mode] = crt_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, based on this ```Dataset``` object, we can construct a ```Dataloader``` object which samples a random mini-batch of data at once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataloader for each split.\n",
    "dataloaders = {}\n",
    "for mode in ['train', 'val', 'test']:\n",
    "    crt_dataloader = DataLoader(\n",
    "        dataset=datasets[mode],\n",
    "        batch_size=256,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    dataloaders[mode] = crt_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the ```Dataloader``` has the ```__iter__()``` method, we can simply iterate through the batches it produces, like this:\n",
    "\n",
    "```python\n",
    "for batch in dataloader['train']:\n",
    "    do_something(batch)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Data Augmentation\n",
    "\n",
    "After the above preprocessing steps, our data is in a good shape and ready to be fed into our network. As explained in the chapter above, we used the transformation functions `RescaleTransform`, `NormalizeTransform` and `FlattenTransform` to achieve this shape. These are the general steps that you need to perform on the data before we can even start the training. Of course, all these steps have to be applied to all three splits of our dataset (train, val and test split). So in other words, preprocessing involves preparing the data before they are used in training and inference. \n",
    "\n",
    "Besides these basic transformations, there are many other transformation methods that you can apply to the images. For example, you can <b>flip the images horizontally</b> or <b>blur the image</b> and use these new images to enlarge your dataset. This idea is called Data Augmentation and it involves methods that alter the training images to generate a synthetic dataset that is larger than your original dataset and will hopefully improve the performance of your model. The purpose here is different than in the data preprocessing steps and there is one big difference between data augmentation and data preprocessing: The transformation methods to enlarge your dataset should only be applied to the training data. The validation and test data are not affected by these methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h3>Task: Check Code</h3>\n",
    "    <p>The choice of transformation methods to use for data augmentation can be seen as a hyperparameter of your model and you can try to include these to enlarge your training data and obtain better results for your model. In <code>exercise_code/data/image_folder_dataset.py</code> we implemented the function <code>RandomHorizontalFlip</code> for you, which is randomly flipping an image. Check out the implementation.</p>\n",
    "    <p> Later, we will apply some hyperparameter tuning and in order to improve your model's accuracy, you could try to include some data augmentation methods. Fell free to play around and maybe also implement some other methods as for example Gaussian Blur or Rotation. </p>       \n",
    "</div>\n",
    "\n",
    "Let us quickly check out the `RandomHorizontalFlip` method with an image of the Cifar10 dataset in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x144 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Load the data in a dataset without any transformation \n",
    "dataset = ImageFolderDataset(\n",
    "        mode=mode,\n",
    "        root=cifar_root, \n",
    "        download_url=download_url,\n",
    "        split={'train': 0.6, 'val': 0.2, 'test': 0.2},\n",
    "    )\n",
    "\n",
    "#Retrieve an image from the dataset and flip it\n",
    "image = dataset[1]['image']\n",
    "transform = RandomHorizontalFlip(1)\n",
    "image_flipped = transform(image)\n",
    "\n",
    "#Show the two images\n",
    "plt.figure(figsize = (2,2))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(image.astype('uint8'))\n",
    "plt.axis('off')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(image_flipped.astype('uint8'))\n",
    "plt.axis('off')\n",
    "plt.title(\"Left: Original Image, Right: Flipped image\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Layers\n",
    "\n",
    "Now, that the data is prepared, we can discuss the model in which we are feeding the data. In our case the model will be a neural network. \n",
    "\n",
    "In Exercise 5, you implemented a simple 2-layer neural network that had a hidden size as a parameter:\n",
    "\n",
    "$$ \n",
    "{\\hat{y}} = \\sigma(\\sigma({x W_1} + {b_1}) {W_2} + {b_2}) \n",
    "$$\n",
    "\n",
    "where $ \\sigma({x}) $ was the sigmoid function, $ {x} $ was the input, $ {W_1}, {W_2} $ the weight matrices and $ {b_1}, {b_2}$ the biases for the two layers.\n",
    "\n",
    "This is how we used this network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = datasets['train'][0]['image'].shape[0]\n",
    "model = ClassificationNet(input_size=input_size, \n",
    "                          hidden_size=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we updated the ```ClassificationNet``` from the previous exercise, so now you can customize more: the number of outputs, the choice of activation function, the hidden size etc. We encourage you to check out the implementation in ```exercise_code/networks/classification_net.py``` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layer = 2\n",
    "reg = 0.1\n",
    "\n",
    "model = ClassificationNet(activation=Sigmoid(), \n",
    "                          num_layer=num_layer, \n",
    "                          reg=reg,\n",
    "                          num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the forward and backward passes through the model were simply:\n",
    "\n",
    "```python\n",
    "\n",
    "# X is a batch of training features \n",
    "# X.shape = (batch_size, features_size)\n",
    "y_out = model.forward(X)\n",
    "\n",
    "# dout is the gradient of the loss function w.r.t the output of the network.\n",
    "# dout.shape = (batch_size, )\n",
    "model.backward(dout)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as the learning rate or the number of iterations we want to train for, the number of hidden layers and the number of units in each hidden layer are also hyperparameters. In this notebook you will play with networks of different sizes and will see the impact that the network capacity has.\n",
    "\n",
    "Before we move on to the loss functions, we want to have a look at the activation functions. The choice of an activation function can have a huge impact on the performance of the network that you are designing. So far, you have implemented the `Sigmoid` and the `Relu` activation function in Exercise 5. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h3>Task: Check Code</h3>\n",
    "    <p>Take a look at the <code>Sigmoid</code> and the <code>Relu</code> class in <code>exercise_code/networks/layer.py</code> and the implementaion of the respective forward and backward pass. Make sure to understand why we use <b>elementwise product</b> instead of dot product in the backward pass of the <code>Sigmoid</code> class to compute the gradient $dx$. That will be helpful for your later implementation of other activation functions.</p>\n",
    "    <p> <b>Note:</b> The <code>cache</code> variable is used to store information from the forward pass and then pass this information in the backward pass to make use of it there. The implementation of both classes show that this variable can be used differently - depending on what information is needed in the backward pass. </p>\n",
    "        \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to have a look at two other, very common activation functions that you have already met in the lecture: Leaky ReLU activation function and Tanh activation function. \n",
    "\n",
    "**Leaky Relus** are one attempt to fix the “dying ReLU” problem. Instead of the function being zero when $x<0$, a leaky ReLU has a small negative slope (for example, 0.01). That is, the function computes $f(x) = \\mathbb{1}(x < 0) (\\alpha x) + \\mathbb{1}(x>=0) (x)$ where $\\alpha$ is a small constant. Some people report success with this form of activation function, but the results are not always consistent.\n",
    "\n",
    "The **tanh non-linearity** squashes a real-valued number to the range [-1, 1]. Like the sigmoid neuron, its activations saturate, but unlike the sigmoid neuron its output is zero-centered. Therefore, in practice the tanh non-linearity is always preferred to the sigmoid non-linearity. Also note that the tanh neuron is simply a scaled sigmoid neuron, in particular the following holds: $\\tanh(x) = 2 \\cdot \\sigma(2x) -1$.\n",
    "\n",
    "<img class=left src=https://pytorch.org/docs/stable/_images/LeakyReLU.png alt=\"Figure3\" width=\"350\" align='left'/> \n",
    "<img class=right src=https://pytorch.org/docs/stable/_images/Tanh.png alt=\"Figure4\" width=\"350\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task: Implement Activation Layers</h3>\n",
    "    <p> Now, it is your turn to implement the <code>LeakyRelu</code> and the <code>Tanh</code> class in <code>exercise_code/networks/layer.py</code> by completing the <code>forward</code> and the <code>backward</code> functions. You can test your implementation in the following two cells. </p>\n",
    "    <p> <b>Note:</b> Always remember to return a cache in <code>forward</code> for later backpropagation in <code>backward</code>. As we have seen above, the <code>cache</code> variable can be used differently for two activation functions.</p>\n",
    "</div>\n",
    "\n",
    "Use this cell to test your implementation of the `LeakyRelu` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeakyReluForwardTest passed.\n",
      "LeakyReluBackwardTest passed.\n",
      "Congratulations you have passed all the unit tests!!! Tests passed: 2/2\n",
      "Score: 100/100\n",
      "You secured a score of :100\n"
     ]
    }
   ],
   "source": [
    "from exercise_code.tests.layer_tests import *\n",
    "print(LeakyReluTestWrapper()())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this cell to test your implementation of the `Tanh` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TanhForwardTest passed.\n",
      "TanhBackwardTest passed.\n",
      "Congratulations you have passed all the unit tests!!! Tests passed: 2/2\n",
      "Score: 100/100\n",
      "You secured a score of :100\n"
     ]
    }
   ],
   "source": [
    "print(TanhTestWrapper()())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations, you now implemented all four different activation functions! These activation layers are now ready to be used when you start building your own network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Loss\n",
    "\n",
    "In order to measure how well a network is performing, we implemented several ```Loss``` classes (```L1```, ```MSE```, ```BCE```, each preferred for a certain type of problems) in ```exercise_code/networks/loss.py```.\n",
    "\n",
    "Each implemented a ```forward()``` method, which outputs a number that we use as a proxy for our network performance. \n",
    "\n",
    "Also, because our goal was to change the weights of the network such that this loss measure decreases, we were also interested in the gradients of the loss w.r.t the outputs of the network, $ \\nabla_{\\hat{y}} L({\\hat{y}}, {y}) $. This was implemented in ```backward()```. \n",
    "\n",
    "In previous exercises we worked with binary classification and used binary cross entropy (```BCE```) as a loss function.\n",
    "\n",
    "$$ BCE(\\hat{y}, y) = \\frac{1}{N} \\sum_{i=1}^N \\Big [-y_i \\log(\\hat{y_i}) - (1-y_i) \\log(1 - \\hat{y_i}) \\Big] $$ \n",
    "\n",
    "where\n",
    "- $ N $ was the number of samples we were considering\n",
    "- $\\hat{y}_i$ was the network's prediction for sample $i$. Note that this was a valid probability $\\in [0, 1]$, because we applied a [sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function) activation on the last layer\n",
    "- $ y_i $ was the ground truth label (0 or 1, depending on the class)\n",
    "\n",
    "Because we have 10 classes in the CIFAR10 dataset, we need a generalization of the binary cross entropy for multiple classes. Remember that this loss has been introduced last week in Exercise 5. It is called the cross entropy loss and has the following definition:\n",
    "\n",
    "$$ CE(\\hat{y}, y) = \\frac{1}{N} \\sum_{i=1}^N \\sum_{k=1}^{C} \\Big[ -y_{ik} \\log(\\hat{y}_{ik}) \\Big] $$\n",
    "\n",
    "where:\n",
    "- $ N $ is again the number of samples\n",
    "- $ C $ is the number of classes\n",
    "- $ \\hat{y}_{ik} $ is the probability that the model assigns for the $k$-th class when the $i$-th sample is the input. **Because we don't apply any activation function on the last layer of our network, its outputs for each sample will not be a valid probability distribution over the classes. We call these raw outputs of the network '[logits](https://datascience.stackexchange.com/questions/31041/what-does-logits-in-machine-learning-mean/31045)' and we will apply a [softmax](https://en.wikipedia.org/wiki/Softmax_function) activation in order to obtain a valid probability distribution.** \n",
    "- $y_{ik} = 1 $ iff the true label of the $i$-th sample is $k$ and 0 otherwise. This is called a [one-hot encoding](https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as with the previous losses we have seen, we can simply get the results of the forward and backward passes as follows:\n",
    "\n",
    "```python\n",
    "# y_out is the output of the neural network\n",
    "# y_truth is the actual label from the dataset\n",
    "loss.forward(y_out, y_truth)\n",
    "loss.backward(y_out, y_truth)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Optimizer\n",
    "\n",
    "Now, knowing the gradient of the loss w.r.t the ouputs of the network, as well as the local gradient for each layer of the network, we can use the chain rule to compute all gradients. \n",
    "\n",
    "We implemented several optimizer classes (```SGD```, ```Adam```, ```sgd_momentum```, which you can check out in ```exercise_code/networks/optimizer.py```) that implement different first-order parameter update rules. The ```step()``` method iterates through all the parameters of a model and updates them using the gradient information.\n",
    "\n",
    "What the optimizer is doing, in pseudocode, is the following:\n",
    "\n",
    "```python\n",
    "for param in model:\n",
    "    # Use the gradient to update the weights.\n",
    "    update(param)\n",
    "    \n",
    "    # Reset the gradient after each update.\n",
    "    param.gradient = 0\n",
    "```\n",
    "\n",
    "```SGD``` had the simplest update rule:\n",
    "```python\n",
    "def update(param):\n",
    "    param = param - learning_rate * param.gradient\n",
    "```\n",
    "\n",
    "For the more complicated update rules, see ```exercise_code/networks/optimizer.py```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Solver\n",
    "\n",
    "The ```Solver``` is where all the above elements come together: Given a train and a validation dataloader, a model, a loss and an optimizer, it uses the training data to optimize a model in order to get better predictions. We simply call ```train()``` and it does its 'magic' for us!\n",
    "```python\n",
    "solver = Solver(model, \n",
    "                dataloaders['train'], \n",
    "                dataloaders['val'], \n",
    "                learning_rate=0.001, \n",
    "                loss_func=MSE(), \n",
    "                optimizer=SGD)\n",
    "\n",
    "solver.train(epochs=epochs)\n",
    "```\n",
    "\n",
    "In order to see, that there is no actual 'magic' check out its implementation in ```exercise_code/solver.py```. :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Weight Regularization\n",
    "\n",
    "Before we finish this section of recap, we want to take a look at some regularization method that has been introduced in the lecture and that is super helpful to improve robustness of our model. Here, we talk about weight regularization.\n",
    "\n",
    "Weight regularization has been introduced to you as a method preventing our model from overfitting. Essentially, it is a term (solely depending on the weights of our model) that is added to the final loss and that encodes some preference for a certain set of weights $W$ over others. In the lecture, we compared two weight regularization methods and their respective preference for weight vectors. We made the following observation: \n",
    "\n",
    "1. L1 regularization: Enforces sparsity \n",
    "2. L2 regularization: Enforces that weights have similar values\n",
    "\n",
    "The most common weight regularization method is the L2 regularization. From the observations made in the lecture that makes totally sense - at least when we compare it to the L1 regularization. The L2 regularization penalty in the loss prefers smaller and more diffuse weight vectors and hence the model is encouraged to take into account all input dimensions to small amounts rather than a few input dimensions and very strongly.\n",
    "\n",
    "When using weight regularization, the loss function is a composition of two parts:\n",
    "$$L = \\underbrace{\\frac{1}{N} \\sum_{i} L_i}_{\\text{data loss}}  + \\underbrace{\\lambda R(W)}_{\\text{regularization loss}}$$\n",
    "The first one being the data loss, which is calculated with the Cross Entropy loss in our model. The second part is called the regularization loss $R(W)$ and is computed in the L2 case as follows:\n",
    "$$R(W) = \\sum_{k} \\sum_{l} w_{k,l}^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h3>Task: Check Code </h3>\n",
    "    <p> For convenience, we have already implemented the L2 weight regularization for you. We highly recommend to check the implementation of the method in <code>exercise_code/networks/classification_net.py</code>. The implementation that takes place in the <code>forward()</code> and <code>backward()</code> method have an impact on the whole training pipeline. Also take a look at the <code>exercise_code/networks/loss.py</code> and <code>exercise_code/solver.py</code>.  </p>\n",
    "        \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 2. Hyperparameters\n",
    "\n",
    "<img src=https://images.deepai.org/glossary-terms/05c646fe1676490aa0b8cab0732a02b2/hyperparams.png alt=hyperparameter width=700>\n",
    "\n",
    "A **hyperparameter** is a parameter that is set before the learning process begins. Recall that the parameters of weight matrix and bias vector are learned during the learning process.\n",
    "\n",
    "The hyperparameters are essential, they control and affect the whole training and have a great impact on the performance of the model. Some examples of hyperparameters we have covered in lectures:\n",
    "* Network architecture\n",
    "    * Choice of activation function\n",
    "    * Number of layers\n",
    "    * ...\n",
    "* Learning rate\n",
    "* Number of epochs\n",
    "* Batch size\n",
    "* Regularization strength\n",
    "* Momentum\n",
    "* ...\n",
    "\n",
    "Do you understand the difference between **hyperparameters** and **learnt parameters** now?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start debugging your own network!\n",
    "\n",
    "As already suggested in the lectures, you may always want to start from small and simple architectures, to make sure you are going the right way. \n",
    "\n",
    "First you may need to overfit a single training sample, then a few batches of training samples, then go deeper with larger neural networks and the whole training data.\n",
    "\n",
    "Here we always provide a default neural network (i.e. `ClassificationNet`) with arbitrary number of layers, which is a generalization from the fixed 2-layer neural network in Exercise 5. You are welcome to implement your own network, in that case just implement **`MyOwnNetwork`** in ```exercise_code/networks/classification_net.py```. You can also copy things from `ClassficationNet` and make a little adjustment to your own network. For either way, just pick a network and comment out the other one, then run the cells below for debugging.\n",
    "\n",
    "__Note__: \n",
    "- Please, make sure you don't modify the `ClassificationNet` itself so that you can always have a working network to fall back on.\n",
    "- In order to pass this submissions, you can **first stick to the default `ClassificationNet` implementation without changing any code at all**. The goal of this submission is to find reasonable hyperparameters and the parameter options of the `ClassificationNet` are broad enough.\n",
    "- Once you have surpassed the submission goal, you can try different activation functions, different weight initializations or other adjustments by writing your own network architecture in the MyOwnNetwork class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's begin with a 2-layer neural network, and overfit one single training sample.\n",
    "\n",
    "After training, let's evaluate the training process by plotting the loss curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1 / 20) train loss: 2.327992; val loss: 2.333577\n",
      "(Epoch 2 / 20) train loss: 2.327992; val loss: 2.320803\n",
      "(Epoch 3 / 20) train loss: 2.201901; val loss: 2.329869\n",
      "(Epoch 4 / 20) train loss: 2.052337; val loss: 2.355061\n",
      "(Epoch 5 / 20) train loss: 1.881469; val loss: 2.387624\n",
      "(Epoch 6 / 20) train loss: 1.711020; val loss: 2.419109\n",
      "(Epoch 7 / 20) train loss: 1.556224; val loss: 2.443894\n",
      "(Epoch 8 / 20) train loss: 1.413779; val loss: 2.460766\n",
      "(Epoch 9 / 20) train loss: 1.277494; val loss: 2.472060\n",
      "(Epoch 10 / 20) train loss: 1.146821; val loss: 2.481483\n",
      "(Epoch 11 / 20) train loss: 1.022606; val loss: 2.492313\n",
      "(Epoch 12 / 20) train loss: 0.906329; val loss: 2.506576\n",
      "(Epoch 13 / 20) train loss: 0.800357; val loss: 2.524869\n",
      "(Epoch 14 / 20) train loss: 0.706310; val loss: 2.546673\n",
      "(Epoch 15 / 20) train loss: 0.624434; val loss: 2.571004\n",
      "(Epoch 16 / 20) train loss: 0.553740; val loss: 2.596932\n",
      "(Epoch 17 / 20) train loss: 0.492828; val loss: 2.623857\n",
      "(Epoch 18 / 20) train loss: 0.440498; val loss: 2.651549\n",
      "(Epoch 19 / 20) train loss: 0.395530; val loss: 2.680034\n",
      "(Epoch 20 / 20) train loss: 0.356914; val loss: 2.709408\n"
     ]
    }
   ],
   "source": [
    "from exercise_code.solver import Solver\n",
    "from exercise_code.networks.optimizer import SGD, Adam\n",
    "from exercise_code.networks import MyOwnNetwork\n",
    "\n",
    "num_layer = 2\n",
    "epochs = 20\n",
    "reg = 0.1\n",
    "batch_size = 4\n",
    "\n",
    "model = ClassificationNet(num_layer=num_layer, reg=reg)\n",
    "# model = MyOwnNetwork()\n",
    "\n",
    "loss = CrossEntropyFromLogits()\n",
    "\n",
    "# Make a new data loader with a single training image\n",
    "overfit_dataset = ImageFolderDataset(\n",
    "    mode='train',\n",
    "    root=cifar_root, \n",
    "    download_url=download_url,\n",
    "    transform=compose_transform,\n",
    "    limit_files=1\n",
    ")\n",
    "dataloaders['train_overfit_single_image'] = DataLoader(\n",
    "    dataset=overfit_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "# Decrease validation data for only debugging\n",
    "debugging_validation_dataset = ImageFolderDataset(\n",
    "    mode='val',\n",
    "    root=cifar_root, \n",
    "    download_url=download_url,\n",
    "    transform=compose_transform,\n",
    "\n",
    "    limit_files=100\n",
    ")\n",
    "dataloaders['val_500files'] = DataLoader(\n",
    "    dataset=debugging_validation_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "solver = Solver(model, dataloaders['train_overfit_single_image'], dataloaders['val_500files'], \n",
    "                learning_rate=1e-3, loss_func=loss, optimizer=Adam)\n",
    "\n",
    "solver.train(epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Loss curves')\n",
    "plt.plot(solver.train_loss_history, '-', label='train')\n",
    "plt.plot(solver.val_loss_history, '-', label='val')\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlabel('Iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuray: 1.00000\n",
      "Validation accuray: 0.07000\n"
     ]
    }
   ],
   "source": [
    "print(\"Training accuray: %.5f\" % (solver.get_dataset_accuracy(dataloaders['train_overfit_single_image'])))\n",
    "print(\"Validation accuray: %.5f\" % (solver.get_dataset_accuracy(dataloaders['val_500files'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time let's try to overfit to a small set of training batch samples. Please observe the difference from above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1 / 300) train loss: 2.331938; val loss: 2.334331\n",
      "(Epoch 2 / 300) train loss: 2.325609; val loss: 2.337541\n",
      "(Epoch 3 / 300) train loss: 2.207519; val loss: 2.391639\n",
      "(Epoch 4 / 300) train loss: 2.048381; val loss: 2.439445\n",
      "(Epoch 5 / 300) train loss: 2.048479; val loss: 2.522273\n",
      "(Epoch 6 / 300) train loss: 1.925998; val loss: 2.566617\n",
      "(Epoch 7 / 300) train loss: 1.857588; val loss: 2.628791\n",
      "(Epoch 8 / 300) train loss: 1.725757; val loss: 2.711391\n",
      "(Epoch 9 / 300) train loss: 1.752357; val loss: 2.795958\n",
      "(Epoch 10 / 300) train loss: 1.810925; val loss: 2.842225\n",
      "(Epoch 11 / 300) train loss: 1.718056; val loss: 2.855684\n",
      "(Epoch 12 / 300) train loss: 1.646908; val loss: 2.979115\n",
      "(Epoch 13 / 300) train loss: 1.625130; val loss: 3.089495\n",
      "(Epoch 14 / 300) train loss: 1.672408; val loss: 3.119818\n",
      "(Epoch 15 / 300) train loss: 1.598648; val loss: 3.084452\n",
      "(Epoch 16 / 300) train loss: 1.674407; val loss: 3.100027\n",
      "(Epoch 17 / 300) train loss: 1.714083; val loss: 3.168348\n",
      "(Epoch 18 / 300) train loss: 1.656148; val loss: 3.252408\n",
      "(Epoch 19 / 300) train loss: 1.739993; val loss: 3.245475\n",
      "(Epoch 20 / 300) train loss: 1.561257; val loss: 3.253790\n",
      "(Epoch 21 / 300) train loss: 1.540376; val loss: 3.314405\n",
      "(Epoch 22 / 300) train loss: 1.646614; val loss: 3.308773\n",
      "(Epoch 23 / 300) train loss: 1.581151; val loss: 3.302904\n",
      "(Epoch 24 / 300) train loss: 1.422976; val loss: 3.371343\n",
      "(Epoch 25 / 300) train loss: 1.562293; val loss: 3.425782\n",
      "(Epoch 26 / 300) train loss: 1.647568; val loss: 3.397219\n",
      "(Epoch 27 / 300) train loss: 1.467402; val loss: 3.377734\n",
      "(Epoch 28 / 300) train loss: 1.495070; val loss: 3.354369\n",
      "(Epoch 29 / 300) train loss: 1.588650; val loss: 3.382466\n",
      "(Epoch 30 / 300) train loss: 1.483492; val loss: 3.407889\n",
      "(Epoch 31 / 300) train loss: 1.515264; val loss: 3.393643\n",
      "(Epoch 32 / 300) train loss: 1.466285; val loss: 3.401993\n",
      "(Epoch 33 / 300) train loss: 1.423159; val loss: 3.425434\n",
      "(Epoch 34 / 300) train loss: 1.400972; val loss: 3.430258\n",
      "(Epoch 35 / 300) train loss: 1.348053; val loss: 3.429814\n",
      "(Epoch 36 / 300) train loss: 1.329759; val loss: 3.433689\n",
      "(Epoch 37 / 300) train loss: 1.288834; val loss: 3.455196\n",
      "(Epoch 38 / 300) train loss: 1.348734; val loss: 3.464836\n",
      "(Epoch 39 / 300) train loss: 1.260980; val loss: 3.498036\n",
      "(Epoch 40 / 300) train loss: 1.358767; val loss: 3.473409\n",
      "(Epoch 41 / 300) train loss: 1.390493; val loss: 3.433425\n",
      "(Epoch 42 / 300) train loss: 1.292045; val loss: 3.491149\n",
      "(Epoch 43 / 300) train loss: 1.268652; val loss: 3.539978\n",
      "(Epoch 44 / 300) train loss: 1.330824; val loss: 3.513853\n",
      "(Epoch 45 / 300) train loss: 1.287729; val loss: 3.509725\n",
      "(Epoch 46 / 300) train loss: 1.290267; val loss: 3.519821\n",
      "(Epoch 47 / 300) train loss: 1.172291; val loss: 3.522474\n",
      "(Epoch 48 / 300) train loss: 1.195694; val loss: 3.491398\n",
      "(Epoch 49 / 300) train loss: 1.170439; val loss: 3.480601\n",
      "(Epoch 50 / 300) train loss: 1.119711; val loss: 3.482389\n",
      "(Epoch 51 / 300) train loss: 1.257560; val loss: 3.513039\n",
      "(Epoch 52 / 300) train loss: 1.181583; val loss: 3.600747\n",
      "(Epoch 53 / 300) train loss: 1.283340; val loss: 3.644795\n",
      "(Epoch 54 / 300) train loss: 1.265615; val loss: 3.624526\n",
      "(Epoch 55 / 300) train loss: 1.159158; val loss: 3.576834\n",
      "(Epoch 56 / 300) train loss: 1.182414; val loss: 3.551872\n",
      "(Epoch 57 / 300) train loss: 1.105850; val loss: 3.560429\n",
      "(Epoch 58 / 300) train loss: 1.130616; val loss: 3.571432\n",
      "(Epoch 59 / 300) train loss: 1.134558; val loss: 3.574873\n",
      "(Epoch 60 / 300) train loss: 1.027422; val loss: 3.568672\n",
      "(Epoch 61 / 300) train loss: 1.027777; val loss: 3.541614\n",
      "(Epoch 62 / 300) train loss: 1.071632; val loss: 3.526807\n",
      "(Epoch 63 / 300) train loss: 1.000724; val loss: 3.545189\n",
      "(Epoch 64 / 300) train loss: 1.001871; val loss: 3.569452\n",
      "(Epoch 65 / 300) train loss: 1.005035; val loss: 3.568322\n",
      "(Epoch 66 / 300) train loss: 1.012345; val loss: 3.581639\n",
      "(Epoch 67 / 300) train loss: 1.031481; val loss: 3.623874\n",
      "(Epoch 68 / 300) train loss: 1.022743; val loss: 3.655385\n",
      "(Epoch 69 / 300) train loss: 1.002597; val loss: 3.638895\n",
      "(Epoch 70 / 300) train loss: 1.047481; val loss: 3.595655\n",
      "(Epoch 71 / 300) train loss: 1.074237; val loss: 3.591181\n",
      "(Epoch 72 / 300) train loss: 1.023344; val loss: 3.627247\n",
      "(Epoch 73 / 300) train loss: 0.985536; val loss: 3.649597\n",
      "(Epoch 74 / 300) train loss: 0.956673; val loss: 3.626058\n",
      "(Epoch 75 / 300) train loss: 0.953968; val loss: 3.563884\n",
      "(Epoch 76 / 300) train loss: 0.932819; val loss: 3.543700\n",
      "(Epoch 77 / 300) train loss: 0.907463; val loss: 3.561685\n",
      "(Epoch 78 / 300) train loss: 0.920114; val loss: 3.581150\n",
      "(Epoch 79 / 300) train loss: 0.902032; val loss: 3.603120\n",
      "(Epoch 80 / 300) train loss: 0.899427; val loss: 3.566424\n",
      "(Epoch 81 / 300) train loss: 0.928899; val loss: 3.577112\n",
      "(Epoch 82 / 300) train loss: 0.924480; val loss: 3.587221\n",
      "(Epoch 83 / 300) train loss: 0.964044; val loss: 3.580696\n",
      "(Epoch 84 / 300) train loss: 0.874808; val loss: 3.583204\n",
      "(Epoch 85 / 300) train loss: 0.938830; val loss: 3.587788\n",
      "(Epoch 86 / 300) train loss: 0.853180; val loss: 3.594803\n",
      "(Epoch 87 / 300) train loss: 0.909091; val loss: 3.579225\n",
      "(Epoch 88 / 300) train loss: 0.899377; val loss: 3.540678\n",
      "(Epoch 89 / 300) train loss: 0.877273; val loss: 3.561657\n",
      "(Epoch 90 / 300) train loss: 0.851209; val loss: 3.592029\n",
      "(Epoch 91 / 300) train loss: 0.860576; val loss: 3.592714\n",
      "(Epoch 92 / 300) train loss: 0.917568; val loss: 3.586197\n",
      "(Epoch 93 / 300) train loss: 0.916384; val loss: 3.648954\n",
      "(Epoch 94 / 300) train loss: 1.027391; val loss: 3.628603\n",
      "(Epoch 95 / 300) train loss: 0.898212; val loss: 3.659187\n",
      "(Epoch 96 / 300) train loss: 0.937496; val loss: 3.673554\n",
      "(Epoch 97 / 300) train loss: 0.982593; val loss: 3.657673\n",
      "(Epoch 98 / 300) train loss: 0.895751; val loss: 3.655952\n",
      "(Epoch 99 / 300) train loss: 0.840275; val loss: 3.596174\n",
      "(Epoch 100 / 300) train loss: 0.906998; val loss: 3.558889\n",
      "(Epoch 101 / 300) train loss: 0.877036; val loss: 3.569467\n",
      "(Epoch 102 / 300) train loss: 0.910354; val loss: 3.572435\n",
      "(Epoch 103 / 300) train loss: 0.814440; val loss: 3.546418\n",
      "(Epoch 104 / 300) train loss: 0.885796; val loss: 3.513039\n",
      "(Epoch 105 / 300) train loss: 0.816405; val loss: 3.528076\n",
      "(Epoch 106 / 300) train loss: 1.010773; val loss: 3.574938\n",
      "(Epoch 107 / 300) train loss: 0.916906; val loss: 3.803623\n",
      "(Epoch 108 / 300) train loss: 1.111099; val loss: 3.850567\n",
      "(Epoch 109 / 300) train loss: 1.042575; val loss: 3.850491\n",
      "(Epoch 110 / 300) train loss: 1.060720; val loss: 3.814949\n",
      "(Epoch 111 / 300) train loss: 0.992984; val loss: 3.771788\n",
      "(Epoch 112 / 300) train loss: 0.930072; val loss: 3.723391\n",
      "(Epoch 113 / 300) train loss: 0.916937; val loss: 3.666048\n",
      "(Epoch 114 / 300) train loss: 0.836462; val loss: 3.638190\n",
      "(Epoch 115 / 300) train loss: 0.889796; val loss: 3.591241\n",
      "(Epoch 116 / 300) train loss: 0.818503; val loss: 3.556719\n",
      "(Epoch 117 / 300) train loss: 0.806898; val loss: 3.529216\n",
      "(Epoch 118 / 300) train loss: 0.849701; val loss: 3.518428\n",
      "(Epoch 119 / 300) train loss: 0.850689; val loss: 3.562668\n",
      "(Epoch 120 / 300) train loss: 0.842383; val loss: 3.619849\n",
      "(Epoch 121 / 300) train loss: 0.814835; val loss: 3.621091\n",
      "(Epoch 122 / 300) train loss: 0.807930; val loss: 3.575001\n",
      "(Epoch 123 / 300) train loss: 0.785612; val loss: 3.524536\n",
      "(Epoch 124 / 300) train loss: 0.795686; val loss: 3.500472\n",
      "(Epoch 125 / 300) train loss: 0.806721; val loss: 3.517049\n",
      "(Epoch 126 / 300) train loss: 0.793731; val loss: 3.564774\n",
      "(Epoch 127 / 300) train loss: 0.804540; val loss: 3.557030\n",
      "(Epoch 128 / 300) train loss: 0.775205; val loss: 3.540857\n",
      "(Epoch 129 / 300) train loss: 0.767131; val loss: 3.546941\n",
      "(Epoch 130 / 300) train loss: 0.810578; val loss: 3.561842\n",
      "(Epoch 131 / 300) train loss: 0.786076; val loss: 3.589078\n",
      "(Epoch 132 / 300) train loss: 0.783697; val loss: 3.589728\n",
      "(Epoch 133 / 300) train loss: 0.821126; val loss: 3.548807\n",
      "(Epoch 134 / 300) train loss: 0.790377; val loss: 3.552377\n",
      "(Epoch 135 / 300) train loss: 0.881686; val loss: 3.609671\n",
      "(Epoch 136 / 300) train loss: 0.831718; val loss: 3.739862\n",
      "(Epoch 137 / 300) train loss: 0.847444; val loss: 3.783653\n",
      "(Epoch 138 / 300) train loss: 0.834472; val loss: 3.730899\n",
      "(Epoch 139 / 300) train loss: 0.794187; val loss: 3.668547\n",
      "(Epoch 140 / 300) train loss: 0.780067; val loss: 3.616526\n",
      "(Epoch 141 / 300) train loss: 0.779698; val loss: 3.568906\n",
      "(Epoch 142 / 300) train loss: 0.787000; val loss: 3.541550\n",
      "(Epoch 143 / 300) train loss: 0.766881; val loss: 3.558284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 144 / 300) train loss: 0.803021; val loss: 3.587206\n",
      "(Epoch 145 / 300) train loss: 0.773339; val loss: 3.613601\n",
      "(Epoch 146 / 300) train loss: 0.801500; val loss: 3.609048\n",
      "(Epoch 147 / 300) train loss: 0.775126; val loss: 3.614348\n",
      "(Epoch 148 / 300) train loss: 0.778844; val loss: 3.610526\n",
      "(Epoch 149 / 300) train loss: 0.796415; val loss: 3.583842\n",
      "(Epoch 150 / 300) train loss: 0.751581; val loss: 3.581504\n",
      "(Epoch 151 / 300) train loss: 0.765140; val loss: 3.614690\n",
      "(Epoch 152 / 300) train loss: 0.761808; val loss: 3.594061\n",
      "(Epoch 153 / 300) train loss: 0.753824; val loss: 3.578165\n",
      "(Epoch 154 / 300) train loss: 0.726522; val loss: 3.556799\n",
      "(Epoch 155 / 300) train loss: 0.744609; val loss: 3.575133\n",
      "(Epoch 156 / 300) train loss: 0.792894; val loss: 3.582753\n",
      "(Epoch 157 / 300) train loss: 0.774394; val loss: 3.571200\n",
      "(Epoch 158 / 300) train loss: 0.739311; val loss: 3.576578\n",
      "(Epoch 159 / 300) train loss: 0.741662; val loss: 3.620671\n",
      "(Epoch 160 / 300) train loss: 0.753935; val loss: 3.646358\n",
      "(Epoch 161 / 300) train loss: 0.782563; val loss: 3.617880\n",
      "(Epoch 162 / 300) train loss: 0.763365; val loss: 3.559865\n",
      "(Epoch 163 / 300) train loss: 0.767466; val loss: 3.564209\n",
      "(Epoch 164 / 300) train loss: 0.774065; val loss: 3.622210\n",
      "(Epoch 165 / 300) train loss: 0.732488; val loss: 3.623258\n",
      "(Epoch 166 / 300) train loss: 0.760286; val loss: 3.601759\n",
      "(Epoch 167 / 300) train loss: 0.757921; val loss: 3.548541\n",
      "(Epoch 168 / 300) train loss: 0.747500; val loss: 3.552871\n",
      "(Epoch 169 / 300) train loss: 0.753144; val loss: 3.606436\n",
      "(Epoch 170 / 300) train loss: 0.749281; val loss: 3.686452\n",
      "(Epoch 171 / 300) train loss: 0.738531; val loss: 3.654629\n",
      "(Epoch 172 / 300) train loss: 0.730391; val loss: 3.604197\n",
      "(Epoch 173 / 300) train loss: 0.759897; val loss: 3.573753\n",
      "(Epoch 174 / 300) train loss: 0.768693; val loss: 3.616529\n",
      "(Epoch 175 / 300) train loss: 0.726986; val loss: 3.724468\n",
      "(Epoch 176 / 300) train loss: 0.804423; val loss: 3.709693\n",
      "(Epoch 177 / 300) train loss: 0.797810; val loss: 3.638303\n",
      "(Epoch 178 / 300) train loss: 0.750599; val loss: 3.585636\n",
      "(Epoch 179 / 300) train loss: 0.797846; val loss: 3.607448\n",
      "(Epoch 180 / 300) train loss: 0.801311; val loss: 3.694049\n",
      "(Epoch 181 / 300) train loss: 0.859225; val loss: 3.712115\n",
      "(Epoch 182 / 300) train loss: 0.754170; val loss: 3.714513\n",
      "(Epoch 183 / 300) train loss: 0.834290; val loss: 3.697909\n",
      "(Epoch 184 / 300) train loss: 0.823297; val loss: 3.689442\n",
      "(Epoch 185 / 300) train loss: 0.787886; val loss: 3.708907\n",
      "(Epoch 186 / 300) train loss: 0.820440; val loss: 3.728826\n",
      "(Epoch 187 / 300) train loss: 0.795656; val loss: 3.689401\n",
      "(Epoch 188 / 300) train loss: 0.757652; val loss: 3.650762\n",
      "(Epoch 189 / 300) train loss: 0.762815; val loss: 3.598670\n",
      "(Epoch 190 / 300) train loss: 0.757050; val loss: 3.551154\n",
      "(Epoch 191 / 300) train loss: 0.800520; val loss: 3.580254\n",
      "(Epoch 192 / 300) train loss: 0.768210; val loss: 3.658468\n",
      "(Epoch 193 / 300) train loss: 0.820095; val loss: 3.696640\n",
      "(Epoch 194 / 300) train loss: 0.788721; val loss: 3.714835\n",
      "(Epoch 195 / 300) train loss: 0.768306; val loss: 3.660237\n",
      "(Epoch 196 / 300) train loss: 0.771151; val loss: 3.611363\n",
      "(Epoch 197 / 300) train loss: 0.768989; val loss: 3.681698\n",
      "(Epoch 198 / 300) train loss: 0.774044; val loss: 3.737884\n",
      "(Epoch 199 / 300) train loss: 0.776415; val loss: 3.728292\n",
      "(Epoch 200 / 300) train loss: 0.799004; val loss: 3.640647\n",
      "(Epoch 201 / 300) train loss: 0.770104; val loss: 3.627651\n",
      "(Epoch 202 / 300) train loss: 0.780966; val loss: 3.623734\n",
      "(Epoch 203 / 300) train loss: 0.730018; val loss: 3.657930\n",
      "(Epoch 204 / 300) train loss: 0.737165; val loss: 3.676205\n",
      "(Epoch 205 / 300) train loss: 0.753726; val loss: 3.601258\n",
      "(Epoch 206 / 300) train loss: 0.724513; val loss: 3.546811\n",
      "(Epoch 207 / 300) train loss: 0.766933; val loss: 3.589951\n",
      "(Epoch 208 / 300) train loss: 0.719522; val loss: 3.691111\n",
      "(Epoch 209 / 300) train loss: 0.714733; val loss: 3.728427\n",
      "(Epoch 210 / 300) train loss: 0.737932; val loss: 3.670059\n",
      "(Epoch 211 / 300) train loss: 0.737858; val loss: 3.615141\n",
      "(Epoch 212 / 300) train loss: 0.724893; val loss: 3.616577\n",
      "(Epoch 213 / 300) train loss: 0.761122; val loss: 3.629339\n",
      "(Epoch 214 / 300) train loss: 0.731882; val loss: 3.628867\n",
      "(Epoch 215 / 300) train loss: 0.731660; val loss: 3.613929\n",
      "(Epoch 216 / 300) train loss: 0.733070; val loss: 3.603583\n",
      "(Epoch 217 / 300) train loss: 0.721475; val loss: 3.622810\n",
      "(Epoch 218 / 300) train loss: 0.734419; val loss: 3.662188\n",
      "(Epoch 219 / 300) train loss: 0.757359; val loss: 3.671290\n",
      "(Epoch 220 / 300) train loss: 0.756611; val loss: 3.644900\n",
      "(Epoch 221 / 300) train loss: 0.727593; val loss: 3.633846\n",
      "(Epoch 222 / 300) train loss: 0.760914; val loss: 3.669818\n",
      "(Epoch 223 / 300) train loss: 0.718413; val loss: 3.683529\n",
      "(Epoch 224 / 300) train loss: 0.730066; val loss: 3.650558\n",
      "(Epoch 225 / 300) train loss: 0.749502; val loss: 3.606464\n",
      "(Epoch 226 / 300) train loss: 0.748110; val loss: 3.629021\n",
      "(Epoch 227 / 300) train loss: 0.746946; val loss: 3.658865\n",
      "(Epoch 228 / 300) train loss: 0.722235; val loss: 3.665191\n",
      "(Epoch 229 / 300) train loss: 0.760407; val loss: 3.681083\n",
      "(Epoch 230 / 300) train loss: 0.735162; val loss: 3.666728\n",
      "(Epoch 231 / 300) train loss: 0.748278; val loss: 3.652603\n",
      "(Epoch 232 / 300) train loss: 0.715551; val loss: 3.647779\n",
      "(Epoch 233 / 300) train loss: 0.770674; val loss: 3.644554\n",
      "(Epoch 234 / 300) train loss: 0.713985; val loss: 3.655581\n",
      "(Epoch 235 / 300) train loss: 0.738823; val loss: 3.659639\n",
      "(Epoch 236 / 300) train loss: 0.738402; val loss: 3.652668\n",
      "(Epoch 237 / 300) train loss: 0.705538; val loss: 3.638517\n",
      "(Epoch 238 / 300) train loss: 0.800849; val loss: 3.623204\n",
      "(Epoch 239 / 300) train loss: 0.740161; val loss: 3.807227\n",
      "(Epoch 240 / 300) train loss: 0.971412; val loss: 3.834113\n",
      "(Epoch 241 / 300) train loss: 0.824002; val loss: 3.994031\n",
      "(Epoch 242 / 300) train loss: 0.914461; val loss: 4.049645\n",
      "(Epoch 243 / 300) train loss: 0.881154; val loss: 3.993486\n",
      "(Epoch 244 / 300) train loss: 0.872629; val loss: 3.901666\n",
      "(Epoch 245 / 300) train loss: 0.812461; val loss: 3.802015\n",
      "(Epoch 246 / 300) train loss: 0.829320; val loss: 3.685460\n",
      "(Epoch 247 / 300) train loss: 0.759923; val loss: 3.584726\n",
      "(Epoch 248 / 300) train loss: 0.850469; val loss: 3.603523\n",
      "(Epoch 249 / 300) train loss: 0.805950; val loss: 3.824855\n",
      "(Epoch 250 / 300) train loss: 0.881791; val loss: 3.926704\n",
      "(Epoch 251 / 300) train loss: 0.889678; val loss: 3.983971\n",
      "(Epoch 252 / 300) train loss: 0.834142; val loss: 3.927223\n",
      "(Epoch 253 / 300) train loss: 0.842214; val loss: 3.826040\n",
      "(Epoch 254 / 300) train loss: 0.823448; val loss: 3.768537\n",
      "(Epoch 255 / 300) train loss: 0.773457; val loss: 3.753602\n",
      "(Epoch 256 / 300) train loss: 0.737834; val loss: 3.714318\n",
      "(Epoch 257 / 300) train loss: 0.755912; val loss: 3.684264\n",
      "(Epoch 258 / 300) train loss: 0.760754; val loss: 3.654820\n",
      "(Epoch 259 / 300) train loss: 0.796119; val loss: 3.592765\n",
      "(Epoch 260 / 300) train loss: 0.714736; val loss: 3.639395\n",
      "(Epoch 261 / 300) train loss: 0.768238; val loss: 3.742469\n",
      "(Epoch 262 / 300) train loss: 0.725549; val loss: 3.810303\n",
      "(Epoch 263 / 300) train loss: 0.764351; val loss: 3.781895\n",
      "(Epoch 264 / 300) train loss: 0.746497; val loss: 3.729046\n",
      "(Epoch 265 / 300) train loss: 0.722479; val loss: 3.648524\n",
      "(Epoch 266 / 300) train loss: 0.725429; val loss: 3.615820\n",
      "(Epoch 267 / 300) train loss: 0.721236; val loss: 3.633468\n",
      "(Epoch 268 / 300) train loss: 0.721535; val loss: 3.649309\n",
      "(Epoch 269 / 300) train loss: 0.766549; val loss: 3.639756\n",
      "(Epoch 270 / 300) train loss: 0.735235; val loss: 3.648308\n",
      "(Epoch 271 / 300) train loss: 0.739928; val loss: 3.685777\n",
      "(Epoch 272 / 300) train loss: 0.744881; val loss: 3.726520\n",
      "(Epoch 273 / 300) train loss: 0.753621; val loss: 3.725402\n",
      "(Epoch 274 / 300) train loss: 0.760492; val loss: 3.692164\n",
      "(Epoch 275 / 300) train loss: 0.736080; val loss: 3.665892\n",
      "(Epoch 276 / 300) train loss: 0.732000; val loss: 3.703524\n",
      "(Epoch 277 / 300) train loss: 0.747861; val loss: 3.704234\n",
      "(Epoch 278 / 300) train loss: 0.707622; val loss: 3.735098\n",
      "(Epoch 279 / 300) train loss: 0.728734; val loss: 3.725019\n",
      "(Epoch 280 / 300) train loss: 0.732564; val loss: 3.673679\n",
      "(Epoch 281 / 300) train loss: 0.707234; val loss: 3.647852\n",
      "(Epoch 282 / 300) train loss: 0.742876; val loss: 3.656642\n",
      "(Epoch 283 / 300) train loss: 0.750598; val loss: 3.701714\n",
      "(Epoch 284 / 300) train loss: 0.737241; val loss: 3.746775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 285 / 300) train loss: 0.713734; val loss: 3.704964\n",
      "(Epoch 286 / 300) train loss: 0.743165; val loss: 3.659031\n",
      "(Epoch 287 / 300) train loss: 0.716651; val loss: 3.657785\n",
      "(Epoch 288 / 300) train loss: 0.728366; val loss: 3.684076\n",
      "(Epoch 289 / 300) train loss: 0.734820; val loss: 3.697469\n",
      "(Epoch 290 / 300) train loss: 0.706329; val loss: 3.646741\n",
      "(Epoch 291 / 300) train loss: 0.702696; val loss: 3.626578\n",
      "(Epoch 292 / 300) train loss: 0.749852; val loss: 3.646237\n",
      "(Epoch 293 / 300) train loss: 0.705670; val loss: 3.714005\n",
      "(Epoch 294 / 300) train loss: 0.747733; val loss: 3.710017\n",
      "(Epoch 295 / 300) train loss: 0.707664; val loss: 3.694517\n",
      "(Epoch 296 / 300) train loss: 0.738340; val loss: 3.695456\n",
      "(Epoch 297 / 300) train loss: 0.717385; val loss: 3.781534\n",
      "(Epoch 298 / 300) train loss: 0.715801; val loss: 3.749063\n",
      "(Epoch 299 / 300) train loss: 0.823464; val loss: 3.703186\n",
      "(Epoch 300 / 300) train loss: 0.736283; val loss: 3.851402\n"
     ]
    }
   ],
   "source": [
    "from exercise_code.networks import MyOwnNetwork\n",
    "\n",
    "num_layer = 2\n",
    "epochs = 300\n",
    "reg = 0.1\n",
    "num_samples = 10\n",
    "\n",
    "model = ClassificationNet(num_layer=num_layer, reg=reg)\n",
    "# model = MyOwnNetwork()\n",
    "\n",
    "loss = CrossEntropyFromLogits()\n",
    "\n",
    "# Make a new data loader with a our num_samples training image\n",
    "overfit_dataset = ImageFolderDataset(\n",
    "    mode='train',\n",
    "    root=cifar_root, \n",
    "    download_url=download_url,\n",
    "    transform=compose_transform,\n",
    "    limit_files=num_samples\n",
    ")\n",
    "dataloaders['train_overfit_10samples'] = DataLoader(\n",
    "    dataset=overfit_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "solver = Solver(model, dataloaders['train_overfit_10samples'], dataloaders['val_500files'], \n",
    "                learning_rate=1e-3, loss_func=loss, optimizer=Adam)\n",
    "\n",
    "solver.train(epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Loss curves')\n",
    "plt.plot(solver.train_loss_history, '-', label='train')\n",
    "plt.plot(solver.val_loss_history, '-', label='val')\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlabel('Iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuray: 0.90000\n",
      "Validation accuray: 0.08000\n"
     ]
    }
   ],
   "source": [
    "print(\"Training accuray: %.5f\" % (solver.get_dataset_accuracy(dataloaders['train_overfit_10samples'])))\n",
    "print(\"Validation accuray: %.5f\" % (solver.get_dataset_accuracy(dataloaders['val_500files'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're overfitting the training data, that means the network's implementation is correct. However, as you have more samples to overfit, your accuracy will be way lower. You can increase the number of epochs above to achieve better results.\n",
    "\n",
    "Now let's try to feed all the training and validation data into the network, but this time we set the same hyperparameters for 2-layer and 5-layer networks, and compare the different behaviors.\n",
    "\n",
    "__Note__: This may take about 1 min for each epoch as the training set is quite large. For convenience, we only train on 1000 images for now but use the full validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1 / 5) train loss: 2.305675; val loss: 2.305666\n",
      "(Epoch 2 / 5) train loss: 2.101538; val loss: 2.002195\n",
      "(Epoch 3 / 5) train loss: 1.972995; val loss: 1.970478\n",
      "(Epoch 4 / 5) train loss: 1.950878; val loss: 1.957107\n",
      "(Epoch 5 / 5) train loss: 1.942987; val loss: 1.953382\n"
     ]
    }
   ],
   "source": [
    "from exercise_code.networks import MyOwnNetwork\n",
    "\n",
    "num_layer = 2\n",
    "epochs = 5\n",
    "reg = 0.01\n",
    "\n",
    "# Make a new data loader with 1000 training samples\n",
    "num_samples = 1000\n",
    "overfit_dataset = ImageFolderDataset(\n",
    "    mode='train',\n",
    "    root=cifar_root, \n",
    "    download_url=download_url,\n",
    "    transform=compose_transform,\n",
    "    limit_files=num_samples\n",
    ")\n",
    "dataloaders['train_small'] = DataLoader(\n",
    "    dataset=overfit_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "\n",
    "# Change here if you want to use the full training set\n",
    "use_full_training_set = True\n",
    "if not use_full_training_set:\n",
    "    train_loader = dataloaders['train_small']\n",
    "else:\n",
    "    train_loader = dataloaders['train']\n",
    "    \n",
    "\n",
    "model = ClassificationNet(num_layer=num_layer, reg=reg)\n",
    "# model = MyOwnNetwork()\n",
    "\n",
    "loss = CrossEntropyFromLogits()\n",
    "\n",
    "solver = Solver(model, train_loader, dataloaders['val'], \n",
    "                learning_rate=1e-3, loss_func=loss, optimizer=Adam)\n",
    "\n",
    "solver.train(epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Loss curves')\n",
    "plt.plot(solver.train_loss_history, '-', label='train')\n",
    "plt.plot(solver.val_loss_history, '-', label='val')\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlabel('Iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuray: 0.36175\n",
      "Validation accuray: 0.34766\n"
     ]
    }
   ],
   "source": [
    "print(\"Training accuray: %.5f\" % (solver.get_dataset_accuracy(train_loader)))\n",
    "print(\"Validation accuray: %.5f\" % (solver.get_dataset_accuracy(dataloaders['val'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1 / 5) train loss: 2.305982; val loss: 2.305993\n",
      "(Epoch 2 / 5) train loss: 2.305021; val loss: 2.305271\n",
      "(Epoch 3 / 5) train loss: 2.304814; val loss: 2.305448\n",
      "(Epoch 4 / 5) train loss: 2.305405; val loss: 2.304000\n",
      "(Epoch 5 / 5) train loss: 2.305156; val loss: 2.304378\n"
     ]
    }
   ],
   "source": [
    "from exercise_code.networks import MyOwnNetwork\n",
    "\n",
    "num_layer = 5\n",
    "epochs = 5\n",
    "reg = 0.01\n",
    "\n",
    "model = ClassificationNet(num_layer=num_layer, reg=reg)\n",
    "# model = MyOwnNetwork()\n",
    "\n",
    "# Change here if you want to use the full training set\n",
    "use_full_training_set = True\n",
    "if not use_full_training_set:\n",
    "    train_loader = dataloaders['train_small']\n",
    "else:\n",
    "    train_loader = dataloaders['train']\n",
    "\n",
    "loss = CrossEntropyFromLogits()\n",
    "\n",
    "solver = Solver(model, train_loader, dataloaders['val'], \n",
    "                learning_rate=1e-3, loss_func=loss, optimizer=Adam)\n",
    "\n",
    "solver.train(epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Loss curves')\n",
    "plt.plot(solver.train_loss_history, '-', label='train')\n",
    "plt.plot(solver.val_loss_history, '-', label='val')\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlabel('Iteration')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuray: 0.09989\n",
      "Validation accuray: 0.09986\n"
     ]
    }
   ],
   "source": [
    "print(\"Training accuray: %.5f\" % (solver.get_dataset_accuracy(train_loader)))\n",
    "print(\"Validation accuray: %.5f\" % (solver.get_dataset_accuracy(dataloaders['val'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from above, the same hyperparameter set can decrease the loss for a 2-layer network, but for a 5-layer network, it hardly works.\n",
    "\n",
    "The steps above are already mentioned in the lectures as debugging steps before training a neural network. \n",
    "\n",
    "If you implement your own network, always make sure you do the steps above before tuning the hyperparameters as below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Difficulty in tuning hyperparameters\n",
    "As can be seen through the results of training a larger network, training with whole data doesn't fit the training data as well as training with small number of training data. Besides, the architecture of neural network makes a difference, too. Small decisions on hyperparameters count. \n",
    "\n",
    "Usually, but not always, hyperparameters cannot be learned using well known gradient based methods (such as gradient descent), which are commonly employed to learn parameters. Besides, some hyperparameters can affect the structure of the model and the loss function.\n",
    "\n",
    "As mentioned before, hyperparameters need to be set before training. Tuning hyperparameters is hard, because you always have to try different combinations of the hyperparameters, train the network, do the validation and pick the best one. Besides, it is not guaranteed that you'll find the best combination.\n",
    "\n",
    "But let's take a more detailed look at hyperparameter tuning methods that are covered in the lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Hyperparameter Tuning\n",
    "\n",
    "![alt text](https://blog.floydhub.com/content/images/2018/08/Screen-Shot-2018-08-22-at-17.59.25.png \"\")\n",
    "\n",
    "One of the main challenges in deep learning is finding the set of hyperparameters that performs best.\n",
    "\n",
    "So far, we have followed a manual approach by guessing hyperparameters, running the model, observing the result and maybe tweaking the hyperparameters based on this result. As you have probably noticed, this manual hyperparameter tuning is unstructured, inefficient and can become very tedious.\n",
    "\n",
    "\n",
    "A more systematic (and actually very simple) approach for hyperparameter tuning that you've already learned in the lecture  is implementing a **Grid Search**. \n",
    "\n",
    "\n",
    "\n",
    "## Grid Search\n",
    "Grid search is a simple and naive, yet effective method to automate the hyperparameter tuning:\n",
    "\n",
    "* First, you define the set of parameters you want to tune, e.g. $\\{learning\\_rate, regularization\\_strength\\}$.\n",
    "\n",
    "* For each hyperparameter, you then define a set of possible values, e.g. $learning\\_rate = \\{0.0001, 0.001, 0.01, 0.1\\}$.\n",
    "\n",
    "* Then, you train a model for every possible combination of these hyperparameter values and afterwards select the combination that works best (e.g. in terms of accuracy on your validation set).\n",
    "\n",
    "**Note**: to keep things simple for the beginning, it'll be enough to just focus on the hyperparameters `learning_rate` and `regularization_strength`  here, as in the example above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h3>Task: Check code </h3>\n",
    "    <p> Check out our grid search implementation in <code>exercise_code/hyperparameter_tuning.py</code>. We show a simple for loop implementation and a more sophisticated one for multiple inputs. </p>\n",
    "</div>\n",
    "\n",
    "Let us try the Grid Search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Config #1 [of 3]:\n",
      " {'learning_rate': 0.01, 'reg': 0.0001}\n",
      "(Epoch 1 / 10) train loss: 2.302614; val loss: 2.302329\n",
      "(Epoch 2 / 10) train loss: 2.536832; val loss: 2.540964\n",
      "(Epoch 3 / 10) train loss: 2.725570; val loss: 2.972955\n",
      "(Epoch 4 / 10) train loss: 2.892284; val loss: 3.055243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yehchenchen/Desktop/TUM/WS20:21/I2DL/i2dl/exercise_06_/exercise_code/networks/layer.py:67: RuntimeWarning: overflow encountered in exp\n",
      "  outputs = 1 / (1 + np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 5 / 10) train loss: 3.044627; val loss: 2.930283\n",
      "(Epoch 6 / 10) train loss: 3.110358; val loss: 3.281921\n",
      "Stopping early at epoch 6!\n",
      "\n",
      "Evaluating Config #2 [of 3]:\n",
      " {'learning_rate': 0.001, 'reg': 0.0001}\n",
      "(Epoch 1 / 10) train loss: 2.302800; val loss: 2.302229\n",
      "(Epoch 2 / 10) train loss: 2.190285; val loss: 2.062868\n",
      "(Epoch 3 / 10) train loss: 2.006236; val loss: 1.928286\n",
      "(Epoch 4 / 10) train loss: 1.918467; val loss: 1.915372\n",
      "(Epoch 5 / 10) train loss: 1.830036; val loss: 1.894378\n",
      "(Epoch 6 / 10) train loss: 1.772693; val loss: 1.877126\n",
      "(Epoch 7 / 10) train loss: 1.694481; val loss: 1.965147\n",
      "(Epoch 8 / 10) train loss: 1.641860; val loss: 1.916634\n",
      "(Epoch 9 / 10) train loss: 1.579695; val loss: 1.939440\n",
      "(Epoch 10 / 10) train loss: 1.518792; val loss: 1.973140\n",
      "\n",
      "Evaluating Config #3 [of 3]:\n",
      " {'learning_rate': 0.0001, 'reg': 0.0001}\n",
      "(Epoch 1 / 10) train loss: 2.302497; val loss: 2.302975\n",
      "(Epoch 2 / 10) train loss: 2.280467; val loss: 2.242676\n",
      "(Epoch 3 / 10) train loss: 2.214792; val loss: 2.180832\n",
      "(Epoch 4 / 10) train loss: 2.160343; val loss: 2.125101\n",
      "(Epoch 5 / 10) train loss: 2.111388; val loss: 2.098036\n",
      "(Epoch 6 / 10) train loss: 2.068360; val loss: 2.056465\n",
      "(Epoch 7 / 10) train loss: 2.028013; val loss: 2.020517\n",
      "(Epoch 8 / 10) train loss: 1.991636; val loss: 2.000669\n",
      "(Epoch 9 / 10) train loss: 1.953962; val loss: 1.974238\n",
      "(Epoch 10 / 10) train loss: 1.919475; val loss: 1.958936\n",
      "\n",
      "Search done. Best Val Loss = 1.8771256720837564\n",
      "Best Config: {'learning_rate': 0.001, 'reg': 0.0001}\n"
     ]
    }
   ],
   "source": [
    "from exercise_code.networks import MyOwnNetwork\n",
    "\n",
    "# Specify the used network\n",
    "model_class = ClassificationNet\n",
    "\n",
    "from exercise_code import hyperparameter_tuning\n",
    "best_model, results = hyperparameter_tuning.grid_search(\n",
    "    dataloaders['train_small'], dataloaders['val_500files'],\n",
    "    grid_search_spaces = {\n",
    "        \"learning_rate\": [1e-2, 1e-3, 1e-4], \n",
    "        \"reg\": [1e-4]\n",
    "    },\n",
    "    model_class=model_class,\n",
    "    epochs=10, patience=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results of your grid search, you might already have found some hyperparameter combinations that work better than others. A common practice is to now repeat the grid search on a more narrow domain centered around the parameters that worked best. \n",
    "\n",
    "**Conclusion Grid Search**\n",
    "\n",
    "With grid search we now have automated the hyperparameter tuning to a certain degree. Another advantage is that since the training of all models are independent of each other, you can parallelize the grid search, i.e.,  try out different hyperparameter configurations in parallel on different machines.\n",
    "\n",
    "However, as you have probably noticed, there is one big problem with this approach: the number of possible combinations to try out grows exponentially with the number of hyperparameters (\"curse of dimensionality\"). As we add more hyperparameters to the grid search, the search space will explode in time complexity, making this strategy unfeasible.\n",
    "\n",
    "Especially when your search space contains more than 3 or 4 dimensions, it is often better to use another, similar hyperparameter tuning method that you've already learned about: random search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Search\n",
    "Random search is very similar to grid search, with the only difference, that instead of providing specific values for every hyperparameter, you only define a range for each hyperparameter - then, the values are sampled randomly from the provided ranges.\n",
    "\n",
    "![alt text](https://i.stack.imgur.com/cIDuR.png \"\")\n",
    "\n",
    "The figure above illustrates the difference in the hyperparameter space exploration between grid search and random search: assume you have 2 hyperparameters with each 3 values. Running a grid search results in training $3^2=9$ different models - but in the end, you've just tired out 3 values for each parameter. For random search on the other hand, after training 9 models you'll have tried out 9 different values for each hyperparameter, which often leads much faster to good results.\n",
    "\n",
    "To get a deeper understanding of random search and why it is more efficient than grid search, you should definitely check out this paper: http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h3>Task: Check code </h3>\n",
    "    <p> Check out our random search implementation in <code>exercise_code/hyperparameter_tuning.py</code>. </p>\n",
    "    <p> <b>Note:</b> Regarding the sample space of each parameter, think about the scale for which it makes most sense to sample in. For example the learning rate is usually sampled on a logarithmic scale!</p>\n",
    "</div>\n",
    "\n",
    "Let us apply a random search in the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Config #1 [of 2]:\n",
      " {'learning_rate': 0.002334971456175869, 'reg': 2.9629135617929315e-05, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff12831a748>}\n",
      "(Epoch 1 / 20) train loss: 2.302505; val loss: 2.302203\n",
      "(Epoch 2 / 20) train loss: 2.184591; val loss: 2.000988\n",
      "(Epoch 3 / 20) train loss: 2.027946; val loss: 1.968003\n",
      "(Epoch 4 / 20) train loss: 1.961123; val loss: 1.988076\n",
      "(Epoch 5 / 20) train loss: 1.912837; val loss: 1.968716\n",
      "(Epoch 6 / 20) train loss: 1.848305; val loss: 2.002818\n",
      "(Epoch 7 / 20) train loss: 1.817505; val loss: 1.953327\n",
      "(Epoch 8 / 20) train loss: 1.797383; val loss: 1.990019\n",
      "(Epoch 9 / 20) train loss: 1.775968; val loss: 2.056090\n",
      "(Epoch 10 / 20) train loss: 1.750964; val loss: 2.095661\n",
      "(Epoch 11 / 20) train loss: 1.720107; val loss: 2.093481\n",
      "(Epoch 12 / 20) train loss: 1.690213; val loss: 2.126658\n",
      "Stopping early at epoch 12!\n",
      "\n",
      "Evaluating Config #2 [of 2]:\n",
      " {'learning_rate': 0.0006964867219676682, 'reg': 0.0005155358553396383, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff12831a748>}\n",
      "(Epoch 1 / 20) train loss: 2.302527; val loss: 2.303270\n",
      "(Epoch 2 / 20) train loss: 2.193350; val loss: 2.079059\n",
      "(Epoch 3 / 20) train loss: 2.031961; val loss: 1.990295\n",
      "(Epoch 4 / 20) train loss: 1.954177; val loss: 1.912431\n",
      "(Epoch 5 / 20) train loss: 1.874952; val loss: 1.898251\n",
      "(Epoch 6 / 20) train loss: 1.824520; val loss: 1.904300\n",
      "(Epoch 7 / 20) train loss: 1.767489; val loss: 1.899964\n",
      "(Epoch 8 / 20) train loss: 1.704084; val loss: 1.898324\n",
      "(Epoch 9 / 20) train loss: 1.646542; val loss: 1.982653\n",
      "(Epoch 10 / 20) train loss: 1.594960; val loss: 1.947072\n",
      "Stopping early at epoch 10!\n",
      "\n",
      "Search done. Best Val Loss = 1.898250945748008\n",
      "Best Config: {'learning_rate': 0.0006964867219676682, 'reg': 0.0005155358553396383, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff12831a748>}\n"
     ]
    }
   ],
   "source": [
    "from exercise_code.hyperparameter_tuning import random_search\n",
    "from exercise_code.networks import MyOwnNetwork\n",
    "\n",
    "# Specify the used network\n",
    "model_class = ClassificationNet\n",
    "\n",
    "best_model, results = random_search(\n",
    "    dataloaders['train_small'], dataloaders['val_500files'],\n",
    "    random_search_spaces = {\n",
    "        \"learning_rate\": ([1e-2, 1e-6], 'log'),\n",
    "        \"reg\": ([1e-3, 1e-7], \"log\"),\n",
    "        \"loss_func\": ([CrossEntropyFromLogits()], \"item\")\n",
    "    },\n",
    "    model_class=model_class,\n",
    "    num_search = 2, epochs=20, patience=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to run it with the whole dataset, and let it search for a few hours for a nice configuration. \n",
    "\n",
    "However, to save some time, let's first implement an **early-stopping** mechanism, that you also already know from the lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping\n",
    "\n",
    "By now you've already seen a lot of training curves:\n",
    "\n",
    "<img src=http://fouryears.eu/wp-content/uploads/2017/12/early_stopping.png></img>\n",
    "\n",
    "Usually, at some point the validation loss goes up again, which is a sign that we're overfitting to our training data. Since it actually doesn't make any sense to train further at this point, it's common practice to apply \"early stopping\", i.e., cancel the training process when the validation loss doesn't improve anymore. The nice thing about this concept is, that not only it improves generalization through the prevention of overfitting, but also it saves us a lot of time - one of our most valuable resources in deep learning.\n",
    "\n",
    "Since there are natural fluctuations in the validation loss, you usually don't cancel the training process right at the first epoch when the validation-loss increases, but instead, you wait for some epochs (specified by the `patience` parameter) and if the loss still doesn't improve, we stop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <h3>Task: Check code </h3>\n",
    "    <p> Take a look at the implement of early stopping mechanism in the <code>exercise_code/solver.py</code> file. </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's find the perfect model!\n",
    "\n",
    "You've now set everything up to start training your model and finding a nice set of hyperparameters using a combination of grid or random search!\n",
    "\n",
    "Since we'll now be training with a much larger number of samples, you should be aware that this process will definitely take some time. So be prepared to let your machine run for a while. \n",
    "\n",
    "You don't have to use the whole dataset at the beginning, instead you can also use a medium large subset of the samples. Also, you don't need to train for a large number of epochs - as mentioned above: We first want to get an overview about our hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h3>Task: Hyperparameters Tunning & Model Training </h3>\n",
    "        <p> Now, it is your turn to do the hyperparamater tuning. In the cell below, you can use the <code>random_search</code> function to find a good choice of parameters. Put in some reasonable ranges for the hyperparameters and evaluate them.\n",
    "    <p> <b>Note:</b> At the beginning, it's a good approach to first do a coarse random search across a <b> wide range of values</b> to find promising sub-ranges of your parameter space and use <b> a medium large subset of the dataset </b>instead the whole as well. Afterwards, you can zoom in to these ranges and do another random search (or grid search) to finetune the configuration. Use the cell below to play around and find good hyperparameters for your model!</p>\n",
    "        <p> Finally, once you've found some promising hyperparameters (or narrowed them down to promising subranges), it's time to utilize these hyperparameters to train your network on the whole dataset for a large number of epochs so that your own model can reach an acceptable performance. \n",
    "        <p> <b>Hint:</b> You may use a <code>Solver</code> class we provided before or directly use the <code>random_search</code> function (as you can also monitor the loss here) for model training.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Config #1 [of 100]:\n",
      " {'learning_rate': 0.0006259743310658067, 'reg': 0.001695347278035171, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 183, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 4}\n",
      "(Epoch 1 / 20) train loss: 2.303655; val loss: 2.303655\n",
      "(Epoch 2 / 20) train loss: 2.302878; val loss: 2.302605\n",
      "(Epoch 3 / 20) train loss: 2.302137; val loss: 2.302500\n",
      "(Epoch 4 / 20) train loss: 2.301744; val loss: 2.302362\n",
      "(Epoch 5 / 20) train loss: 2.301230; val loss: 2.302580\n",
      "(Epoch 6 / 20) train loss: 2.300943; val loss: 2.302333\n",
      "(Epoch 7 / 20) train loss: 2.300673; val loss: 2.302648\n",
      "(Epoch 8 / 20) train loss: 2.300439; val loss: 2.302630\n",
      "(Epoch 9 / 20) train loss: 2.300294; val loss: 2.302706\n",
      "(Epoch 10 / 20) train loss: 2.300118; val loss: 2.302742\n",
      "(Epoch 11 / 20) train loss: 2.300073; val loss: 2.302973\n",
      "Stopping early at epoch 11!\n",
      "\n",
      "Evaluating Config #2 [of 100]:\n",
      " {'learning_rate': 3.1303970806154505e-06, 'reg': 0.001939622572388047, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 375, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 3}\n",
      "(Epoch 1 / 20) train loss: 2.305098; val loss: 2.305098\n",
      "(Epoch 2 / 20) train loss: 2.305026; val loss: 2.304952\n",
      "(Epoch 3 / 20) train loss: 2.304881; val loss: 2.304814\n",
      "(Epoch 4 / 20) train loss: 2.304743; val loss: 2.304677\n",
      "(Epoch 5 / 20) train loss: 2.304606; val loss: 2.304541\n",
      "(Epoch 6 / 20) train loss: 2.304463; val loss: 2.304387\n",
      "(Epoch 7 / 20) train loss: 2.304294; val loss: 2.304178\n",
      "(Epoch 8 / 20) train loss: 2.304039; val loss: 2.303786\n",
      "(Epoch 9 / 20) train loss: 2.303476; val loss: 2.302682\n",
      "(Epoch 10 / 20) train loss: 2.301720; val loss: 2.298766\n",
      "(Epoch 11 / 20) train loss: 2.295700; val loss: 2.285595\n",
      "(Epoch 12 / 20) train loss: 2.279227; val loss: 2.255884\n",
      "(Epoch 13 / 20) train loss: 2.250053; val loss: 2.215032\n",
      "(Epoch 14 / 20) train loss: 2.217288; val loss: 2.178082\n",
      "(Epoch 15 / 20) train loss: 2.190510; val loss: 2.151486\n",
      "(Epoch 16 / 20) train loss: 2.171438; val loss: 2.132540\n",
      "(Epoch 17 / 20) train loss: 2.157477; val loss: 2.120302\n",
      "(Epoch 18 / 20) train loss: 2.147013; val loss: 2.111000\n",
      "(Epoch 19 / 20) train loss: 2.138077; val loss: 2.103433\n",
      "(Epoch 20 / 20) train loss: 2.130610; val loss: 2.097772\n",
      "\n",
      "Evaluating Config #3 [of 100]:\n",
      " {'learning_rate': 1.925336271285624e-06, 'reg': 5.146868907611237e-05, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 391, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 3}\n",
      "(Epoch 1 / 20) train loss: 2.302656; val loss: 2.302655\n",
      "(Epoch 2 / 20) train loss: 2.302655; val loss: 2.302649\n",
      "(Epoch 3 / 20) train loss: 2.302647; val loss: 2.302643\n",
      "(Epoch 4 / 20) train loss: 2.302638; val loss: 2.302636\n",
      "(Epoch 5 / 20) train loss: 2.302628; val loss: 2.302626\n",
      "(Epoch 6 / 20) train loss: 2.302615; val loss: 2.302611\n",
      "(Epoch 7 / 20) train loss: 2.302597; val loss: 2.302588\n",
      "(Epoch 8 / 20) train loss: 2.302569; val loss: 2.302550\n",
      "(Epoch 9 / 20) train loss: 2.302526; val loss: 2.302485\n",
      "(Epoch 10 / 20) train loss: 2.302451; val loss: 2.302365\n",
      "(Epoch 11 / 20) train loss: 2.302313; val loss: 2.302126\n",
      "(Epoch 12 / 20) train loss: 2.302036; val loss: 2.301635\n",
      "(Epoch 13 / 20) train loss: 2.301453; val loss: 2.300569\n",
      "(Epoch 14 / 20) train loss: 2.300199; val loss: 2.298158\n",
      "(Epoch 15 / 20) train loss: 2.297578; val loss: 2.293303\n",
      "(Epoch 16 / 20) train loss: 2.292483; val loss: 2.284346\n",
      "(Epoch 17 / 20) train loss: 2.283790; val loss: 2.270193\n",
      "(Epoch 18 / 20) train loss: 2.271013; val loss: 2.251028\n",
      "(Epoch 19 / 20) train loss: 2.255151; val loss: 2.229118\n",
      "(Epoch 20 / 20) train loss: 2.237576; val loss: 2.206729\n",
      "\n",
      "Evaluating Config #4 [of 100]:\n",
      " {'learning_rate': 0.00433779571539933, 'reg': 0.0003868582058842059, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 339, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 3}\n",
      "(Epoch 1 / 20) train loss: 2.303033; val loss: 2.303034\n",
      "(Epoch 2 / 20) train loss: 3.475387; val loss: 4.448056\n",
      "(Epoch 3 / 20) train loss: 3.894421; val loss: 3.720305\n",
      "(Epoch 4 / 20) train loss: 4.332571; val loss: 5.320740\n",
      "(Epoch 5 / 20) train loss: 4.801328; val loss: 4.673051\n",
      "(Epoch 6 / 20) train loss: 4.606474; val loss: 5.317012\n",
      "Stopping early at epoch 6!\n",
      "\n",
      "Evaluating Config #5 [of 100]:\n",
      " {'learning_rate': 0.00031081143773888635, 'reg': 7.030637664608416e-06, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 319, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 2}\n",
      "(Epoch 1 / 20) train loss: 2.302553; val loss: 2.302616\n",
      "(Epoch 2 / 20) train loss: 2.076350; val loss: 1.820383\n",
      "(Epoch 3 / 20) train loss: 1.822530; val loss: 1.894325\n",
      "(Epoch 4 / 20) train loss: 1.561774; val loss: 1.974400\n",
      "(Epoch 5 / 20) train loss: 1.351227; val loss: 1.936555\n",
      "(Epoch 6 / 20) train loss: 1.075790; val loss: 1.955719\n",
      "(Epoch 7 / 20) train loss: 0.843955; val loss: 2.041097\n",
      "Stopping early at epoch 7!\n",
      "\n",
      "Evaluating Config #6 [of 100]:\n",
      " {'learning_rate': 0.00042982512519484467, 'reg': 1.8572852917632522e-05, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 136, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 2}\n",
      "(Epoch 1 / 20) train loss: 2.302559; val loss: 2.302577\n",
      "(Epoch 2 / 20) train loss: 2.097767; val loss: 1.859283\n",
      "(Epoch 3 / 20) train loss: 1.790052; val loss: 1.900791\n",
      "(Epoch 4 / 20) train loss: 1.588077; val loss: 1.944164\n",
      "(Epoch 5 / 20) train loss: 1.388032; val loss: 1.927890\n",
      "(Epoch 6 / 20) train loss: 1.168632; val loss: 1.998861\n",
      "(Epoch 7 / 20) train loss: 0.953614; val loss: 2.094226\n",
      "Stopping early at epoch 7!\n",
      "\n",
      "Evaluating Config #7 [of 100]:\n",
      " {'learning_rate': 0.0002765063194880146, 'reg': 8.254301974993412e-05, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 202, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 3}\n",
      "(Epoch 1 / 20) train loss: 2.302640; val loss: 2.302641\n",
      "(Epoch 2 / 20) train loss: 2.208659; val loss: 2.070590\n",
      "(Epoch 3 / 20) train loss: 2.013162; val loss: 1.977729\n",
      "(Epoch 4 / 20) train loss: 1.855758; val loss: 1.904436\n",
      "(Epoch 5 / 20) train loss: 1.721670; val loss: 1.941223\n",
      "(Epoch 6 / 20) train loss: 1.579552; val loss: 1.826131\n",
      "(Epoch 7 / 20) train loss: 1.504790; val loss: 1.897187\n",
      "(Epoch 8 / 20) train loss: 1.340238; val loss: 2.066768\n",
      "(Epoch 9 / 20) train loss: 1.211365; val loss: 2.284206\n",
      "(Epoch 10 / 20) train loss: 1.066142; val loss: 2.340861\n",
      "(Epoch 11 / 20) train loss: 0.950005; val loss: 2.706098\n",
      "Stopping early at epoch 11!\n",
      "\n",
      "Evaluating Config #8 [of 100]:\n",
      " {'learning_rate': 0.0045696046119018795, 'reg': 0.0041908226283385785, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 152, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 2}\n",
      "(Epoch 1 / 20) train loss: 2.304554; val loss: 2.304531\n",
      "(Epoch 2 / 20) train loss: 6.828124; val loss: 8.151636\n",
      "(Epoch 3 / 20) train loss: 7.753861; val loss: 7.387878\n",
      "(Epoch 4 / 20) train loss: 6.821341; val loss: 5.995461\n",
      "(Epoch 5 / 20) train loss: 6.596184; val loss: 6.027353\n",
      "(Epoch 6 / 20) train loss: 5.874492; val loss: 5.920414\n",
      "Stopping early at epoch 6!\n",
      "\n",
      "Evaluating Config #9 [of 100]:\n",
      " {'learning_rate': 0.0006404570796767062, 'reg': 0.005186888666820398, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 393, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 2}\n",
      "(Epoch 1 / 20) train loss: 2.308829; val loss: 2.308860\n",
      "(Epoch 2 / 20) train loss: 2.324298; val loss: 2.397768\n",
      "(Epoch 3 / 20) train loss: 2.239608; val loss: 2.307406\n",
      "(Epoch 4 / 20) train loss: 2.200940; val loss: 2.302901\n",
      "(Epoch 5 / 20) train loss: 2.135665; val loss: 2.278666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 6 / 20) train loss: 2.083900; val loss: 2.382597\n",
      "(Epoch 7 / 20) train loss: 1.963431; val loss: 2.458617\n",
      "(Epoch 8 / 20) train loss: 1.846910; val loss: 2.543466\n",
      "(Epoch 9 / 20) train loss: 1.854641; val loss: 2.656204\n",
      "(Epoch 10 / 20) train loss: 1.717401; val loss: 2.699555\n",
      "Stopping early at epoch 10!\n",
      "\n",
      "Evaluating Config #10 [of 100]:\n",
      " {'learning_rate': 0.007243157898396276, 'reg': 5.612779291674538e-05, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 288, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 3}\n",
      "(Epoch 1 / 20) train loss: 2.302640; val loss: 2.302641\n",
      "(Epoch 2 / 20) train loss: 5.714437; val loss: 7.889247\n",
      "(Epoch 3 / 20) train loss: 11.555013; val loss: 8.000943\n",
      "(Epoch 4 / 20) train loss: 10.917157; val loss: 9.216508\n",
      "(Epoch 5 / 20) train loss: 14.110815; val loss: 15.275675\n",
      "(Epoch 6 / 20) train loss: 16.556736; val loss: 15.219450\n",
      "Stopping early at epoch 6!\n",
      "\n",
      "Evaluating Config #11 [of 100]:\n",
      " {'learning_rate': 1.9859509907137955e-06, 'reg': 8.437004694129963e-06, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 174, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 3}\n",
      "(Epoch 1 / 20) train loss: 2.302590; val loss: 2.302591\n",
      "(Epoch 2 / 20) train loss: 2.302590; val loss: 2.302589\n",
      "(Epoch 3 / 20) train loss: 2.302586; val loss: 2.302587\n",
      "(Epoch 4 / 20) train loss: 2.302582; val loss: 2.302585\n",
      "(Epoch 5 / 20) train loss: 2.302579; val loss: 2.302582\n",
      "(Epoch 6 / 20) train loss: 2.302575; val loss: 2.302581\n",
      "(Epoch 7 / 20) train loss: 2.302570; val loss: 2.302577\n",
      "(Epoch 8 / 20) train loss: 2.302565; val loss: 2.302573\n",
      "(Epoch 9 / 20) train loss: 2.302560; val loss: 2.302568\n",
      "(Epoch 10 / 20) train loss: 2.302552; val loss: 2.302562\n",
      "(Epoch 11 / 20) train loss: 2.302543; val loss: 2.302552\n",
      "(Epoch 12 / 20) train loss: 2.302531; val loss: 2.302538\n",
      "(Epoch 13 / 20) train loss: 2.302513; val loss: 2.302516\n",
      "(Epoch 14 / 20) train loss: 2.302488; val loss: 2.302483\n",
      "(Epoch 15 / 20) train loss: 2.302449; val loss: 2.302428\n",
      "(Epoch 16 / 20) train loss: 2.302385; val loss: 2.302333\n",
      "(Epoch 17 / 20) train loss: 2.302276; val loss: 2.302165\n",
      "(Epoch 18 / 20) train loss: 2.302086; val loss: 2.301877\n",
      "(Epoch 19 / 20) train loss: 2.301750; val loss: 2.301327\n",
      "(Epoch 20 / 20) train loss: 2.301151; val loss: 2.300396\n",
      "\n",
      "Evaluating Config #12 [of 100]:\n",
      " {'learning_rate': 4.4228364062856634e-05, 'reg': 3.817432401156144e-05, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 368, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 2}\n",
      "(Epoch 1 / 20) train loss: 2.302575; val loss: 2.302566\n",
      "(Epoch 2 / 20) train loss: 2.211252; val loss: 2.046186\n",
      "(Epoch 3 / 20) train loss: 1.989432; val loss: 1.907631\n",
      "(Epoch 4 / 20) train loss: 1.858306; val loss: 1.843417\n",
      "(Epoch 5 / 20) train loss: 1.769025; val loss: 1.811649\n",
      "(Epoch 6 / 20) train loss: 1.695519; val loss: 1.789995\n",
      "(Epoch 7 / 20) train loss: 1.630290; val loss: 1.772952\n",
      "(Epoch 8 / 20) train loss: 1.574248; val loss: 1.797143\n",
      "(Epoch 9 / 20) train loss: 1.515676; val loss: 1.773747\n",
      "(Epoch 10 / 20) train loss: 1.459982; val loss: 1.789303\n",
      "(Epoch 11 / 20) train loss: 1.400274; val loss: 1.798130\n",
      "(Epoch 12 / 20) train loss: 1.343692; val loss: 1.794982\n",
      "Stopping early at epoch 12!\n",
      "\n",
      "Evaluating Config #13 [of 100]:\n",
      " {'learning_rate': 7.219847115069291e-06, 'reg': 0.003935350358610014, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 223, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 4}\n",
      "(Epoch 1 / 20) train loss: 2.305682; val loss: 2.305682\n",
      "(Epoch 2 / 20) train loss: 2.305322; val loss: 2.304985\n",
      "(Epoch 3 / 20) train loss: 2.304696; val loss: 2.304437\n",
      "(Epoch 4 / 20) train loss: 2.304207; val loss: 2.304016\n",
      "(Epoch 5 / 20) train loss: 2.303825; val loss: 2.303684\n",
      "(Epoch 6 / 20) train loss: 2.303527; val loss: 2.303426\n",
      "(Epoch 7 / 20) train loss: 2.303293; val loss: 2.303229\n",
      "(Epoch 8 / 20) train loss: 2.303111; val loss: 2.303072\n",
      "(Epoch 9 / 20) train loss: 2.302970; val loss: 2.302953\n",
      "(Epoch 10 / 20) train loss: 2.302858; val loss: 2.302860\n",
      "(Epoch 11 / 20) train loss: 2.302770; val loss: 2.302789\n",
      "(Epoch 12 / 20) train loss: 2.302701; val loss: 2.302734\n",
      "(Epoch 13 / 20) train loss: 2.302646; val loss: 2.302689\n",
      "(Epoch 14 / 20) train loss: 2.302601; val loss: 2.302657\n",
      "(Epoch 15 / 20) train loss: 2.302565; val loss: 2.302629\n",
      "(Epoch 16 / 20) train loss: 2.302535; val loss: 2.302609\n",
      "(Epoch 17 / 20) train loss: 2.302510; val loss: 2.302592\n",
      "(Epoch 18 / 20) train loss: 2.302491; val loss: 2.302579\n",
      "(Epoch 19 / 20) train loss: 2.302472; val loss: 2.302567\n",
      "(Epoch 20 / 20) train loss: 2.302456; val loss: 2.302563\n",
      "\n",
      "Evaluating Config #14 [of 100]:\n",
      " {'learning_rate': 0.0013253345206349421, 'reg': 5.267768536723431e-06, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 310, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 2}\n",
      "(Epoch 1 / 20) train loss: 2.302525; val loss: 2.302426\n",
      "(Epoch 2 / 20) train loss: 2.603887; val loss: 2.625814\n",
      "(Epoch 3 / 20) train loss: 2.565438; val loss: 2.557701\n",
      "(Epoch 4 / 20) train loss: 2.453701; val loss: 3.124799\n",
      "(Epoch 5 / 20) train loss: 2.235079; val loss: 3.189541\n",
      "(Epoch 6 / 20) train loss: 2.076141; val loss: 3.780410\n",
      "Stopping early at epoch 6!\n",
      "\n",
      "Evaluating Config #15 [of 100]:\n",
      " {'learning_rate': 0.000671819676982086, 'reg': 0.004231045396057932, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 339, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 3}\n",
      "(Epoch 1 / 20) train loss: 2.307493; val loss: 2.307493\n",
      "(Epoch 2 / 20) train loss: 2.301002; val loss: 2.283539\n",
      "(Epoch 3 / 20) train loss: 2.236526; val loss: 2.147177\n",
      "(Epoch 4 / 20) train loss: 2.143884; val loss: 2.050593\n",
      "(Epoch 5 / 20) train loss: 2.147557; val loss: 2.184642\n",
      "(Epoch 6 / 20) train loss: 2.074508; val loss: 2.064475\n",
      "(Epoch 7 / 20) train loss: 2.040733; val loss: 2.179048\n",
      "(Epoch 8 / 20) train loss: 2.025597; val loss: 2.230138\n",
      "(Epoch 9 / 20) train loss: 2.004676; val loss: 2.206233\n",
      "Stopping early at epoch 9!\n",
      "\n",
      "Evaluating Config #16 [of 100]:\n",
      " {'learning_rate': 0.0001273470619302169, 'reg': 1.4411552864366953e-05, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 124, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 2}\n",
      "(Epoch 1 / 20) train loss: 2.302516; val loss: 2.302522\n",
      "(Epoch 2 / 20) train loss: 2.166578; val loss: 1.923724\n",
      "(Epoch 3 / 20) train loss: 1.904212; val loss: 1.846931\n",
      "(Epoch 4 / 20) train loss: 1.770022; val loss: 1.787399\n",
      "(Epoch 5 / 20) train loss: 1.669131; val loss: 1.768554\n",
      "(Epoch 6 / 20) train loss: 1.573704; val loss: 1.768672\n",
      "(Epoch 7 / 20) train loss: 1.487090; val loss: 1.834157\n",
      "(Epoch 8 / 20) train loss: 1.401934; val loss: 1.833050\n",
      "(Epoch 9 / 20) train loss: 1.313774; val loss: 1.838712\n",
      "(Epoch 10 / 20) train loss: 1.220916; val loss: 1.826670\n",
      "Stopping early at epoch 10!\n",
      "\n",
      "Evaluating Config #17 [of 100]:\n",
      " {'learning_rate': 4.789847241296964e-06, 'reg': 0.0022609370047914487, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 241, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 2}\n",
      "(Epoch 1 / 20) train loss: 2.304239; val loss: 2.304222\n",
      "(Epoch 2 / 20) train loss: 2.301730; val loss: 2.296629\n",
      "(Epoch 3 / 20) train loss: 2.289080; val loss: 2.274286\n",
      "(Epoch 4 / 20) train loss: 2.262650; val loss: 2.237571\n",
      "(Epoch 5 / 20) train loss: 2.227631; val loss: 2.196201\n",
      "(Epoch 6 / 20) train loss: 2.192766; val loss: 2.159026\n",
      "(Epoch 7 / 20) train loss: 2.162527; val loss: 2.127146\n",
      "(Epoch 8 / 20) train loss: 2.136191; val loss: 2.100450\n",
      "(Epoch 9 / 20) train loss: 2.113372; val loss: 2.079741\n",
      "(Epoch 10 / 20) train loss: 2.092824; val loss: 2.060447\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 11 / 20) train loss: 2.074149; val loss: 2.043761\n",
      "(Epoch 12 / 20) train loss: 2.056956; val loss: 2.028310\n",
      "(Epoch 13 / 20) train loss: 2.040859; val loss: 2.015443\n",
      "(Epoch 14 / 20) train loss: 2.026002; val loss: 2.004157\n",
      "(Epoch 15 / 20) train loss: 2.011850; val loss: 1.992482\n",
      "(Epoch 16 / 20) train loss: 1.998295; val loss: 1.981321\n",
      "(Epoch 17 / 20) train loss: 1.985588; val loss: 1.972149\n",
      "(Epoch 18 / 20) train loss: 1.973454; val loss: 1.963875\n",
      "(Epoch 19 / 20) train loss: 1.961892; val loss: 1.955304\n",
      "(Epoch 20 / 20) train loss: 1.950807; val loss: 1.947491\n",
      "\n",
      "Evaluating Config #18 [of 100]:\n",
      " {'learning_rate': 8.171829048500421e-05, 'reg': 0.0015659441002270262, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 243, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 2}\n",
      "(Epoch 1 / 20) train loss: 2.303754; val loss: 2.303776\n",
      "(Epoch 2 / 20) train loss: 2.165106; val loss: 1.958541\n",
      "(Epoch 3 / 20) train loss: 1.934076; val loss: 1.867716\n",
      "(Epoch 4 / 20) train loss: 1.813646; val loss: 1.834317\n",
      "(Epoch 5 / 20) train loss: 1.724442; val loss: 1.838465\n",
      "(Epoch 6 / 20) train loss: 1.647379; val loss: 1.837192\n",
      "(Epoch 7 / 20) train loss: 1.582982; val loss: 1.788313\n",
      "(Epoch 8 / 20) train loss: 1.509159; val loss: 1.806966\n",
      "(Epoch 9 / 20) train loss: 1.435706; val loss: 1.805013\n",
      "(Epoch 10 / 20) train loss: 1.372487; val loss: 1.837493\n",
      "(Epoch 11 / 20) train loss: 1.296725; val loss: 1.837955\n",
      "(Epoch 12 / 20) train loss: 1.225425; val loss: 1.818421\n",
      "Stopping early at epoch 12!\n",
      "\n",
      "Evaluating Config #19 [of 100]:\n",
      " {'learning_rate': 1.1304879003182862e-05, 'reg': 0.0003460623307333558, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 108, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 3}\n",
      "(Epoch 1 / 20) train loss: 2.302704; val loss: 2.302704\n",
      "(Epoch 2 / 20) train loss: 2.302704; val loss: 2.302694\n",
      "(Epoch 3 / 20) train loss: 2.302679; val loss: 2.302671\n",
      "(Epoch 4 / 20) train loss: 2.302620; val loss: 2.302509\n",
      "(Epoch 5 / 20) train loss: 2.301630; val loss: 2.297761\n",
      "(Epoch 6 / 20) train loss: 2.283896; val loss: 2.238679\n",
      "(Epoch 7 / 20) train loss: 2.227341; val loss: 2.160471\n",
      "(Epoch 8 / 20) train loss: 2.174293; val loss: 2.113865\n",
      "(Epoch 9 / 20) train loss: 2.144204; val loss: 2.099579\n",
      "(Epoch 10 / 20) train loss: 2.126258; val loss: 2.089707\n",
      "(Epoch 11 / 20) train loss: 2.112891; val loss: 2.081188\n",
      "(Epoch 12 / 20) train loss: 2.102502; val loss: 2.075728\n",
      "(Epoch 13 / 20) train loss: 2.092895; val loss: 2.071266\n",
      "(Epoch 14 / 20) train loss: 2.084706; val loss: 2.066289\n",
      "(Epoch 15 / 20) train loss: 2.077385; val loss: 2.062781\n",
      "(Epoch 16 / 20) train loss: 2.070312; val loss: 2.059045\n",
      "(Epoch 17 / 20) train loss: 2.064637; val loss: 2.055735\n",
      "(Epoch 18 / 20) train loss: 2.057728; val loss: 2.049087\n",
      "(Epoch 19 / 20) train loss: 2.051993; val loss: 2.046782\n",
      "(Epoch 20 / 20) train loss: 2.046968; val loss: 2.044747\n",
      "\n",
      "Evaluating Config #20 [of 100]:\n",
      " {'learning_rate': 2.3664464058309796e-06, 'reg': 7.899493491708301e-05, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 289, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 2}\n",
      "(Epoch 1 / 20) train loss: 2.302694; val loss: 2.302691\n",
      "(Epoch 2 / 20) train loss: 2.301406; val loss: 2.299498\n",
      "(Epoch 3 / 20) train loss: 2.297082; val loss: 2.293617\n",
      "(Epoch 4 / 20) train loss: 2.289524; val loss: 2.283500\n",
      "(Epoch 5 / 20) train loss: 2.277929; val loss: 2.268797\n",
      "(Epoch 6 / 20) train loss: 2.262571; val loss: 2.249940\n",
      "(Epoch 7 / 20) train loss: 2.244571; val loss: 2.229391\n",
      "(Epoch 8 / 20) train loss: 2.225304; val loss: 2.207591\n",
      "(Epoch 9 / 20) train loss: 2.206069; val loss: 2.186329\n",
      "(Epoch 10 / 20) train loss: 2.187632; val loss: 2.166406\n",
      "(Epoch 11 / 20) train loss: 2.170622; val loss: 2.147920\n",
      "(Epoch 12 / 20) train loss: 2.154852; val loss: 2.131183\n",
      "(Epoch 13 / 20) train loss: 2.140275; val loss: 2.115835\n",
      "(Epoch 14 / 20) train loss: 2.126606; val loss: 2.102003\n",
      "(Epoch 15 / 20) train loss: 2.113928; val loss: 2.089319\n",
      "(Epoch 16 / 20) train loss: 2.101887; val loss: 2.077190\n",
      "(Epoch 17 / 20) train loss: 2.090504; val loss: 2.065642\n",
      "(Epoch 18 / 20) train loss: 2.079808; val loss: 2.055689\n",
      "(Epoch 19 / 20) train loss: 2.069488; val loss: 2.045993\n",
      "(Epoch 20 / 20) train loss: 2.059636; val loss: 2.036514\n",
      "\n",
      "Evaluating Config #21 [of 100]:\n",
      " {'learning_rate': 0.0005792468351549701, 'reg': 0.001424371233793795, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 312, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 4}\n",
      "(Epoch 1 / 20) train loss: 2.304231; val loss: 2.304231\n",
      "(Epoch 2 / 20) train loss: 2.303007; val loss: 2.302409\n",
      "(Epoch 3 / 20) train loss: 2.302175; val loss: 2.302323\n",
      "(Epoch 4 / 20) train loss: 2.301744; val loss: 2.302419\n",
      "(Epoch 5 / 20) train loss: 2.301303; val loss: 2.302195\n",
      "(Epoch 6 / 20) train loss: 2.301001; val loss: 2.302385\n",
      "(Epoch 7 / 20) train loss: 2.300725; val loss: 2.302340\n",
      "(Epoch 8 / 20) train loss: 2.300512; val loss: 2.302355\n",
      "(Epoch 9 / 20) train loss: 2.300364; val loss: 2.302526\n",
      "(Epoch 10 / 20) train loss: 2.300190; val loss: 2.302805\n",
      "Stopping early at epoch 10!\n",
      "\n",
      "Evaluating Config #22 [of 100]:\n",
      " {'learning_rate': 4.51126369971843e-06, 'reg': 1.9737482347937705e-05, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 331, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 4}\n",
      "(Epoch 1 / 20) train loss: 2.302610; val loss: 2.302610\n",
      "(Epoch 2 / 20) train loss: 2.302610; val loss: 2.302607\n",
      "(Epoch 3 / 20) train loss: 2.302605; val loss: 2.302605\n",
      "(Epoch 4 / 20) train loss: 2.302599; val loss: 2.302605\n",
      "(Epoch 5 / 20) train loss: 2.302594; val loss: 2.302603\n",
      "(Epoch 6 / 20) train loss: 2.302588; val loss: 2.302601\n",
      "(Epoch 7 / 20) train loss: 2.302583; val loss: 2.302600\n",
      "(Epoch 8 / 20) train loss: 2.302578; val loss: 2.302598\n",
      "(Epoch 9 / 20) train loss: 2.302573; val loss: 2.302596\n",
      "(Epoch 10 / 20) train loss: 2.302567; val loss: 2.302594\n",
      "(Epoch 11 / 20) train loss: 2.302561; val loss: 2.302593\n",
      "(Epoch 12 / 20) train loss: 2.302556; val loss: 2.302591\n",
      "(Epoch 13 / 20) train loss: 2.302550; val loss: 2.302590\n",
      "(Epoch 14 / 20) train loss: 2.302544; val loss: 2.302587\n",
      "(Epoch 15 / 20) train loss: 2.302539; val loss: 2.302587\n",
      "(Epoch 16 / 20) train loss: 2.302533; val loss: 2.302584\n",
      "(Epoch 17 / 20) train loss: 2.302527; val loss: 2.302582\n",
      "(Epoch 18 / 20) train loss: 2.302521; val loss: 2.302581\n",
      "(Epoch 19 / 20) train loss: 2.302515; val loss: 2.302578\n",
      "(Epoch 20 / 20) train loss: 2.302508; val loss: 2.302578\n",
      "\n",
      "Evaluating Config #23 [of 100]:\n",
      " {'learning_rate': 0.003007728832773382, 'reg': 4.8315958178886936e-05, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 126, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 2}\n",
      "(Epoch 1 / 20) train loss: 2.302545; val loss: 2.302542\n",
      "(Epoch 2 / 20) train loss: 3.589668; val loss: 2.871357\n",
      "(Epoch 3 / 20) train loss: 3.442413; val loss: 4.153128\n",
      "(Epoch 4 / 20) train loss: 3.759861; val loss: 4.723046\n",
      "(Epoch 5 / 20) train loss: 3.701750; val loss: 4.500063\n",
      "(Epoch 6 / 20) train loss: 3.583825; val loss: 5.520281\n",
      "Stopping early at epoch 6!\n",
      "\n",
      "Evaluating Config #24 [of 100]:\n",
      " {'learning_rate': 0.0005946387596623997, 'reg': 0.00012999426576603345, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 180, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 4}\n",
      "(Epoch 1 / 20) train loss: 2.302666; val loss: 2.302666\n",
      "(Epoch 2 / 20) train loss: 2.302839; val loss: 2.302528\n",
      "(Epoch 3 / 20) train loss: 2.302144; val loss: 2.302584\n",
      "(Epoch 4 / 20) train loss: 2.301673; val loss: 2.302249\n",
      "(Epoch 5 / 20) train loss: 2.301231; val loss: 2.302414\n",
      "(Epoch 6 / 20) train loss: 2.300932; val loss: 2.302302\n",
      "(Epoch 7 / 20) train loss: 2.300687; val loss: 2.302561\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 8 / 20) train loss: 2.300530; val loss: 2.302814\n",
      "(Epoch 9 / 20) train loss: 2.300337; val loss: 2.302473\n",
      "Stopping early at epoch 9!\n",
      "\n",
      "Evaluating Config #25 [of 100]:\n",
      " {'learning_rate': 0.00013169947912258285, 'reg': 6.372100716765528e-06, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 271, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 4}\n",
      "(Epoch 1 / 20) train loss: 2.302591; val loss: 2.302591\n",
      "(Epoch 2 / 20) train loss: 2.302631; val loss: 2.302478\n",
      "(Epoch 3 / 20) train loss: 2.302457; val loss: 2.302412\n",
      "(Epoch 4 / 20) train loss: 2.302310; val loss: 2.302423\n",
      "(Epoch 5 / 20) train loss: 2.302161; val loss: 2.302384\n",
      "(Epoch 6 / 20) train loss: 2.301944; val loss: 2.302298\n",
      "(Epoch 7 / 20) train loss: 2.250566; val loss: 2.134527\n",
      "(Epoch 8 / 20) train loss: 2.107904; val loss: 2.024990\n",
      "(Epoch 9 / 20) train loss: 2.033673; val loss: 1.997914\n",
      "(Epoch 10 / 20) train loss: 1.981443; val loss: 1.953526\n",
      "(Epoch 11 / 20) train loss: 1.928855; val loss: 1.980831\n",
      "(Epoch 12 / 20) train loss: 1.861085; val loss: 1.924914\n",
      "(Epoch 13 / 20) train loss: 1.767421; val loss: 2.005761\n",
      "(Epoch 14 / 20) train loss: 1.683599; val loss: 1.981927\n",
      "(Epoch 15 / 20) train loss: 1.577281; val loss: 2.038999\n",
      "(Epoch 16 / 20) train loss: 1.485277; val loss: 2.244171\n",
      "(Epoch 17 / 20) train loss: 1.384671; val loss: 2.414366\n",
      "Stopping early at epoch 17!\n",
      "\n",
      "Evaluating Config #26 [of 100]:\n",
      " {'learning_rate': 1.042468813726556e-06, 'reg': 0.00033549691840080605, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 367, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 3}\n",
      "(Epoch 1 / 20) train loss: 2.303010; val loss: 2.303009\n",
      "(Epoch 2 / 20) train loss: 2.303008; val loss: 2.303005\n",
      "(Epoch 3 / 20) train loss: 2.303003; val loss: 2.303002\n",
      "(Epoch 4 / 20) train loss: 2.302998; val loss: 2.302997\n",
      "(Epoch 5 / 20) train loss: 2.302992; val loss: 2.302992\n",
      "(Epoch 6 / 20) train loss: 2.302987; val loss: 2.302988\n",
      "(Epoch 7 / 20) train loss: 2.302980; val loss: 2.302983\n",
      "(Epoch 8 / 20) train loss: 2.302974; val loss: 2.302977\n",
      "(Epoch 9 / 20) train loss: 2.302966; val loss: 2.302970\n",
      "(Epoch 10 / 20) train loss: 2.302958; val loss: 2.302962\n",
      "(Epoch 11 / 20) train loss: 2.302948; val loss: 2.302951\n",
      "(Epoch 12 / 20) train loss: 2.302937; val loss: 2.302940\n",
      "(Epoch 13 / 20) train loss: 2.302923; val loss: 2.302924\n",
      "(Epoch 14 / 20) train loss: 2.302906; val loss: 2.302905\n",
      "(Epoch 15 / 20) train loss: 2.302885; val loss: 2.302880\n",
      "(Epoch 16 / 20) train loss: 2.302859; val loss: 2.302848\n",
      "(Epoch 17 / 20) train loss: 2.302824; val loss: 2.302803\n",
      "(Epoch 18 / 20) train loss: 2.302779; val loss: 2.302745\n",
      "(Epoch 19 / 20) train loss: 2.302719; val loss: 2.302663\n",
      "(Epoch 20 / 20) train loss: 2.302636; val loss: 2.302550\n",
      "\n",
      "Evaluating Config #27 [of 100]:\n",
      " {'learning_rate': 0.0006654683774889452, 'reg': 3.1668011307407637e-06, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 341, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 4}\n",
      "(Epoch 1 / 20) train loss: 2.302589; val loss: 2.302589\n",
      "(Epoch 2 / 20) train loss: 2.302711; val loss: 2.302321\n",
      "(Epoch 3 / 20) train loss: 2.300142; val loss: 2.311556\n",
      "(Epoch 4 / 20) train loss: 2.217953; val loss: 2.155722\n",
      "(Epoch 5 / 20) train loss: 2.044192; val loss: 1.920398\n",
      "(Epoch 6 / 20) train loss: 1.894684; val loss: 1.862144\n",
      "(Epoch 7 / 20) train loss: 1.742533; val loss: 2.121880\n",
      "(Epoch 8 / 20) train loss: 1.595272; val loss: 1.866418\n",
      "(Epoch 9 / 20) train loss: 1.474682; val loss: 2.017787\n",
      "(Epoch 10 / 20) train loss: 1.319167; val loss: 1.941235\n",
      "(Epoch 11 / 20) train loss: 1.248386; val loss: 2.215404\n",
      "Stopping early at epoch 11!\n",
      "\n",
      "Evaluating Config #28 [of 100]:\n",
      " {'learning_rate': 1.3550749968716512e-05, 'reg': 8.206607443946311e-05, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 232, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 4}\n",
      "(Epoch 1 / 20) train loss: 2.302653; val loss: 2.302653\n",
      "(Epoch 2 / 20) train loss: 2.302657; val loss: 2.302658\n",
      "(Epoch 3 / 20) train loss: 2.302640; val loss: 2.302652\n",
      "(Epoch 4 / 20) train loss: 2.302622; val loss: 2.302647\n",
      "(Epoch 5 / 20) train loss: 2.302607; val loss: 2.302636\n",
      "(Epoch 6 / 20) train loss: 2.302590; val loss: 2.302634\n",
      "(Epoch 7 / 20) train loss: 2.302573; val loss: 2.302626\n",
      "(Epoch 8 / 20) train loss: 2.302556; val loss: 2.302626\n",
      "(Epoch 9 / 20) train loss: 2.302540; val loss: 2.302620\n",
      "(Epoch 10 / 20) train loss: 2.302524; val loss: 2.302610\n",
      "(Epoch 11 / 20) train loss: 2.302506; val loss: 2.302607\n",
      "(Epoch 12 / 20) train loss: 2.302489; val loss: 2.302604\n",
      "(Epoch 13 / 20) train loss: 2.302468; val loss: 2.302598\n",
      "(Epoch 14 / 20) train loss: 2.302449; val loss: 2.302598\n",
      "(Epoch 15 / 20) train loss: 2.302430; val loss: 2.302591\n",
      "(Epoch 16 / 20) train loss: 2.302410; val loss: 2.302582\n",
      "(Epoch 17 / 20) train loss: 2.302392; val loss: 2.302577\n",
      "(Epoch 18 / 20) train loss: 2.302368; val loss: 2.302572\n",
      "(Epoch 19 / 20) train loss: 2.302345; val loss: 2.302562\n",
      "(Epoch 20 / 20) train loss: 2.302323; val loss: 2.302558\n",
      "\n",
      "Evaluating Config #29 [of 100]:\n",
      " {'learning_rate': 0.0007614960973599311, 'reg': 3.228617682049182e-05, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 170, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 2}\n",
      "(Epoch 1 / 20) train loss: 2.302579; val loss: 2.302524\n",
      "(Epoch 2 / 20) train loss: 2.141151; val loss: 1.933781\n",
      "(Epoch 3 / 20) train loss: 1.849408; val loss: 1.915334\n",
      "(Epoch 4 / 20) train loss: 1.644047; val loss: 1.956279\n",
      "(Epoch 5 / 20) train loss: 1.392574; val loss: 2.728013\n",
      "(Epoch 6 / 20) train loss: 1.167578; val loss: 2.606054\n",
      "(Epoch 7 / 20) train loss: 0.955462; val loss: 2.600939\n",
      "(Epoch 8 / 20) train loss: 0.742517; val loss: 2.539755\n",
      "Stopping early at epoch 8!\n",
      "\n",
      "Evaluating Config #30 [of 100]:\n",
      " {'learning_rate': 6.603389005388103e-05, 'reg': 5.424567472188712e-06, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 273, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 2}\n",
      "(Epoch 1 / 20) train loss: 2.302562; val loss: 2.302615\n",
      "(Epoch 2 / 20) train loss: 2.183760; val loss: 1.972298\n",
      "(Epoch 3 / 20) train loss: 1.932386; val loss: 1.881117\n",
      "(Epoch 4 / 20) train loss: 1.808547; val loss: 1.830182\n",
      "(Epoch 5 / 20) train loss: 1.720148; val loss: 1.790349\n",
      "(Epoch 6 / 20) train loss: 1.641933; val loss: 1.785265\n",
      "(Epoch 7 / 20) train loss: 1.569060; val loss: 1.785972\n",
      "(Epoch 8 / 20) train loss: 1.505384; val loss: 1.781512\n",
      "(Epoch 9 / 20) train loss: 1.429320; val loss: 1.789144\n",
      "(Epoch 10 / 20) train loss: 1.359399; val loss: 1.809458\n",
      "(Epoch 11 / 20) train loss: 1.291096; val loss: 1.818807\n",
      "(Epoch 12 / 20) train loss: 1.220904; val loss: 1.849980\n",
      "(Epoch 13 / 20) train loss: 1.145034; val loss: 1.842421\n",
      "Stopping early at epoch 13!\n",
      "\n",
      "Evaluating Config #31 [of 100]:\n",
      " {'learning_rate': 5.682247730928085e-06, 'reg': 0.00015003878163832, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 362, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 3}\n",
      "(Epoch 1 / 20) train loss: 2.302772; val loss: 2.302772\n",
      "(Epoch 2 / 20) train loss: 2.302767; val loss: 2.302756\n",
      "(Epoch 3 / 20) train loss: 2.302738; val loss: 2.302721\n",
      "(Epoch 4 / 20) train loss: 2.302665; val loss: 2.302563\n",
      "(Epoch 5 / 20) train loss: 2.302274; val loss: 2.301290\n",
      "(Epoch 6 / 20) train loss: 2.298192; val loss: 2.285816\n",
      "(Epoch 7 / 20) train loss: 2.270204; val loss: 2.220567\n",
      "(Epoch 8 / 20) train loss: 2.218749; val loss: 2.154460\n",
      "(Epoch 9 / 20) train loss: 2.171821; val loss: 2.115359\n",
      "(Epoch 10 / 20) train loss: 2.143248; val loss: 2.098905\n",
      "(Epoch 11 / 20) train loss: 2.124744; val loss: 2.087288\n",
      "(Epoch 12 / 20) train loss: 2.110685; val loss: 2.080037\n",
      "(Epoch 13 / 20) train loss: 2.099148; val loss: 2.072290\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 14 / 20) train loss: 2.089354; val loss: 2.064657\n",
      "(Epoch 15 / 20) train loss: 2.079357; val loss: 2.060253\n",
      "(Epoch 16 / 20) train loss: 2.070741; val loss: 2.054150\n",
      "(Epoch 17 / 20) train loss: 2.062297; val loss: 2.050010\n",
      "(Epoch 18 / 20) train loss: 2.054570; val loss: 2.045989\n",
      "(Epoch 19 / 20) train loss: 2.046522; val loss: 2.039115\n",
      "(Epoch 20 / 20) train loss: 2.038596; val loss: 2.027910\n",
      "\n",
      "Evaluating Config #32 [of 100]:\n",
      " {'learning_rate': 0.0007218728602098722, 'reg': 1.235272149082126e-05, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 133, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 4}\n",
      "(Epoch 1 / 20) train loss: 2.302591; val loss: 2.302591\n",
      "(Epoch 2 / 20) train loss: 2.302691; val loss: 2.302397\n",
      "(Epoch 3 / 20) train loss: 2.302031; val loss: 2.302281\n",
      "(Epoch 4 / 20) train loss: 2.301477; val loss: 2.302318\n",
      "(Epoch 5 / 20) train loss: 2.238343; val loss: 2.116464\n",
      "(Epoch 6 / 20) train loss: 2.098590; val loss: 1.945056\n",
      "(Epoch 7 / 20) train loss: 1.896978; val loss: 1.943511\n",
      "(Epoch 8 / 20) train loss: 1.753366; val loss: 1.899056\n",
      "(Epoch 9 / 20) train loss: 1.615051; val loss: 1.964012\n",
      "(Epoch 10 / 20) train loss: 1.503317; val loss: 2.003570\n",
      "(Epoch 11 / 20) train loss: 1.327712; val loss: 2.015441\n",
      "(Epoch 12 / 20) train loss: 1.226965; val loss: 2.275285\n",
      "(Epoch 13 / 20) train loss: 1.038143; val loss: 2.270831\n",
      "Stopping early at epoch 13!\n",
      "\n",
      "Evaluating Config #33 [of 100]:\n",
      " {'learning_rate': 1.094661349279389e-06, 'reg': 0.0024996703516445653, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 366, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 4}\n",
      "(Epoch 1 / 20) train loss: 2.306070; val loss: 2.306071\n",
      "(Epoch 2 / 20) train loss: 2.306027; val loss: 2.305983\n",
      "(Epoch 3 / 20) train loss: 2.305939; val loss: 2.305898\n",
      "(Epoch 4 / 20) train loss: 2.305854; val loss: 2.305814\n",
      "(Epoch 5 / 20) train loss: 2.305770; val loss: 2.305733\n",
      "(Epoch 6 / 20) train loss: 2.305689; val loss: 2.305654\n",
      "(Epoch 7 / 20) train loss: 2.305610; val loss: 2.305576\n",
      "(Epoch 8 / 20) train loss: 2.305532; val loss: 2.305500\n",
      "(Epoch 9 / 20) train loss: 2.305456; val loss: 2.305427\n",
      "(Epoch 10 / 20) train loss: 2.305383; val loss: 2.305354\n",
      "(Epoch 11 / 20) train loss: 2.305311; val loss: 2.305284\n",
      "(Epoch 12 / 20) train loss: 2.305240; val loss: 2.305216\n",
      "(Epoch 13 / 20) train loss: 2.305172; val loss: 2.305149\n",
      "(Epoch 14 / 20) train loss: 2.305105; val loss: 2.305084\n",
      "(Epoch 15 / 20) train loss: 2.305039; val loss: 2.305020\n",
      "(Epoch 16 / 20) train loss: 2.304976; val loss: 2.304958\n",
      "(Epoch 17 / 20) train loss: 2.304914; val loss: 2.304898\n",
      "(Epoch 18 / 20) train loss: 2.304853; val loss: 2.304838\n",
      "(Epoch 19 / 20) train loss: 2.304794; val loss: 2.304781\n",
      "(Epoch 20 / 20) train loss: 2.304736; val loss: 2.304725\n",
      "\n",
      "Evaluating Config #34 [of 100]:\n",
      " {'learning_rate': 7.17376270861252e-05, 'reg': 3.234261512362405e-05, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 219, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 2}\n",
      "(Epoch 1 / 20) train loss: 2.302608; val loss: 2.302709\n",
      "(Epoch 2 / 20) train loss: 2.189284; val loss: 1.995759\n",
      "(Epoch 3 / 20) train loss: 1.946135; val loss: 1.856989\n",
      "(Epoch 4 / 20) train loss: 1.819986; val loss: 1.802449\n",
      "(Epoch 5 / 20) train loss: 1.726106; val loss: 1.792231\n",
      "(Epoch 6 / 20) train loss: 1.648462; val loss: 1.760147\n",
      "(Epoch 7 / 20) train loss: 1.576922; val loss: 1.763867\n",
      "(Epoch 8 / 20) train loss: 1.501247; val loss: 1.777169\n",
      "(Epoch 9 / 20) train loss: 1.435245; val loss: 1.814868\n",
      "(Epoch 10 / 20) train loss: 1.366534; val loss: 1.803761\n",
      "(Epoch 11 / 20) train loss: 1.295419; val loss: 1.836447\n",
      "Stopping early at epoch 11!\n",
      "\n",
      "Evaluating Config #35 [of 100]:\n",
      " {'learning_rate': 0.0017399661020328062, 'reg': 0.0022757492636244973, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 370, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 2}\n",
      "(Epoch 1 / 20) train loss: 2.305153; val loss: 2.305110\n",
      "(Epoch 2 / 20) train loss: 3.733120; val loss: 4.230446\n",
      "(Epoch 3 / 20) train loss: 3.710932; val loss: 3.453619\n",
      "(Epoch 4 / 20) train loss: 3.801981; val loss: 3.838670\n",
      "(Epoch 5 / 20) train loss: 3.654898; val loss: 3.919864\n",
      "(Epoch 6 / 20) train loss: 3.952162; val loss: 4.408553\n",
      "Stopping early at epoch 6!\n",
      "\n",
      "Evaluating Config #36 [of 100]:\n",
      " {'learning_rate': 0.0008352069382657006, 'reg': 0.00011735036316158395, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 147, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 4}\n",
      "(Epoch 1 / 20) train loss: 2.302643; val loss: 2.302643\n",
      "(Epoch 2 / 20) train loss: 2.302913; val loss: 2.302544\n",
      "(Epoch 3 / 20) train loss: 2.279155; val loss: 2.212610\n",
      "(Epoch 4 / 20) train loss: 2.120998; val loss: 2.006218\n",
      "(Epoch 5 / 20) train loss: 1.940607; val loss: 1.828802\n",
      "(Epoch 6 / 20) train loss: 1.838155; val loss: 1.981436\n",
      "(Epoch 7 / 20) train loss: 1.739211; val loss: 1.991889\n",
      "(Epoch 8 / 20) train loss: 1.609333; val loss: 1.935281\n",
      "(Epoch 9 / 20) train loss: 1.478951; val loss: 2.272593\n",
      "(Epoch 10 / 20) train loss: 1.410355; val loss: 2.144657\n",
      "Stopping early at epoch 10!\n",
      "\n",
      "Evaluating Config #37 [of 100]:\n",
      " {'learning_rate': 0.00022124744508522187, 'reg': 2.9950751164643997e-06, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 274, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 3}\n",
      "(Epoch 1 / 20) train loss: 2.302588; val loss: 2.302587\n",
      "(Epoch 2 / 20) train loss: 2.200584; val loss: 2.054940\n",
      "(Epoch 3 / 20) train loss: 2.048018; val loss: 1.901520\n",
      "(Epoch 4 / 20) train loss: 1.855086; val loss: 1.866791\n",
      "(Epoch 5 / 20) train loss: 1.720654; val loss: 1.892235\n",
      "(Epoch 6 / 20) train loss: 1.597138; val loss: 1.892896\n",
      "(Epoch 7 / 20) train loss: 1.467285; val loss: 2.170210\n",
      "(Epoch 8 / 20) train loss: 1.348199; val loss: 1.990597\n",
      "(Epoch 9 / 20) train loss: 1.198947; val loss: 2.130934\n",
      "Stopping early at epoch 9!\n",
      "\n",
      "Evaluating Config #38 [of 100]:\n",
      " {'learning_rate': 4.275682875016489e-05, 'reg': 0.0015881712179247446, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 276, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 2}\n",
      "(Epoch 1 / 20) train loss: 2.303924; val loss: 2.303975\n",
      "(Epoch 2 / 20) train loss: 2.220328; val loss: 2.071864\n",
      "(Epoch 3 / 20) train loss: 2.027036; val loss: 1.935736\n",
      "(Epoch 4 / 20) train loss: 1.909791; val loss: 1.877682\n",
      "(Epoch 5 / 20) train loss: 1.832546; val loss: 1.844792\n",
      "(Epoch 6 / 20) train loss: 1.775165; val loss: 1.824954\n",
      "(Epoch 7 / 20) train loss: 1.728599; val loss: 1.818053\n",
      "(Epoch 8 / 20) train loss: 1.682768; val loss: 1.843206\n",
      "(Epoch 9 / 20) train loss: 1.640052; val loss: 1.815927\n",
      "(Epoch 10 / 20) train loss: 1.598826; val loss: 1.798209\n",
      "(Epoch 11 / 20) train loss: 1.558928; val loss: 1.790514\n",
      "(Epoch 12 / 20) train loss: 1.518683; val loss: 1.799609\n",
      "(Epoch 13 / 20) train loss: 1.478502; val loss: 1.824956\n",
      "(Epoch 14 / 20) train loss: 1.437917; val loss: 1.809879\n",
      "(Epoch 15 / 20) train loss: 1.393748; val loss: 1.825093\n",
      "(Epoch 16 / 20) train loss: 1.357463; val loss: 1.831668\n",
      "Stopping early at epoch 16!\n",
      "\n",
      "Evaluating Config #39 [of 100]:\n",
      " {'learning_rate': 1.0823661976005081e-06, 'reg': 0.0005401476551041352, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 296, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 4}\n",
      "(Epoch 1 / 20) train loss: 2.303173; val loss: 2.303173\n",
      "(Epoch 2 / 20) train loss: 2.303171; val loss: 2.303170\n",
      "(Epoch 3 / 20) train loss: 2.303167; val loss: 2.303166\n",
      "(Epoch 4 / 20) train loss: 2.303162; val loss: 2.303162\n",
      "(Epoch 5 / 20) train loss: 2.303158; val loss: 2.303159\n",
      "(Epoch 6 / 20) train loss: 2.303153; val loss: 2.303155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 7 / 20) train loss: 2.303148; val loss: 2.303151\n",
      "(Epoch 8 / 20) train loss: 2.303144; val loss: 2.303148\n",
      "(Epoch 9 / 20) train loss: 2.303139; val loss: 2.303144\n",
      "(Epoch 10 / 20) train loss: 2.303135; val loss: 2.303141\n",
      "(Epoch 11 / 20) train loss: 2.303131; val loss: 2.303137\n",
      "(Epoch 12 / 20) train loss: 2.303126; val loss: 2.303134\n",
      "(Epoch 13 / 20) train loss: 2.303122; val loss: 2.303130\n",
      "(Epoch 14 / 20) train loss: 2.303117; val loss: 2.303127\n",
      "(Epoch 15 / 20) train loss: 2.303113; val loss: 2.303123\n",
      "(Epoch 16 / 20) train loss: 2.303108; val loss: 2.303120\n",
      "(Epoch 17 / 20) train loss: 2.303104; val loss: 2.303116\n",
      "(Epoch 18 / 20) train loss: 2.303100; val loss: 2.303113\n",
      "(Epoch 19 / 20) train loss: 2.303095; val loss: 2.303110\n",
      "(Epoch 20 / 20) train loss: 2.303091; val loss: 2.303106\n",
      "\n",
      "Evaluating Config #40 [of 100]:\n",
      " {'learning_rate': 8.208193455737654e-06, 'reg': 4.471130201619175e-06, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 301, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 2}\n",
      "(Epoch 1 / 20) train loss: 2.302571; val loss: 2.302595\n",
      "(Epoch 2 / 20) train loss: 2.295604; val loss: 2.279026\n",
      "(Epoch 3 / 20) train loss: 2.252083; val loss: 2.203558\n",
      "(Epoch 4 / 20) train loss: 2.182550; val loss: 2.129538\n",
      "(Epoch 5 / 20) train loss: 2.124410; val loss: 2.074337\n",
      "(Epoch 6 / 20) train loss: 2.078059; val loss: 2.033422\n",
      "(Epoch 7 / 20) train loss: 2.039689; val loss: 2.003304\n",
      "(Epoch 8 / 20) train loss: 2.005660; val loss: 1.975451\n",
      "(Epoch 9 / 20) train loss: 1.975377; val loss: 1.953397\n",
      "(Epoch 10 / 20) train loss: 1.947635; val loss: 1.934573\n",
      "(Epoch 11 / 20) train loss: 1.922531; val loss: 1.917231\n",
      "(Epoch 12 / 20) train loss: 1.899526; val loss: 1.903327\n",
      "(Epoch 13 / 20) train loss: 1.878965; val loss: 1.891577\n",
      "(Epoch 14 / 20) train loss: 1.859867; val loss: 1.881287\n",
      "(Epoch 15 / 20) train loss: 1.842058; val loss: 1.869897\n",
      "(Epoch 16 / 20) train loss: 1.825815; val loss: 1.863169\n",
      "(Epoch 17 / 20) train loss: 1.810351; val loss: 1.854073\n",
      "(Epoch 18 / 20) train loss: 1.795643; val loss: 1.846828\n",
      "(Epoch 19 / 20) train loss: 1.782284; val loss: 1.844173\n",
      "(Epoch 20 / 20) train loss: 1.768422; val loss: 1.836308\n",
      "\n",
      "Evaluating Config #41 [of 100]:\n",
      " {'learning_rate': 0.002434898936936431, 'reg': 0.0008746532715514651, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 393, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 3}\n",
      "(Epoch 1 / 20) train loss: 2.303779; val loss: 2.303780\n",
      "(Epoch 2 / 20) train loss: 2.678746; val loss: 2.576681\n",
      "(Epoch 3 / 20) train loss: 2.976262; val loss: 2.775359\n",
      "(Epoch 4 / 20) train loss: 2.957012; val loss: 2.994803\n",
      "(Epoch 5 / 20) train loss: 3.089382; val loss: 2.790337\n",
      "(Epoch 6 / 20) train loss: 3.017755; val loss: 3.174916\n",
      "Stopping early at epoch 6!\n",
      "\n",
      "Evaluating Config #42 [of 100]:\n",
      " {'learning_rate': 1.1004477725799366e-05, 'reg': 0.0001452814098638946, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 246, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 3}\n",
      "(Epoch 1 / 20) train loss: 2.302704; val loss: 2.302705\n",
      "(Epoch 2 / 20) train loss: 2.302699; val loss: 2.302688\n",
      "(Epoch 3 / 20) train loss: 2.302640; val loss: 2.302545\n",
      "(Epoch 4 / 20) train loss: 2.301682; val loss: 2.297369\n",
      "(Epoch 5 / 20) train loss: 2.269742; val loss: 2.189473\n",
      "(Epoch 6 / 20) train loss: 2.180781; val loss: 2.115039\n",
      "(Epoch 7 / 20) train loss: 2.134423; val loss: 2.090581\n",
      "(Epoch 8 / 20) train loss: 2.109942; val loss: 2.076635\n",
      "(Epoch 9 / 20) train loss: 2.092262; val loss: 2.071149\n",
      "(Epoch 10 / 20) train loss: 2.077288; val loss: 2.058861\n",
      "(Epoch 11 / 20) train loss: 2.064203; val loss: 2.052341\n",
      "(Epoch 12 / 20) train loss: 2.052581; val loss: 2.047259\n",
      "(Epoch 13 / 20) train loss: 2.042109; val loss: 2.040710\n",
      "(Epoch 14 / 20) train loss: 2.031750; val loss: 2.029910\n",
      "(Epoch 15 / 20) train loss: 2.020875; val loss: 2.024009\n",
      "(Epoch 16 / 20) train loss: 2.009962; val loss: 2.012873\n",
      "(Epoch 17 / 20) train loss: 1.999063; val loss: 2.005642\n",
      "(Epoch 18 / 20) train loss: 1.986033; val loss: 1.993265\n",
      "(Epoch 19 / 20) train loss: 1.972218; val loss: 1.977979\n",
      "(Epoch 20 / 20) train loss: 1.958716; val loss: 1.982072\n",
      "\n",
      "Evaluating Config #43 [of 100]:\n",
      " {'learning_rate': 1.6609824273937208e-05, 'reg': 0.0005084328081326934, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 301, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 4}\n",
      "(Epoch 1 / 20) train loss: 2.303149; val loss: 2.303149\n",
      "(Epoch 2 / 20) train loss: 2.303132; val loss: 2.303111\n",
      "(Epoch 3 / 20) train loss: 2.303067; val loss: 2.303058\n",
      "(Epoch 4 / 20) train loss: 2.303009; val loss: 2.303013\n",
      "(Epoch 5 / 20) train loss: 2.302952; val loss: 2.302975\n",
      "(Epoch 6 / 20) train loss: 2.302900; val loss: 2.302937\n",
      "(Epoch 7 / 20) train loss: 2.302851; val loss: 2.302904\n",
      "(Epoch 8 / 20) train loss: 2.302801; val loss: 2.302869\n",
      "(Epoch 9 / 20) train loss: 2.302754; val loss: 2.302833\n",
      "(Epoch 10 / 20) train loss: 2.302711; val loss: 2.302803\n",
      "(Epoch 11 / 20) train loss: 2.302669; val loss: 2.302780\n",
      "(Epoch 12 / 20) train loss: 2.302628; val loss: 2.302764\n",
      "(Epoch 13 / 20) train loss: 2.302586; val loss: 2.302731\n",
      "(Epoch 14 / 20) train loss: 2.302545; val loss: 2.302716\n",
      "(Epoch 15 / 20) train loss: 2.302506; val loss: 2.302690\n",
      "(Epoch 16 / 20) train loss: 2.302470; val loss: 2.302678\n",
      "(Epoch 17 / 20) train loss: 2.302431; val loss: 2.302652\n",
      "(Epoch 18 / 20) train loss: 2.302390; val loss: 2.302633\n",
      "(Epoch 19 / 20) train loss: 2.302355; val loss: 2.302621\n",
      "(Epoch 20 / 20) train loss: 2.302316; val loss: 2.302597\n",
      "\n",
      "Evaluating Config #44 [of 100]:\n",
      " {'learning_rate': 0.003209793494520255, 'reg': 5.241569511267996e-05, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 283, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 4}\n",
      "(Epoch 1 / 20) train loss: 2.302639; val loss: 2.302639\n",
      "(Epoch 2 / 20) train loss: 2.307880; val loss: 2.415038\n",
      "(Epoch 3 / 20) train loss: 2.970036; val loss: 2.396835\n",
      "(Epoch 4 / 20) train loss: 2.613022; val loss: 2.390341\n",
      "(Epoch 5 / 20) train loss: 2.829129; val loss: 2.899336\n",
      "(Epoch 6 / 20) train loss: 3.461089; val loss: 3.282543\n",
      "Stopping early at epoch 6!\n",
      "\n",
      "Evaluating Config #45 [of 100]:\n",
      " {'learning_rate': 6.990793417331573e-06, 'reg': 2.915387314097167e-05, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 254, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 3}\n",
      "(Epoch 1 / 20) train loss: 2.302610; val loss: 2.302609\n",
      "(Epoch 2 / 20) train loss: 2.302608; val loss: 2.302592\n",
      "(Epoch 3 / 20) train loss: 2.302585; val loss: 2.302564\n",
      "(Epoch 4 / 20) train loss: 2.302526; val loss: 2.302423\n",
      "(Epoch 5 / 20) train loss: 2.302079; val loss: 2.300537\n",
      "(Epoch 6 / 20) train loss: 2.295136; val loss: 2.273787\n",
      "(Epoch 7 / 20) train loss: 2.255374; val loss: 2.192069\n",
      "(Epoch 8 / 20) train loss: 2.198246; val loss: 2.131504\n",
      "(Epoch 9 / 20) train loss: 2.156222; val loss: 2.105774\n",
      "(Epoch 10 / 20) train loss: 2.131800; val loss: 2.090706\n",
      "(Epoch 11 / 20) train loss: 2.114707; val loss: 2.082101\n",
      "(Epoch 12 / 20) train loss: 2.102546; val loss: 2.075742\n",
      "(Epoch 13 / 20) train loss: 2.091593; val loss: 2.068895\n",
      "(Epoch 14 / 20) train loss: 2.082616; val loss: 2.062556\n",
      "(Epoch 15 / 20) train loss: 2.073651; val loss: 2.059757\n",
      "(Epoch 16 / 20) train loss: 2.066420; val loss: 2.054427\n",
      "(Epoch 17 / 20) train loss: 2.058449; val loss: 2.048454\n",
      "(Epoch 18 / 20) train loss: 2.051897; val loss: 2.045057\n",
      "(Epoch 19 / 20) train loss: 2.045589; val loss: 2.041224\n",
      "(Epoch 20 / 20) train loss: 2.039025; val loss: 2.034361\n",
      "\n",
      "Evaluating Config #46 [of 100]:\n",
      " {'learning_rate': 4.82076320713652e-06, 'reg': 0.00011026779474028842, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 365, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 3}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1 / 20) train loss: 2.302724; val loss: 2.302723\n",
      "(Epoch 2 / 20) train loss: 2.302718; val loss: 2.302707\n",
      "(Epoch 3 / 20) train loss: 2.302693; val loss: 2.302676\n",
      "(Epoch 4 / 20) train loss: 2.302643; val loss: 2.302585\n",
      "(Epoch 5 / 20) train loss: 2.302469; val loss: 2.302114\n",
      "(Epoch 6 / 20) train loss: 2.301363; val loss: 2.298309\n",
      "(Epoch 7 / 20) train loss: 2.293041; val loss: 2.273613\n",
      "(Epoch 8 / 20) train loss: 2.261823; val loss: 2.213179\n",
      "(Epoch 9 / 20) train loss: 2.217113; val loss: 2.155450\n",
      "(Epoch 10 / 20) train loss: 2.175584; val loss: 2.122477\n",
      "(Epoch 11 / 20) train loss: 2.148188; val loss: 2.102966\n",
      "(Epoch 12 / 20) train loss: 2.130617; val loss: 2.094090\n",
      "(Epoch 13 / 20) train loss: 2.117476; val loss: 2.085230\n",
      "(Epoch 14 / 20) train loss: 2.106616; val loss: 2.078953\n",
      "(Epoch 15 / 20) train loss: 2.097391; val loss: 2.073402\n",
      "(Epoch 16 / 20) train loss: 2.089042; val loss: 2.069514\n",
      "(Epoch 17 / 20) train loss: 2.081395; val loss: 2.062867\n",
      "(Epoch 18 / 20) train loss: 2.074526; val loss: 2.057741\n",
      "(Epoch 19 / 20) train loss: 2.067707; val loss: 2.055569\n",
      "(Epoch 20 / 20) train loss: 2.060995; val loss: 2.051251\n",
      "\n",
      "Evaluating Config #47 [of 100]:\n",
      " {'learning_rate': 4.6179057934708805e-06, 'reg': 1.5186831245740338e-05, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 383, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 4}\n",
      "(Epoch 1 / 20) train loss: 2.302607; val loss: 2.302607\n",
      "(Epoch 2 / 20) train loss: 2.302608; val loss: 2.302605\n",
      "(Epoch 3 / 20) train loss: 2.302603; val loss: 2.302603\n",
      "(Epoch 4 / 20) train loss: 2.302598; val loss: 2.302600\n",
      "(Epoch 5 / 20) train loss: 2.302591; val loss: 2.302599\n",
      "(Epoch 6 / 20) train loss: 2.302587; val loss: 2.302597\n",
      "(Epoch 7 / 20) train loss: 2.302580; val loss: 2.302596\n",
      "(Epoch 8 / 20) train loss: 2.302576; val loss: 2.302594\n",
      "(Epoch 9 / 20) train loss: 2.302569; val loss: 2.302592\n",
      "(Epoch 10 / 20) train loss: 2.302564; val loss: 2.302589\n",
      "(Epoch 11 / 20) train loss: 2.302559; val loss: 2.302589\n",
      "(Epoch 12 / 20) train loss: 2.302553; val loss: 2.302588\n",
      "(Epoch 13 / 20) train loss: 2.302547; val loss: 2.302586\n",
      "(Epoch 14 / 20) train loss: 2.302541; val loss: 2.302583\n",
      "(Epoch 15 / 20) train loss: 2.302535; val loss: 2.302581\n",
      "(Epoch 16 / 20) train loss: 2.302530; val loss: 2.302579\n",
      "(Epoch 17 / 20) train loss: 2.302523; val loss: 2.302578\n",
      "(Epoch 18 / 20) train loss: 2.302518; val loss: 2.302576\n",
      "(Epoch 19 / 20) train loss: 2.302511; val loss: 2.302574\n",
      "(Epoch 20 / 20) train loss: 2.302505; val loss: 2.302572\n",
      "\n",
      "Evaluating Config #48 [of 100]:\n",
      " {'learning_rate': 4.2156057350734446e-05, 'reg': 0.009099810828558808, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 131, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 3}\n",
      "(Epoch 1 / 20) train loss: 2.306434; val loss: 2.306434\n",
      "(Epoch 2 / 20) train loss: 2.303738; val loss: 2.302685\n",
      "(Epoch 3 / 20) train loss: 2.302570; val loss: 2.302528\n",
      "(Epoch 4 / 20) train loss: 2.302488; val loss: 2.302504\n",
      "(Epoch 5 / 20) train loss: 2.302438; val loss: 2.302501\n",
      "(Epoch 6 / 20) train loss: 2.302390; val loss: 2.302491\n",
      "(Epoch 7 / 20) train loss: 2.302344; val loss: 2.302461\n",
      "(Epoch 8 / 20) train loss: 2.302298; val loss: 2.302459\n",
      "(Epoch 9 / 20) train loss: 2.302254; val loss: 2.302449\n",
      "(Epoch 10 / 20) train loss: 2.302204; val loss: 2.302446\n",
      "(Epoch 11 / 20) train loss: 2.302152; val loss: 2.302425\n",
      "(Epoch 12 / 20) train loss: 2.301886; val loss: 2.301144\n",
      "(Epoch 13 / 20) train loss: 2.261782; val loss: 2.243725\n",
      "(Epoch 14 / 20) train loss: 2.228311; val loss: 2.229729\n",
      "(Epoch 15 / 20) train loss: 2.215990; val loss: 2.225792\n",
      "(Epoch 16 / 20) train loss: 2.206773; val loss: 2.221757\n",
      "(Epoch 17 / 20) train loss: 2.201131; val loss: 2.209071\n",
      "(Epoch 18 / 20) train loss: 2.194600; val loss: 2.206946\n",
      "(Epoch 19 / 20) train loss: 2.184722; val loss: 2.211857\n",
      "(Epoch 20 / 20) train loss: 2.180886; val loss: 2.195298\n",
      "\n",
      "Evaluating Config #49 [of 100]:\n",
      " {'learning_rate': 0.0015490578467604406, 'reg': 3.977592441745721e-06, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 258, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 3}\n",
      "(Epoch 1 / 20) train loss: 2.302588; val loss: 2.302589\n",
      "(Epoch 2 / 20) train loss: 2.266976; val loss: 2.033342\n",
      "(Epoch 3 / 20) train loss: 2.044672; val loss: 1.986271\n",
      "(Epoch 4 / 20) train loss: 1.990592; val loss: 2.068034\n",
      "(Epoch 5 / 20) train loss: 1.951052; val loss: 2.138908\n",
      "(Epoch 6 / 20) train loss: 1.800847; val loss: 1.912285\n",
      "(Epoch 7 / 20) train loss: 1.629031; val loss: 1.957856\n",
      "(Epoch 8 / 20) train loss: 1.595784; val loss: 2.419303\n",
      "(Epoch 9 / 20) train loss: 1.463240; val loss: 2.300597\n",
      "(Epoch 10 / 20) train loss: 1.424625; val loss: 2.308148\n",
      "(Epoch 11 / 20) train loss: 1.370243; val loss: 2.487415\n",
      "Stopping early at epoch 11!\n",
      "\n",
      "Evaluating Config #50 [of 100]:\n",
      " {'learning_rate': 0.006329430138832522, 'reg': 0.00019312740811938458, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 160, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 4}\n",
      "(Epoch 1 / 20) train loss: 2.302691; val loss: 2.302691\n",
      "(Epoch 2 / 20) train loss: 2.793768; val loss: 2.622794\n",
      "(Epoch 3 / 20) train loss: 3.300620; val loss: 2.679298\n",
      "(Epoch 4 / 20) train loss: 2.985462; val loss: 2.907092\n",
      "(Epoch 5 / 20) train loss: 3.424708; val loss: 2.921990\n",
      "(Epoch 6 / 20) train loss: 2.958523; val loss: 3.374576\n",
      "Stopping early at epoch 6!\n",
      "\n",
      "Evaluating Config #51 [of 100]:\n",
      " {'learning_rate': 1.3484310545599625e-06, 'reg': 5.908212301543499e-06, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 200, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 3}\n",
      "(Epoch 1 / 20) train loss: 2.302589; val loss: 2.302589\n",
      "(Epoch 2 / 20) train loss: 2.302589; val loss: 2.302587\n",
      "(Epoch 3 / 20) train loss: 2.302586; val loss: 2.302586\n",
      "(Epoch 4 / 20) train loss: 2.302583; val loss: 2.302584\n",
      "(Epoch 5 / 20) train loss: 2.302580; val loss: 2.302583\n",
      "(Epoch 6 / 20) train loss: 2.302577; val loss: 2.302580\n",
      "(Epoch 7 / 20) train loss: 2.302574; val loss: 2.302579\n",
      "(Epoch 8 / 20) train loss: 2.302570; val loss: 2.302575\n",
      "(Epoch 9 / 20) train loss: 2.302566; val loss: 2.302572\n",
      "(Epoch 10 / 20) train loss: 2.302562; val loss: 2.302568\n",
      "(Epoch 11 / 20) train loss: 2.302556; val loss: 2.302563\n",
      "(Epoch 12 / 20) train loss: 2.302550; val loss: 2.302556\n",
      "(Epoch 13 / 20) train loss: 2.302542; val loss: 2.302548\n",
      "(Epoch 14 / 20) train loss: 2.302533; val loss: 2.302536\n",
      "(Epoch 15 / 20) train loss: 2.302521; val loss: 2.302522\n",
      "(Epoch 16 / 20) train loss: 2.302505; val loss: 2.302501\n",
      "(Epoch 17 / 20) train loss: 2.302484; val loss: 2.302474\n",
      "(Epoch 18 / 20) train loss: 2.302456; val loss: 2.302434\n",
      "(Epoch 19 / 20) train loss: 2.302416; val loss: 2.302377\n",
      "(Epoch 20 / 20) train loss: 2.302359; val loss: 2.302292\n",
      "\n",
      "Evaluating Config #52 [of 100]:\n",
      " {'learning_rate': 0.00020568808425310573, 'reg': 0.0019033155101209412, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 260, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 3}\n",
      "(Epoch 1 / 20) train loss: 2.304233; val loss: 2.304234\n",
      "(Epoch 2 / 20) train loss: 2.229422; val loss: 2.068616\n",
      "(Epoch 3 / 20) train loss: 2.034314; val loss: 1.914273\n",
      "(Epoch 4 / 20) train loss: 1.907130; val loss: 1.894648\n",
      "(Epoch 5 / 20) train loss: 1.829542; val loss: 1.866709\n",
      "(Epoch 6 / 20) train loss: 1.749857; val loss: 1.915008\n",
      "(Epoch 7 / 20) train loss: 1.638354; val loss: 1.942920\n",
      "(Epoch 8 / 20) train loss: 1.551514; val loss: 2.173255\n",
      "(Epoch 9 / 20) train loss: 1.466534; val loss: 2.058701\n",
      "(Epoch 10 / 20) train loss: 1.388191; val loss: 2.224485\n",
      "Stopping early at epoch 10!\n",
      "\n",
      "Evaluating Config #53 [of 100]:\n",
      " {'learning_rate': 0.002328714054963534, 'reg': 0.0011333032422828238, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 318, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 3}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1 / 20) train loss: 2.303812; val loss: 2.303813\n",
      "(Epoch 2 / 20) train loss: 2.613492; val loss: 2.561649\n",
      "(Epoch 3 / 20) train loss: 2.740994; val loss: 2.714694\n",
      "(Epoch 4 / 20) train loss: 2.695601; val loss: 2.778909\n",
      "(Epoch 5 / 20) train loss: 2.788582; val loss: 2.833293\n",
      "(Epoch 6 / 20) train loss: 2.726423; val loss: 2.722155\n",
      "Stopping early at epoch 6!\n",
      "\n",
      "Evaluating Config #54 [of 100]:\n",
      " {'learning_rate': 0.00023911299474759586, 'reg': 5.19416457978179e-06, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 202, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 2}\n",
      "(Epoch 1 / 20) train loss: 2.302571; val loss: 2.302667\n",
      "(Epoch 2 / 20) train loss: 2.095745; val loss: 1.907874\n",
      "(Epoch 3 / 20) train loss: 1.827128; val loss: 1.826773\n",
      "(Epoch 4 / 20) train loss: 1.642203; val loss: 1.842562\n",
      "(Epoch 5 / 20) train loss: 1.460513; val loss: 1.794462\n",
      "(Epoch 6 / 20) train loss: 1.271721; val loss: 1.820894\n",
      "(Epoch 7 / 20) train loss: 1.100238; val loss: 1.878485\n",
      "(Epoch 8 / 20) train loss: 0.942284; val loss: 1.931530\n",
      "(Epoch 9 / 20) train loss: 0.775816; val loss: 1.999031\n",
      "(Epoch 10 / 20) train loss: 0.636921; val loss: 2.132750\n",
      "Stopping early at epoch 10!\n",
      "\n",
      "Evaluating Config #55 [of 100]:\n",
      " {'learning_rate': 0.0029084885874250017, 'reg': 2.0813721955084414e-06, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 146, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 4}\n",
      "(Epoch 1 / 20) train loss: 2.302586; val loss: 2.302586\n",
      "(Epoch 2 / 20) train loss: 2.303196; val loss: 2.301977\n",
      "(Epoch 3 / 20) train loss: 2.301922; val loss: 2.302356\n",
      "(Epoch 4 / 20) train loss: 2.527489; val loss: 2.309094\n",
      "(Epoch 5 / 20) train loss: 2.377008; val loss: 2.542820\n",
      "(Epoch 6 / 20) train loss: 2.272034; val loss: 2.245508\n",
      "(Epoch 7 / 20) train loss: 2.119069; val loss: 2.115823\n",
      "(Epoch 8 / 20) train loss: 2.107526; val loss: 2.143163\n",
      "(Epoch 9 / 20) train loss: 2.074138; val loss: 2.005625\n",
      "(Epoch 10 / 20) train loss: 1.944388; val loss: 1.846741\n",
      "(Epoch 11 / 20) train loss: 1.903582; val loss: 1.952157\n",
      "(Epoch 12 / 20) train loss: 1.895683; val loss: 1.919429\n",
      "(Epoch 13 / 20) train loss: 1.889368; val loss: 1.898361\n",
      "(Epoch 14 / 20) train loss: 1.778508; val loss: 2.129372\n",
      "(Epoch 15 / 20) train loss: 1.714638; val loss: 2.156013\n",
      "Stopping early at epoch 15!\n",
      "\n",
      "Evaluating Config #56 [of 100]:\n",
      " {'learning_rate': 0.006313060700556456, 'reg': 0.00024902141448597314, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 176, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 3}\n",
      "(Epoch 1 / 20) train loss: 2.302728; val loss: 2.302729\n",
      "(Epoch 2 / 20) train loss: 3.445808; val loss: 3.647874\n",
      "(Epoch 3 / 20) train loss: 3.680587; val loss: 3.139356\n",
      "(Epoch 4 / 20) train loss: 4.208999; val loss: 4.113084\n",
      "(Epoch 5 / 20) train loss: 4.874077; val loss: 6.177334\n",
      "(Epoch 6 / 20) train loss: 5.651758; val loss: 4.997342\n",
      "Stopping early at epoch 6!\n",
      "\n",
      "Evaluating Config #57 [of 100]:\n",
      " {'learning_rate': 0.00022925344202267268, 'reg': 0.0033496898001715647, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 379, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 3}\n",
      "(Epoch 1 / 20) train loss: 2.306977; val loss: 2.306976\n",
      "(Epoch 2 / 20) train loss: 2.213684; val loss: 2.150127\n",
      "(Epoch 3 / 20) train loss: 2.118790; val loss: 2.056306\n",
      "(Epoch 4 / 20) train loss: 1.983110; val loss: 1.912824\n",
      "(Epoch 5 / 20) train loss: 1.899271; val loss: 2.053626\n",
      "(Epoch 6 / 20) train loss: 1.829716; val loss: 2.020383\n",
      "(Epoch 7 / 20) train loss: 1.729080; val loss: 2.065049\n",
      "(Epoch 8 / 20) train loss: 1.663342; val loss: 2.063271\n",
      "(Epoch 9 / 20) train loss: 1.594665; val loss: 2.172850\n",
      "Stopping early at epoch 9!\n",
      "\n",
      "Evaluating Config #58 [of 100]:\n",
      " {'learning_rate': 0.0008778020657480505, 'reg': 3.8765474829733596e-05, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 133, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 2}\n",
      "(Epoch 1 / 20) train loss: 2.302608; val loss: 2.302656\n",
      "(Epoch 2 / 20) train loss: 2.159960; val loss: 2.160751\n",
      "(Epoch 3 / 20) train loss: 1.889255; val loss: 2.022669\n",
      "(Epoch 4 / 20) train loss: 1.627386; val loss: 2.336239\n",
      "(Epoch 5 / 20) train loss: 1.403356; val loss: 2.414534\n",
      "(Epoch 6 / 20) train loss: 1.164455; val loss: 2.672629\n",
      "(Epoch 7 / 20) train loss: 0.911313; val loss: 2.766494\n",
      "(Epoch 8 / 20) train loss: 0.743499; val loss: 2.907974\n",
      "Stopping early at epoch 8!\n",
      "\n",
      "Evaluating Config #59 [of 100]:\n",
      " {'learning_rate': 0.0009054943154243556, 'reg': 1.652223738010034e-05, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 259, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 3}\n",
      "(Epoch 1 / 20) train loss: 2.302599; val loss: 2.302598\n",
      "(Epoch 2 / 20) train loss: 2.222043; val loss: 2.076241\n",
      "(Epoch 3 / 20) train loss: 2.002349; val loss: 2.039195\n",
      "(Epoch 4 / 20) train loss: 1.835224; val loss: 1.902583\n",
      "(Epoch 5 / 20) train loss: 1.650751; val loss: 1.940277\n",
      "(Epoch 6 / 20) train loss: 1.503794; val loss: 2.026286\n",
      "(Epoch 7 / 20) train loss: 1.311946; val loss: 1.941103\n",
      "(Epoch 8 / 20) train loss: 1.190724; val loss: 2.414448\n",
      "(Epoch 9 / 20) train loss: 1.030850; val loss: 2.375688\n",
      "Stopping early at epoch 9!\n",
      "\n",
      "Evaluating Config #60 [of 100]:\n",
      " {'learning_rate': 0.000948123538722864, 'reg': 0.0009514098259728364, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 327, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 4}\n",
      "(Epoch 1 / 20) train loss: 2.303747; val loss: 2.303747\n",
      "(Epoch 2 / 20) train loss: 2.303242; val loss: 2.271863\n",
      "(Epoch 3 / 20) train loss: 2.317463; val loss: 2.210872\n",
      "(Epoch 4 / 20) train loss: 2.349454; val loss: 2.126275\n",
      "(Epoch 5 / 20) train loss: 2.232368; val loss: 2.049150\n",
      "(Epoch 6 / 20) train loss: 2.146462; val loss: 2.120076\n",
      "(Epoch 7 / 20) train loss: 2.104080; val loss: 2.135527\n",
      "(Epoch 8 / 20) train loss: 2.059956; val loss: 2.062627\n",
      "(Epoch 9 / 20) train loss: 1.972701; val loss: 2.033842\n",
      "(Epoch 10 / 20) train loss: 1.974308; val loss: 2.436858\n",
      "(Epoch 11 / 20) train loss: 1.939511; val loss: 2.388869\n",
      "(Epoch 12 / 20) train loss: 1.901979; val loss: 2.168378\n",
      "(Epoch 13 / 20) train loss: 1.821887; val loss: 2.484427\n",
      "(Epoch 14 / 20) train loss: 1.803686; val loss: 2.374054\n",
      "Stopping early at epoch 14!\n",
      "\n",
      "Evaluating Config #61 [of 100]:\n",
      " {'learning_rate': 0.0002105824180370168, 'reg': 0.001907830647604943, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 366, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 2}\n",
      "(Epoch 1 / 20) train loss: 2.304771; val loss: 2.304693\n",
      "(Epoch 2 / 20) train loss: 2.090127; val loss: 1.913097\n",
      "(Epoch 3 / 20) train loss: 1.839397; val loss: 1.949412\n",
      "(Epoch 4 / 20) train loss: 1.678764; val loss: 1.981459\n",
      "(Epoch 5 / 20) train loss: 1.549870; val loss: 1.952477\n",
      "(Epoch 6 / 20) train loss: 1.387099; val loss: 2.178656\n",
      "(Epoch 7 / 20) train loss: 1.257391; val loss: 2.289096\n",
      "Stopping early at epoch 7!\n",
      "\n",
      "Evaluating Config #62 [of 100]:\n",
      " {'learning_rate': 1.413034351184279e-06, 'reg': 0.009676729064603012, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 214, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 2}\n",
      "(Epoch 1 / 20) train loss: 2.308963; val loss: 2.308932\n",
      "(Epoch 2 / 20) train loss: 2.308299; val loss: 2.307374\n",
      "(Epoch 3 / 20) train loss: 2.306609; val loss: 2.305323\n",
      "(Epoch 4 / 20) train loss: 2.304406; val loss: 2.302605\n",
      "(Epoch 5 / 20) train loss: 2.301418; val loss: 2.298932\n",
      "(Epoch 6 / 20) train loss: 2.297541; val loss: 2.294161\n",
      "(Epoch 7 / 20) train loss: 2.292683; val loss: 2.288432\n",
      "(Epoch 8 / 20) train loss: 2.286927; val loss: 2.281766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 9 / 20) train loss: 2.280303; val loss: 2.273962\n",
      "(Epoch 10 / 20) train loss: 2.273069; val loss: 2.265907\n",
      "(Epoch 11 / 20) train loss: 2.265341; val loss: 2.257284\n",
      "(Epoch 12 / 20) train loss: 2.257299; val loss: 2.248230\n",
      "(Epoch 13 / 20) train loss: 2.249076; val loss: 2.239288\n",
      "(Epoch 14 / 20) train loss: 2.240880; val loss: 2.230244\n",
      "(Epoch 15 / 20) train loss: 2.232706; val loss: 2.221196\n",
      "(Epoch 16 / 20) train loss: 2.224656; val loss: 2.212500\n",
      "(Epoch 17 / 20) train loss: 2.216858; val loss: 2.203835\n",
      "(Epoch 18 / 20) train loss: 2.209248; val loss: 2.195555\n",
      "(Epoch 19 / 20) train loss: 2.201918; val loss: 2.187522\n",
      "(Epoch 20 / 20) train loss: 2.194889; val loss: 2.179889\n",
      "\n",
      "Evaluating Config #63 [of 100]:\n",
      " {'learning_rate': 0.00032149557359145177, 'reg': 1.7018222749600556e-05, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 314, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 3}\n",
      "(Epoch 1 / 20) train loss: 2.302603; val loss: 2.302604\n",
      "(Epoch 2 / 20) train loss: 2.215531; val loss: 1.988700\n",
      "(Epoch 3 / 20) train loss: 1.978857; val loss: 1.866373\n",
      "(Epoch 4 / 20) train loss: 1.789839; val loss: 1.907708\n",
      "(Epoch 5 / 20) train loss: 1.639907; val loss: 1.971182\n",
      "(Epoch 6 / 20) train loss: 1.499545; val loss: 1.937584\n",
      "(Epoch 7 / 20) train loss: 1.332387; val loss: 2.108416\n",
      "(Epoch 8 / 20) train loss: 1.160479; val loss: 2.182159\n",
      "Stopping early at epoch 8!\n",
      "\n",
      "Evaluating Config #64 [of 100]:\n",
      " {'learning_rate': 4.8622278908469335e-05, 'reg': 1.427518687892912e-06, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 280, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 3}\n",
      "(Epoch 1 / 20) train loss: 2.302586; val loss: 2.302584\n",
      "(Epoch 2 / 20) train loss: 2.298622; val loss: 2.233478\n",
      "(Epoch 3 / 20) train loss: 2.148608; val loss: 2.095810\n",
      "(Epoch 4 / 20) train loss: 2.063752; val loss: 2.008780\n",
      "(Epoch 5 / 20) train loss: 1.988173; val loss: 1.953303\n",
      "(Epoch 6 / 20) train loss: 1.902989; val loss: 1.875094\n",
      "(Epoch 7 / 20) train loss: 1.840025; val loss: 1.918021\n",
      "(Epoch 8 / 20) train loss: 1.789391; val loss: 1.871626\n",
      "(Epoch 9 / 20) train loss: 1.741833; val loss: 1.877769\n",
      "(Epoch 10 / 20) train loss: 1.695576; val loss: 1.876325\n",
      "(Epoch 11 / 20) train loss: 1.649224; val loss: 1.952058\n",
      "(Epoch 12 / 20) train loss: 1.603831; val loss: 1.871938\n",
      "(Epoch 13 / 20) train loss: 1.561014; val loss: 1.903549\n",
      "Stopping early at epoch 13!\n",
      "\n",
      "Evaluating Config #65 [of 100]:\n",
      " {'learning_rate': 0.0008609801980872412, 'reg': 0.00010222058829501057, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 117, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 3}\n",
      "(Epoch 1 / 20) train loss: 2.302624; val loss: 2.302624\n",
      "(Epoch 2 / 20) train loss: 2.215441; val loss: 2.010128\n",
      "(Epoch 3 / 20) train loss: 1.972216; val loss: 1.926841\n",
      "(Epoch 4 / 20) train loss: 1.843138; val loss: 2.112444\n",
      "(Epoch 5 / 20) train loss: 1.679929; val loss: 1.847188\n",
      "(Epoch 6 / 20) train loss: 1.565110; val loss: 1.976002\n",
      "(Epoch 7 / 20) train loss: 1.427857; val loss: 2.098283\n",
      "(Epoch 8 / 20) train loss: 1.250859; val loss: 2.194401\n",
      "(Epoch 9 / 20) train loss: 1.148872; val loss: 2.519922\n",
      "(Epoch 10 / 20) train loss: 0.973421; val loss: 2.590538\n",
      "Stopping early at epoch 10!\n",
      "\n",
      "Evaluating Config #66 [of 100]:\n",
      " {'learning_rate': 0.001029609200314114, 'reg': 0.0014819544543598915, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 146, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 3}\n",
      "(Epoch 1 / 20) train loss: 2.303286; val loss: 2.303285\n",
      "(Epoch 2 / 20) train loss: 2.293102; val loss: 2.149839\n",
      "(Epoch 3 / 20) train loss: 2.124647; val loss: 2.057390\n",
      "(Epoch 4 / 20) train loss: 2.033061; val loss: 1.959430\n",
      "(Epoch 5 / 20) train loss: 1.986723; val loss: 2.042341\n",
      "(Epoch 6 / 20) train loss: 1.959454; val loss: 2.093408\n",
      "(Epoch 7 / 20) train loss: 1.884694; val loss: 2.076708\n",
      "(Epoch 8 / 20) train loss: 1.861937; val loss: 2.050138\n",
      "(Epoch 9 / 20) train loss: 1.815611; val loss: 2.195262\n",
      "Stopping early at epoch 9!\n",
      "\n",
      "Evaluating Config #67 [of 100]:\n",
      " {'learning_rate': 0.0005210833031054622, 'reg': 0.00037503280518479647, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 145, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 2}\n",
      "(Epoch 1 / 20) train loss: 2.302803; val loss: 2.302812\n",
      "(Epoch 2 / 20) train loss: 2.097923; val loss: 2.004747\n",
      "(Epoch 3 / 20) train loss: 1.814703; val loss: 1.908307\n",
      "(Epoch 4 / 20) train loss: 1.594065; val loss: 1.852572\n",
      "(Epoch 5 / 20) train loss: 1.392865; val loss: 2.199211\n",
      "(Epoch 6 / 20) train loss: 1.154792; val loss: 2.274268\n",
      "(Epoch 7 / 20) train loss: 1.014063; val loss: 2.289148\n",
      "(Epoch 8 / 20) train loss: 0.761514; val loss: 2.509898\n",
      "(Epoch 9 / 20) train loss: 0.657166; val loss: 2.603808\n",
      "Stopping early at epoch 9!\n",
      "\n",
      "Evaluating Config #68 [of 100]:\n",
      " {'learning_rate': 0.0008803017954760083, 'reg': 0.00036213730499090043, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 362, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 2}\n",
      "(Epoch 1 / 20) train loss: 2.303029; val loss: 2.303071\n",
      "(Epoch 2 / 20) train loss: 2.342338; val loss: 2.172714\n",
      "(Epoch 3 / 20) train loss: 2.136982; val loss: 2.156516\n",
      "(Epoch 4 / 20) train loss: 1.976328; val loss: 2.524679\n",
      "(Epoch 5 / 20) train loss: 1.741530; val loss: 2.528420\n",
      "(Epoch 6 / 20) train loss: 1.672396; val loss: 2.966264\n",
      "(Epoch 7 / 20) train loss: 1.456389; val loss: 3.240169\n",
      "(Epoch 8 / 20) train loss: 1.262638; val loss: 3.541520\n",
      "Stopping early at epoch 8!\n",
      "\n",
      "Evaluating Config #69 [of 100]:\n",
      " {'learning_rate': 1.8344408338803823e-06, 'reg': 1.3112351284926699e-05, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 267, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 3}\n",
      "(Epoch 1 / 20) train loss: 2.302597; val loss: 2.302597\n",
      "(Epoch 2 / 20) train loss: 2.302596; val loss: 2.302593\n",
      "(Epoch 3 / 20) train loss: 2.302591; val loss: 2.302590\n",
      "(Epoch 4 / 20) train loss: 2.302586; val loss: 2.302586\n",
      "(Epoch 5 / 20) train loss: 2.302579; val loss: 2.302579\n",
      "(Epoch 6 / 20) train loss: 2.302572; val loss: 2.302571\n",
      "(Epoch 7 / 20) train loss: 2.302562; val loss: 2.302561\n",
      "(Epoch 8 / 20) train loss: 2.302550; val loss: 2.302543\n",
      "(Epoch 9 / 20) train loss: 2.302531; val loss: 2.302518\n",
      "(Epoch 10 / 20) train loss: 2.302503; val loss: 2.302475\n",
      "(Epoch 11 / 20) train loss: 2.302458; val loss: 2.302397\n",
      "(Epoch 12 / 20) train loss: 2.302378; val loss: 2.302261\n",
      "(Epoch 13 / 20) train loss: 2.302234; val loss: 2.302000\n",
      "(Epoch 14 / 20) train loss: 2.301960; val loss: 2.301495\n",
      "(Epoch 15 / 20) train loss: 2.301436; val loss: 2.300537\n",
      "(Epoch 16 / 20) train loss: 2.300440; val loss: 2.298660\n",
      "(Epoch 17 / 20) train loss: 2.298612; val loss: 2.295357\n",
      "(Epoch 18 / 20) train loss: 2.295497; val loss: 2.289918\n",
      "(Epoch 19 / 20) train loss: 2.290599; val loss: 2.281892\n",
      "(Epoch 20 / 20) train loss: 2.283814; val loss: 2.271217\n",
      "\n",
      "Evaluating Config #70 [of 100]:\n",
      " {'learning_rate': 2.7897721487892786e-06, 'reg': 1.9007658802193757e-05, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 247, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 3}\n",
      "(Epoch 1 / 20) train loss: 2.302601; val loss: 2.302601\n",
      "(Epoch 2 / 20) train loss: 2.302600; val loss: 2.302598\n",
      "(Epoch 3 / 20) train loss: 2.302593; val loss: 2.302592\n",
      "(Epoch 4 / 20) train loss: 2.302585; val loss: 2.302586\n",
      "(Epoch 5 / 20) train loss: 2.302575; val loss: 2.302575\n",
      "(Epoch 6 / 20) train loss: 2.302561; val loss: 2.302557\n",
      "(Epoch 7 / 20) train loss: 2.302539; val loss: 2.302528\n",
      "(Epoch 8 / 20) train loss: 2.302500; val loss: 2.302466\n",
      "(Epoch 9 / 20) train loss: 2.302422; val loss: 2.302322\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 10 / 20) train loss: 2.302241; val loss: 2.301977\n",
      "(Epoch 11 / 20) train loss: 2.301778; val loss: 2.301027\n",
      "(Epoch 12 / 20) train loss: 2.300501; val loss: 2.298424\n",
      "(Epoch 13 / 20) train loss: 2.297153; val loss: 2.291612\n",
      "(Epoch 14 / 20) train loss: 2.289651; val loss: 2.278751\n",
      "(Epoch 15 / 20) train loss: 2.276580; val loss: 2.256752\n",
      "(Epoch 16 / 20) train loss: 2.258214; val loss: 2.231806\n",
      "(Epoch 17 / 20) train loss: 2.237297; val loss: 2.205696\n",
      "(Epoch 18 / 20) train loss: 2.216382; val loss: 2.180750\n",
      "(Epoch 19 / 20) train loss: 2.198146; val loss: 2.161105\n",
      "(Epoch 20 / 20) train loss: 2.182866; val loss: 2.144496\n",
      "\n",
      "Evaluating Config #71 [of 100]:\n",
      " {'learning_rate': 0.00010494572417776729, 'reg': 0.004491488116626496, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 279, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 3}\n",
      "(Epoch 1 / 20) train loss: 2.306802; val loss: 2.306802\n",
      "(Epoch 2 / 20) train loss: 2.260904; val loss: 2.092414\n",
      "(Epoch 3 / 20) train loss: 2.120142; val loss: 2.026801\n",
      "(Epoch 4 / 20) train loss: 2.070252; val loss: 2.018987\n",
      "(Epoch 5 / 20) train loss: 2.010812; val loss: 2.015263\n",
      "(Epoch 6 / 20) train loss: 1.937965; val loss: 2.013727\n",
      "(Epoch 7 / 20) train loss: 1.863440; val loss: 1.972921\n",
      "(Epoch 8 / 20) train loss: 1.817591; val loss: 1.965786\n",
      "(Epoch 9 / 20) train loss: 1.773397; val loss: 2.038384\n",
      "(Epoch 10 / 20) train loss: 1.703728; val loss: 2.087698\n",
      "(Epoch 11 / 20) train loss: 1.663285; val loss: 2.098443\n",
      "(Epoch 12 / 20) train loss: 1.604763; val loss: 2.091112\n",
      "(Epoch 13 / 20) train loss: 1.551549; val loss: 2.221449\n",
      "Stopping early at epoch 13!\n",
      "\n",
      "Evaluating Config #72 [of 100]:\n",
      " {'learning_rate': 0.007292121953324948, 'reg': 3.7348082101819644e-06, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 336, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 4}\n",
      "(Epoch 1 / 20) train loss: 2.302590; val loss: 2.302590\n",
      "(Epoch 2 / 20) train loss: nan; val loss: 5.591389\n",
      "(Epoch 3 / 20) train loss: 7.006740; val loss: 11.740844\n",
      "(Epoch 4 / 20) train loss: nan; val loss: 7.151551\n",
      "(Epoch 5 / 20) train loss: 7.834200; val loss: 15.874255\n",
      "(Epoch 6 / 20) train loss: 6.608338; val loss: 4.473269\n",
      "Stopping early at epoch 6!\n",
      "\n",
      "Evaluating Config #73 [of 100]:\n",
      " {'learning_rate': 0.0007109905278409217, 'reg': 0.0001387908812660977, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 247, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 4}\n",
      "(Epoch 1 / 20) train loss: 2.302708; val loss: 2.302708\n",
      "(Epoch 2 / 20) train loss: 2.302863; val loss: 2.302235\n",
      "(Epoch 3 / 20) train loss: 2.251400; val loss: 2.145304\n",
      "(Epoch 4 / 20) train loss: 2.099139; val loss: 1.938372\n",
      "(Epoch 5 / 20) train loss: 1.960566; val loss: 1.973368\n",
      "(Epoch 6 / 20) train loss: 1.809045; val loss: 1.957447\n",
      "(Epoch 7 / 20) train loss: 1.699117; val loss: 2.253002\n",
      "(Epoch 8 / 20) train loss: 1.617833; val loss: 1.974027\n",
      "(Epoch 9 / 20) train loss: 1.498827; val loss: 2.318130\n",
      "Stopping early at epoch 9!\n",
      "\n",
      "Evaluating Config #74 [of 100]:\n",
      " {'learning_rate': 0.00037254836091004333, 'reg': 0.000329571501228096, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 123, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 3}\n",
      "(Epoch 1 / 20) train loss: 2.302715; val loss: 2.302715\n",
      "(Epoch 2 / 20) train loss: 2.228560; val loss: 2.052052\n",
      "(Epoch 3 / 20) train loss: 2.020759; val loss: 1.933345\n",
      "(Epoch 4 / 20) train loss: 1.860818; val loss: 1.865824\n",
      "(Epoch 5 / 20) train loss: 1.739850; val loss: 1.864552\n",
      "(Epoch 6 / 20) train loss: 1.623883; val loss: 2.022426\n",
      "(Epoch 7 / 20) train loss: 1.505895; val loss: 1.840328\n",
      "(Epoch 8 / 20) train loss: 1.360525; val loss: 2.124466\n",
      "(Epoch 9 / 20) train loss: 1.242200; val loss: 2.357715\n",
      "(Epoch 10 / 20) train loss: 1.108921; val loss: 2.345265\n",
      "(Epoch 11 / 20) train loss: 0.992804; val loss: 2.335822\n",
      "(Epoch 12 / 20) train loss: 0.844089; val loss: 2.775248\n",
      "Stopping early at epoch 12!\n",
      "\n",
      "Evaluating Config #75 [of 100]:\n",
      " {'learning_rate': 0.0011621780284252375, 'reg': 3.269672179791538e-05, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 295, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 2}\n",
      "(Epoch 1 / 20) train loss: 2.302637; val loss: 2.302623\n",
      "(Epoch 2 / 20) train loss: 2.436630; val loss: 2.456994\n",
      "(Epoch 3 / 20) train loss: 2.348169; val loss: 2.575231\n",
      "(Epoch 4 / 20) train loss: 2.202668; val loss: 2.902068\n",
      "(Epoch 5 / 20) train loss: 2.002109; val loss: 2.691573\n",
      "(Epoch 6 / 20) train loss: 1.681396; val loss: 3.108310\n",
      "Stopping early at epoch 6!\n",
      "\n",
      "Evaluating Config #76 [of 100]:\n",
      " {'learning_rate': 0.0003745555079155835, 'reg': 0.0010510736224974844, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 229, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 3}\n",
      "(Epoch 1 / 20) train loss: 2.303383; val loss: 2.303383\n",
      "(Epoch 2 / 20) train loss: 2.222083; val loss: 2.005243\n",
      "(Epoch 3 / 20) train loss: 2.033680; val loss: 1.875082\n",
      "(Epoch 4 / 20) train loss: 1.890652; val loss: 1.818907\n",
      "(Epoch 5 / 20) train loss: 1.765233; val loss: 1.984211\n",
      "(Epoch 6 / 20) train loss: 1.656491; val loss: 1.887838\n",
      "(Epoch 7 / 20) train loss: 1.512798; val loss: 2.059241\n",
      "(Epoch 8 / 20) train loss: 1.412943; val loss: 2.348558\n",
      "(Epoch 9 / 20) train loss: 1.303973; val loss: 2.291242\n",
      "Stopping early at epoch 9!\n",
      "\n",
      "Evaluating Config #77 [of 100]:\n",
      " {'learning_rate': 0.00101373192333768, 'reg': 3.327344855236799e-06, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 390, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 3}\n",
      "(Epoch 1 / 20) train loss: 2.302588; val loss: 2.302589\n",
      "(Epoch 2 / 20) train loss: 2.211036; val loss: 1.988012\n",
      "(Epoch 3 / 20) train loss: 2.065125; val loss: 2.039065\n",
      "(Epoch 4 / 20) train loss: 1.934208; val loss: 1.995877\n",
      "(Epoch 5 / 20) train loss: 1.762544; val loss: 2.122818\n",
      "(Epoch 6 / 20) train loss: 1.678533; val loss: 1.998696\n",
      "(Epoch 7 / 20) train loss: 1.454547; val loss: 2.134298\n",
      "Stopping early at epoch 7!\n",
      "\n",
      "Evaluating Config #78 [of 100]:\n",
      " {'learning_rate': 0.00019894842566774954, 'reg': 0.0009344785715650276, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 156, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 3}\n",
      "(Epoch 1 / 20) train loss: 2.303059; val loss: 2.303058\n",
      "(Epoch 2 / 20) train loss: 2.233396; val loss: 2.029189\n",
      "(Epoch 3 / 20) train loss: 2.090864; val loss: 1.944514\n",
      "(Epoch 4 / 20) train loss: 1.992224; val loss: 1.896088\n",
      "(Epoch 5 / 20) train loss: 1.862866; val loss: 1.864817\n",
      "(Epoch 6 / 20) train loss: 1.766961; val loss: 1.803422\n",
      "(Epoch 7 / 20) train loss: 1.692813; val loss: 1.820821\n",
      "(Epoch 8 / 20) train loss: 1.618789; val loss: 1.920529\n",
      "(Epoch 9 / 20) train loss: 1.522749; val loss: 1.876446\n",
      "(Epoch 10 / 20) train loss: 1.445568; val loss: 1.877884\n",
      "(Epoch 11 / 20) train loss: 1.366231; val loss: 1.944041\n",
      "Stopping early at epoch 11!\n",
      "\n",
      "Evaluating Config #79 [of 100]:\n",
      " {'learning_rate': 3.0153058999266143e-05, 'reg': 0.00015698698001321128, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 157, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 3}\n",
      "(Epoch 1 / 20) train loss: 2.302665; val loss: 2.302665\n",
      "(Epoch 2 / 20) train loss: 2.302660; val loss: 2.302580\n",
      "(Epoch 3 / 20) train loss: 2.289781; val loss: 2.206239\n",
      "(Epoch 4 / 20) train loss: 2.168267; val loss: 2.108218\n",
      "(Epoch 5 / 20) train loss: 2.105968; val loss: 2.072217\n",
      "(Epoch 6 / 20) train loss: 2.072992; val loss: 2.044162\n",
      "(Epoch 7 / 20) train loss: 2.048644; val loss: 2.027551\n",
      "(Epoch 8 / 20) train loss: 2.023301; val loss: 2.008688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 9 / 20) train loss: 1.999318; val loss: 1.990633\n",
      "(Epoch 10 / 20) train loss: 1.968405; val loss: 1.951352\n",
      "(Epoch 11 / 20) train loss: 1.933153; val loss: 1.935356\n",
      "(Epoch 12 / 20) train loss: 1.901186; val loss: 1.918333\n",
      "(Epoch 13 / 20) train loss: 1.869900; val loss: 1.929658\n",
      "(Epoch 14 / 20) train loss: 1.847050; val loss: 1.917648\n",
      "(Epoch 15 / 20) train loss: 1.821101; val loss: 1.896080\n",
      "(Epoch 16 / 20) train loss: 1.797158; val loss: 1.897257\n",
      "(Epoch 17 / 20) train loss: 1.781191; val loss: 1.879465\n",
      "(Epoch 18 / 20) train loss: 1.756831; val loss: 1.947534\n",
      "(Epoch 19 / 20) train loss: 1.734238; val loss: 1.880136\n",
      "(Epoch 20 / 20) train loss: 1.718393; val loss: 1.935131\n",
      "\n",
      "Evaluating Config #80 [of 100]:\n",
      " {'learning_rate': 0.0004254128891496339, 'reg': 0.0028837997682737925, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 397, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 4}\n",
      "(Epoch 1 / 20) train loss: 2.307023; val loss: 2.307023\n",
      "(Epoch 2 / 20) train loss: 2.302952; val loss: 2.302356\n",
      "(Epoch 3 / 20) train loss: 2.302183; val loss: 2.302314\n",
      "(Epoch 4 / 20) train loss: 2.301859; val loss: 2.302383\n",
      "(Epoch 5 / 20) train loss: 2.301512; val loss: 2.302224\n",
      "(Epoch 6 / 20) train loss: 2.301226; val loss: 2.302192\n",
      "(Epoch 7 / 20) train loss: 2.301018; val loss: 2.302308\n",
      "(Epoch 8 / 20) train loss: 2.300760; val loss: 2.302431\n",
      "(Epoch 9 / 20) train loss: 2.300600; val loss: 2.302318\n",
      "(Epoch 10 / 20) train loss: 2.300489; val loss: 2.302476\n",
      "(Epoch 11 / 20) train loss: 2.300325; val loss: 2.302430\n",
      "Stopping early at epoch 11!\n",
      "\n",
      "Evaluating Config #81 [of 100]:\n",
      " {'learning_rate': 0.0005916994874154706, 'reg': 0.007534947554078817, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 277, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 2}\n",
      "(Epoch 1 / 20) train loss: 2.309102; val loss: 2.309142\n",
      "(Epoch 2 / 20) train loss: 2.267129; val loss: 2.376860\n",
      "(Epoch 3 / 20) train loss: 2.166573; val loss: 2.250534\n",
      "(Epoch 4 / 20) train loss: 2.130235; val loss: 2.177795\n",
      "(Epoch 5 / 20) train loss: 2.072475; val loss: 2.204121\n",
      "(Epoch 6 / 20) train loss: 2.002737; val loss: 2.286849\n",
      "(Epoch 7 / 20) train loss: 1.947128; val loss: 2.380302\n",
      "(Epoch 8 / 20) train loss: 1.892210; val loss: 2.279775\n",
      "(Epoch 9 / 20) train loss: 1.816142; val loss: 2.559473\n",
      "Stopping early at epoch 9!\n",
      "\n",
      "Evaluating Config #82 [of 100]:\n",
      " {'learning_rate': 0.0001776205615413553, 'reg': 0.006315205788625518, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 158, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 2}\n",
      "(Epoch 1 / 20) train loss: 2.305570; val loss: 2.305648\n",
      "(Epoch 2 / 20) train loss: 2.142411; val loss: 1.938034\n",
      "(Epoch 3 / 20) train loss: 1.927744; val loss: 1.946716\n",
      "(Epoch 4 / 20) train loss: 1.824609; val loss: 1.918303\n",
      "(Epoch 5 / 20) train loss: 1.760776; val loss: 1.911709\n",
      "(Epoch 6 / 20) train loss: 1.687923; val loss: 1.963548\n",
      "(Epoch 7 / 20) train loss: 1.617538; val loss: 1.998727\n",
      "(Epoch 8 / 20) train loss: 1.558859; val loss: 1.982390\n",
      "(Epoch 9 / 20) train loss: 1.475980; val loss: 2.042191\n",
      "(Epoch 10 / 20) train loss: 1.396786; val loss: 2.100839\n",
      "Stopping early at epoch 10!\n",
      "\n",
      "Evaluating Config #83 [of 100]:\n",
      " {'learning_rate': 0.0018638195065350824, 'reg': 0.0037470309379726554, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 180, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 2}\n",
      "(Epoch 1 / 20) train loss: 2.304674; val loss: 2.304681\n",
      "(Epoch 2 / 20) train loss: 2.874640; val loss: 2.858339\n",
      "(Epoch 3 / 20) train loss: 2.983640; val loss: 3.154694\n",
      "(Epoch 4 / 20) train loss: 2.849018; val loss: 3.046994\n",
      "(Epoch 5 / 20) train loss: 2.778530; val loss: 3.194823\n",
      "(Epoch 6 / 20) train loss: 2.700417; val loss: 2.664658\n",
      "Stopping early at epoch 6!\n",
      "\n",
      "Evaluating Config #84 [of 100]:\n",
      " {'learning_rate': 4.147160737869376e-06, 'reg': 2.407050494073857e-05, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 121, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 4}\n",
      "(Epoch 1 / 20) train loss: 2.302595; val loss: 2.302595\n",
      "(Epoch 2 / 20) train loss: 2.302595; val loss: 2.302594\n",
      "(Epoch 3 / 20) train loss: 2.302590; val loss: 2.302593\n",
      "(Epoch 4 / 20) train loss: 2.302585; val loss: 2.302594\n",
      "(Epoch 5 / 20) train loss: 2.302580; val loss: 2.302591\n",
      "(Epoch 6 / 20) train loss: 2.302576; val loss: 2.302590\n",
      "(Epoch 7 / 20) train loss: 2.302571; val loss: 2.302589\n",
      "(Epoch 8 / 20) train loss: 2.302566; val loss: 2.302587\n",
      "(Epoch 9 / 20) train loss: 2.302561; val loss: 2.302587\n",
      "(Epoch 10 / 20) train loss: 2.302556; val loss: 2.302584\n",
      "(Epoch 11 / 20) train loss: 2.302552; val loss: 2.302584\n",
      "(Epoch 12 / 20) train loss: 2.302547; val loss: 2.302582\n",
      "(Epoch 13 / 20) train loss: 2.302542; val loss: 2.302580\n",
      "(Epoch 14 / 20) train loss: 2.302538; val loss: 2.302579\n",
      "(Epoch 15 / 20) train loss: 2.302533; val loss: 2.302578\n",
      "(Epoch 16 / 20) train loss: 2.302528; val loss: 2.302576\n",
      "(Epoch 17 / 20) train loss: 2.302523; val loss: 2.302573\n",
      "(Epoch 18 / 20) train loss: 2.302518; val loss: 2.302574\n",
      "(Epoch 19 / 20) train loss: 2.302513; val loss: 2.302571\n",
      "(Epoch 20 / 20) train loss: 2.302508; val loss: 2.302570\n",
      "\n",
      "Evaluating Config #85 [of 100]:\n",
      " {'learning_rate': 0.0009301954663139979, 'reg': 0.003459022184514678, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 278, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 2}\n",
      "(Epoch 1 / 20) train loss: 2.305557; val loss: 2.305518\n",
      "(Epoch 2 / 20) train loss: 2.383981; val loss: 2.424511\n",
      "(Epoch 3 / 20) train loss: 2.405900; val loss: 2.358806\n",
      "(Epoch 4 / 20) train loss: 2.341200; val loss: 2.541547\n",
      "(Epoch 5 / 20) train loss: 2.217536; val loss: 2.600512\n",
      "(Epoch 6 / 20) train loss: 2.068080; val loss: 2.525285\n",
      "Stopping early at epoch 6!\n",
      "\n",
      "Evaluating Config #86 [of 100]:\n",
      " {'learning_rate': 4.086969359354629e-05, 'reg': 0.0004011196403510144, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 283, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 4}\n",
      "(Epoch 1 / 20) train loss: 2.302998; val loss: 2.302998\n",
      "(Epoch 2 / 20) train loss: 2.302974; val loss: 2.302930\n",
      "(Epoch 3 / 20) train loss: 2.302865; val loss: 2.302868\n",
      "(Epoch 4 / 20) train loss: 2.302771; val loss: 2.302806\n",
      "(Epoch 5 / 20) train loss: 2.302679; val loss: 2.302752\n",
      "(Epoch 6 / 20) train loss: 2.302597; val loss: 2.302715\n",
      "(Epoch 7 / 20) train loss: 2.302509; val loss: 2.302665\n",
      "(Epoch 8 / 20) train loss: 2.302424; val loss: 2.302650\n",
      "(Epoch 9 / 20) train loss: 2.302340; val loss: 2.302597\n",
      "(Epoch 10 / 20) train loss: 2.302250; val loss: 2.302563\n",
      "(Epoch 11 / 20) train loss: 2.302171; val loss: 2.302538\n",
      "(Epoch 12 / 20) train loss: 2.302095; val loss: 2.302528\n",
      "(Epoch 13 / 20) train loss: 2.302001; val loss: 2.302475\n",
      "(Epoch 14 / 20) train loss: 2.301914; val loss: 2.302452\n",
      "(Epoch 15 / 20) train loss: 2.301824; val loss: 2.302434\n",
      "(Epoch 16 / 20) train loss: 2.301742; val loss: 2.302410\n",
      "(Epoch 17 / 20) train loss: 2.301648; val loss: 2.302388\n",
      "(Epoch 18 / 20) train loss: 2.301554; val loss: 2.302399\n",
      "(Epoch 19 / 20) train loss: 2.301458; val loss: 2.302370\n",
      "(Epoch 20 / 20) train loss: 2.300619; val loss: 2.300426\n",
      "\n",
      "Evaluating Config #87 [of 100]:\n",
      " {'learning_rate': 7.024967337672036e-05, 'reg': 1.4402117092079504e-05, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 205, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 2}\n",
      "(Epoch 1 / 20) train loss: 2.302584; val loss: 2.302552\n",
      "(Epoch 2 / 20) train loss: 2.182374; val loss: 2.006467\n",
      "(Epoch 3 / 20) train loss: 1.957636; val loss: 1.884757\n",
      "(Epoch 4 / 20) train loss: 1.837875; val loss: 1.834026\n",
      "(Epoch 5 / 20) train loss: 1.748451; val loss: 1.806045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 6 / 20) train loss: 1.682007; val loss: 1.803250\n",
      "(Epoch 7 / 20) train loss: 1.615451; val loss: 1.758609\n",
      "(Epoch 8 / 20) train loss: 1.546253; val loss: 1.815200\n",
      "(Epoch 9 / 20) train loss: 1.482058; val loss: 1.770498\n",
      "(Epoch 10 / 20) train loss: 1.413099; val loss: 1.798895\n",
      "(Epoch 11 / 20) train loss: 1.350937; val loss: 1.806842\n",
      "(Epoch 12 / 20) train loss: 1.280412; val loss: 1.813815\n",
      "Stopping early at epoch 12!\n",
      "\n",
      "Evaluating Config #88 [of 100]:\n",
      " {'learning_rate': 0.007134220583603247, 'reg': 0.005488934951150825, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 226, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 2}\n",
      "(Epoch 1 / 20) train loss: 2.306420; val loss: 2.306457\n",
      "(Epoch 2 / 20) train loss: 24.525209; val loss: 32.777430\n",
      "(Epoch 3 / 20) train loss: 26.198728; val loss: 19.609162\n",
      "(Epoch 4 / 20) train loss: 19.357826; val loss: 15.861000\n",
      "(Epoch 5 / 20) train loss: 17.930893; val loss: 20.080055\n",
      "(Epoch 6 / 20) train loss: 23.611799; val loss: 25.999636\n",
      "Stopping early at epoch 6!\n",
      "\n",
      "Evaluating Config #89 [of 100]:\n",
      " {'learning_rate': 0.00021382790553602739, 'reg': 1.1239537028316363e-05, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 189, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 4}\n",
      "(Epoch 1 / 20) train loss: 2.302592; val loss: 2.302592\n",
      "(Epoch 2 / 20) train loss: 2.302644; val loss: 2.302594\n",
      "(Epoch 3 / 20) train loss: 2.302379; val loss: 2.302561\n",
      "(Epoch 4 / 20) train loss: 2.302155; val loss: 2.302558\n",
      "(Epoch 5 / 20) train loss: 2.301978; val loss: 2.302443\n",
      "(Epoch 6 / 20) train loss: 2.301734; val loss: 2.302457\n",
      "(Epoch 7 / 20) train loss: 2.254637; val loss: 2.068188\n",
      "(Epoch 8 / 20) train loss: 2.105338; val loss: 2.065957\n",
      "(Epoch 9 / 20) train loss: 2.039700; val loss: 1.995688\n",
      "(Epoch 10 / 20) train loss: 1.997221; val loss: 1.965651\n",
      "(Epoch 11 / 20) train loss: 1.927583; val loss: 1.989169\n",
      "(Epoch 12 / 20) train loss: 1.831143; val loss: 1.935945\n",
      "(Epoch 13 / 20) train loss: 1.741746; val loss: 1.939443\n",
      "(Epoch 14 / 20) train loss: 1.626178; val loss: 1.953426\n",
      "(Epoch 15 / 20) train loss: 1.535510; val loss: 2.093341\n",
      "(Epoch 16 / 20) train loss: 1.427121; val loss: 2.120164\n",
      "(Epoch 17 / 20) train loss: 1.307826; val loss: 2.320493\n",
      "Stopping early at epoch 17!\n",
      "\n",
      "Evaluating Config #90 [of 100]:\n",
      " {'learning_rate': 2.4449269230370073e-06, 'reg': 0.0006305076060681835, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 126, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 2}\n",
      "(Epoch 1 / 20) train loss: 2.302791; val loss: 2.302750\n",
      "(Epoch 2 / 20) train loss: 2.302116; val loss: 2.301014\n",
      "(Epoch 3 / 20) train loss: 2.299766; val loss: 2.297782\n",
      "(Epoch 4 / 20) train loss: 2.295638; val loss: 2.292179\n",
      "(Epoch 5 / 20) train loss: 2.289259; val loss: 2.283877\n",
      "(Epoch 6 / 20) train loss: 2.280592; val loss: 2.272794\n",
      "(Epoch 7 / 20) train loss: 2.269868; val loss: 2.260356\n",
      "(Epoch 8 / 20) train loss: 2.257802; val loss: 2.246414\n",
      "(Epoch 9 / 20) train loss: 2.244924; val loss: 2.231359\n",
      "(Epoch 10 / 20) train loss: 2.231889; val loss: 2.216732\n",
      "(Epoch 11 / 20) train loss: 2.219078; val loss: 2.202192\n",
      "(Epoch 12 / 20) train loss: 2.206740; val loss: 2.188852\n",
      "(Epoch 13 / 20) train loss: 2.195055; val loss: 2.175924\n",
      "(Epoch 14 / 20) train loss: 2.184076; val loss: 2.163940\n",
      "(Epoch 15 / 20) train loss: 2.173761; val loss: 2.152486\n",
      "(Epoch 16 / 20) train loss: 2.164228; val loss: 2.141947\n",
      "(Epoch 17 / 20) train loss: 2.155012; val loss: 2.132202\n",
      "(Epoch 18 / 20) train loss: 2.146290; val loss: 2.122879\n",
      "(Epoch 19 / 20) train loss: 2.138139; val loss: 2.114293\n",
      "(Epoch 20 / 20) train loss: 2.130293; val loss: 2.105892\n",
      "\n",
      "Evaluating Config #91 [of 100]:\n",
      " {'learning_rate': 0.0010808092823042519, 'reg': 0.00533485860170077, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 153, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 4}\n",
      "(Epoch 1 / 20) train loss: 2.305354; val loss: 2.305354\n",
      "(Epoch 2 / 20) train loss: 2.302754; val loss: 2.302504\n",
      "(Epoch 3 / 20) train loss: 2.301810; val loss: 2.302226\n",
      "(Epoch 4 / 20) train loss: 2.301219; val loss: 2.302198\n",
      "(Epoch 5 / 20) train loss: 2.300891; val loss: 2.302548\n",
      "(Epoch 6 / 20) train loss: 2.300554; val loss: 2.302913\n",
      "(Epoch 7 / 20) train loss: 2.300321; val loss: 2.303251\n",
      "(Epoch 8 / 20) train loss: 2.300250; val loss: 2.303490\n",
      "(Epoch 9 / 20) train loss: 2.300200; val loss: 2.303426\n",
      "Stopping early at epoch 9!\n",
      "\n",
      "Evaluating Config #92 [of 100]:\n",
      " {'learning_rate': 7.955647534463154e-05, 'reg': 0.00018678752160177737, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 322, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 3}\n",
      "(Epoch 1 / 20) train loss: 2.302790; val loss: 2.302790\n",
      "(Epoch 2 / 20) train loss: 2.260185; val loss: 2.101419\n",
      "(Epoch 3 / 20) train loss: 2.094261; val loss: 2.010250\n",
      "(Epoch 4 / 20) train loss: 1.972103; val loss: 1.962821\n",
      "(Epoch 5 / 20) train loss: 1.859740; val loss: 1.908123\n",
      "(Epoch 6 / 20) train loss: 1.767738; val loss: 1.868232\n",
      "(Epoch 7 / 20) train loss: 1.691556; val loss: 1.853148\n",
      "(Epoch 8 / 20) train loss: 1.609334; val loss: 1.924903\n",
      "(Epoch 9 / 20) train loss: 1.543829; val loss: 1.933520\n",
      "(Epoch 10 / 20) train loss: 1.476175; val loss: 1.901165\n",
      "(Epoch 11 / 20) train loss: 1.400073; val loss: 1.930682\n",
      "(Epoch 12 / 20) train loss: 1.315432; val loss: 1.943891\n",
      "Stopping early at epoch 12!\n",
      "\n",
      "Evaluating Config #93 [of 100]:\n",
      " {'learning_rate': 1.7364326385182836e-06, 'reg': 0.00014948620565077576, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 308, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 3}\n",
      "(Epoch 1 / 20) train loss: 2.302743; val loss: 2.302743\n",
      "(Epoch 2 / 20) train loss: 2.302742; val loss: 2.302739\n",
      "(Epoch 3 / 20) train loss: 2.302736; val loss: 2.302736\n",
      "(Epoch 4 / 20) train loss: 2.302731; val loss: 2.302731\n",
      "(Epoch 5 / 20) train loss: 2.302725; val loss: 2.302727\n",
      "(Epoch 6 / 20) train loss: 2.302718; val loss: 2.302720\n",
      "(Epoch 7 / 20) train loss: 2.302709; val loss: 2.302712\n",
      "(Epoch 8 / 20) train loss: 2.302699; val loss: 2.302702\n",
      "(Epoch 9 / 20) train loss: 2.302685; val loss: 2.302687\n",
      "(Epoch 10 / 20) train loss: 2.302667; val loss: 2.302665\n",
      "(Epoch 11 / 20) train loss: 2.302641; val loss: 2.302630\n",
      "(Epoch 12 / 20) train loss: 2.302601; val loss: 2.302575\n",
      "(Epoch 13 / 20) train loss: 2.302539; val loss: 2.302484\n",
      "(Epoch 14 / 20) train loss: 2.302436; val loss: 2.302321\n",
      "(Epoch 15 / 20) train loss: 2.302256; val loss: 2.302035\n",
      "(Epoch 16 / 20) train loss: 2.301934; val loss: 2.301504\n",
      "(Epoch 17 / 20) train loss: 2.301342; val loss: 2.300511\n",
      "(Epoch 18 / 20) train loss: 2.300257; val loss: 2.298669\n",
      "(Epoch 19 / 20) train loss: 2.298309; val loss: 2.295519\n",
      "(Epoch 20 / 20) train loss: 2.294969; val loss: 2.290109\n",
      "\n",
      "Evaluating Config #94 [of 100]:\n",
      " {'learning_rate': 6.142934670908655e-06, 'reg': 0.001545978187559687, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 121, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 3}\n",
      "(Epoch 1 / 20) train loss: 2.303184; val loss: 2.303184\n",
      "(Epoch 2 / 20) train loss: 2.303158; val loss: 2.303131\n",
      "(Epoch 3 / 20) train loss: 2.303099; val loss: 2.303080\n",
      "(Epoch 4 / 20) train loss: 2.303044; val loss: 2.303032\n",
      "(Epoch 5 / 20) train loss: 2.302990; val loss: 2.302981\n",
      "(Epoch 6 / 20) train loss: 2.302931; val loss: 2.302915\n",
      "(Epoch 7 / 20) train loss: 2.302837; val loss: 2.302765\n",
      "(Epoch 8 / 20) train loss: 2.302550; val loss: 2.302124\n",
      "(Epoch 9 / 20) train loss: 2.300892; val loss: 2.297978\n",
      "(Epoch 10 / 20) train loss: 2.291255; val loss: 2.277432\n",
      "(Epoch 11 / 20) train loss: 2.262236; val loss: 2.234212\n",
      "(Epoch 12 / 20) train loss: 2.222571; val loss: 2.188383\n",
      "(Epoch 13 / 20) train loss: 2.191470; val loss: 2.158994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 14 / 20) train loss: 2.171441; val loss: 2.137552\n",
      "(Epoch 15 / 20) train loss: 2.157635; val loss: 2.123324\n",
      "(Epoch 16 / 20) train loss: 2.146808; val loss: 2.112344\n",
      "(Epoch 17 / 20) train loss: 2.138170; val loss: 2.103900\n",
      "(Epoch 18 / 20) train loss: 2.130630; val loss: 2.098161\n",
      "(Epoch 19 / 20) train loss: 2.123742; val loss: 2.092642\n",
      "(Epoch 20 / 20) train loss: 2.117963; val loss: 2.088762\n",
      "\n",
      "Evaluating Config #95 [of 100]:\n",
      " {'learning_rate': 6.022924331266206e-06, 'reg': 9.70024983061797e-06, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 188, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 2}\n",
      "(Epoch 1 / 20) train loss: 2.302517; val loss: 2.302495\n",
      "(Epoch 2 / 20) train loss: 2.299893; val loss: 2.294553\n",
      "(Epoch 3 / 20) train loss: 2.284905; val loss: 2.268458\n",
      "(Epoch 4 / 20) train loss: 2.252024; val loss: 2.225370\n",
      "(Epoch 5 / 20) train loss: 2.209892; val loss: 2.177030\n",
      "(Epoch 6 / 20) train loss: 2.170356; val loss: 2.136176\n",
      "(Epoch 7 / 20) train loss: 2.137036; val loss: 2.103505\n",
      "(Epoch 8 / 20) train loss: 2.109127; val loss: 2.075241\n",
      "(Epoch 9 / 20) train loss: 2.084307; val loss: 2.050855\n",
      "(Epoch 10 / 20) train loss: 2.061852; val loss: 2.030732\n",
      "(Epoch 11 / 20) train loss: 2.041273; val loss: 2.011368\n",
      "(Epoch 12 / 20) train loss: 2.022131; val loss: 1.994444\n",
      "(Epoch 13 / 20) train loss: 2.004036; val loss: 1.979781\n",
      "(Epoch 14 / 20) train loss: 1.987079; val loss: 1.966094\n",
      "(Epoch 15 / 20) train loss: 1.970935; val loss: 1.953499\n",
      "(Epoch 16 / 20) train loss: 1.955410; val loss: 1.941457\n",
      "(Epoch 17 / 20) train loss: 1.941023; val loss: 1.931600\n",
      "(Epoch 18 / 20) train loss: 1.927023; val loss: 1.920750\n",
      "(Epoch 19 / 20) train loss: 1.914074; val loss: 1.913188\n",
      "(Epoch 20 / 20) train loss: 1.901555; val loss: 1.904493\n",
      "\n",
      "Evaluating Config #96 [of 100]:\n",
      " {'learning_rate': 7.710945419165225e-06, 'reg': 0.00019165299800265456, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 147, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 2}\n",
      "(Epoch 1 / 20) train loss: 2.302640; val loss: 2.302632\n",
      "(Epoch 2 / 20) train loss: 2.298636; val loss: 2.289254\n",
      "(Epoch 3 / 20) train loss: 2.274354; val loss: 2.246152\n",
      "(Epoch 4 / 20) train loss: 2.228810; val loss: 2.188297\n",
      "(Epoch 5 / 20) train loss: 2.181953; val loss: 2.139612\n",
      "(Epoch 6 / 20) train loss: 2.143245; val loss: 2.103241\n",
      "(Epoch 7 / 20) train loss: 2.111976; val loss: 2.073914\n",
      "(Epoch 8 / 20) train loss: 2.085381; val loss: 2.050581\n",
      "(Epoch 9 / 20) train loss: 2.062035; val loss: 2.031495\n",
      "(Epoch 10 / 20) train loss: 2.040565; val loss: 2.013942\n",
      "(Epoch 11 / 20) train loss: 2.021284; val loss: 1.998894\n",
      "(Epoch 12 / 20) train loss: 2.002830; val loss: 1.984428\n",
      "(Epoch 13 / 20) train loss: 1.985790; val loss: 1.971888\n",
      "(Epoch 14 / 20) train loss: 1.969393; val loss: 1.960684\n",
      "(Epoch 15 / 20) train loss: 1.954270; val loss: 1.948462\n",
      "(Epoch 16 / 20) train loss: 1.939107; val loss: 1.939015\n",
      "(Epoch 17 / 20) train loss: 1.925237; val loss: 1.929456\n",
      "(Epoch 18 / 20) train loss: 1.912287; val loss: 1.920904\n",
      "(Epoch 19 / 20) train loss: 1.899302; val loss: 1.913439\n",
      "(Epoch 20 / 20) train loss: 1.887207; val loss: 1.906478\n",
      "\n",
      "Evaluating Config #97 [of 100]:\n",
      " {'learning_rate': 0.00956648315770334, 'reg': 2.62582099573057e-05, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 393, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 3}\n",
      "(Epoch 1 / 20) train loss: 2.302621; val loss: 2.302621\n",
      "(Epoch 2 / 20) train loss: 22.751613; val loss: 30.070583\n",
      "(Epoch 3 / 20) train loss: nan; val loss: 24.893429\n",
      "(Epoch 4 / 20) train loss: 34.553230; val loss: 64.278625\n",
      "(Epoch 5 / 20) train loss: nan; val loss: 45.188946\n",
      "(Epoch 6 / 20) train loss: nan; val loss: 52.665807\n",
      "Stopping early at epoch 6!\n",
      "\n",
      "Evaluating Config #98 [of 100]:\n",
      " {'learning_rate': 3.6576847755853367e-06, 'reg': 0.00020180323914993514, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 250, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 4}\n",
      "(Epoch 1 / 20) train loss: 2.302766; val loss: 2.302766\n",
      "(Epoch 2 / 20) train loss: 2.302766; val loss: 2.302764\n",
      "(Epoch 3 / 20) train loss: 2.302760; val loss: 2.302761\n",
      "(Epoch 4 / 20) train loss: 2.302755; val loss: 2.302760\n",
      "(Epoch 5 / 20) train loss: 2.302749; val loss: 2.302757\n",
      "(Epoch 6 / 20) train loss: 2.302744; val loss: 2.302753\n",
      "(Epoch 7 / 20) train loss: 2.302738; val loss: 2.302751\n",
      "(Epoch 8 / 20) train loss: 2.302732; val loss: 2.302749\n",
      "(Epoch 9 / 20) train loss: 2.302727; val loss: 2.302747\n",
      "(Epoch 10 / 20) train loss: 2.302722; val loss: 2.302745\n",
      "(Epoch 11 / 20) train loss: 2.302716; val loss: 2.302743\n",
      "(Epoch 12 / 20) train loss: 2.302710; val loss: 2.302740\n",
      "(Epoch 13 / 20) train loss: 2.302705; val loss: 2.302737\n",
      "(Epoch 14 / 20) train loss: 2.302699; val loss: 2.302736\n",
      "(Epoch 15 / 20) train loss: 2.302693; val loss: 2.302732\n",
      "(Epoch 16 / 20) train loss: 2.302688; val loss: 2.302730\n",
      "(Epoch 17 / 20) train loss: 2.302683; val loss: 2.302728\n",
      "(Epoch 18 / 20) train loss: 2.302677; val loss: 2.302725\n",
      "(Epoch 19 / 20) train loss: 2.302671; val loss: 2.302723\n",
      "(Epoch 20 / 20) train loss: 2.302666; val loss: 2.302721\n",
      "\n",
      "Evaluating Config #99 [of 100]:\n",
      " {'learning_rate': 1.1731555390603147e-06, 'reg': 0.0035996593707318626, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 335, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 4}\n",
      "(Epoch 1 / 20) train loss: 2.307107; val loss: 2.307107\n",
      "(Epoch 2 / 20) train loss: 2.307023; val loss: 2.306937\n",
      "(Epoch 3 / 20) train loss: 2.306856; val loss: 2.306775\n",
      "(Epoch 4 / 20) train loss: 2.306695; val loss: 2.306618\n",
      "(Epoch 5 / 20) train loss: 2.306540; val loss: 2.306467\n",
      "(Epoch 6 / 20) train loss: 2.306390; val loss: 2.306321\n",
      "(Epoch 7 / 20) train loss: 2.306246; val loss: 2.306180\n",
      "(Epoch 8 / 20) train loss: 2.306107; val loss: 2.306045\n",
      "(Epoch 9 / 20) train loss: 2.305973; val loss: 2.305915\n",
      "(Epoch 10 / 20) train loss: 2.305844; val loss: 2.305789\n",
      "(Epoch 11 / 20) train loss: 2.305720; val loss: 2.305667\n",
      "(Epoch 12 / 20) train loss: 2.305600; val loss: 2.305550\n",
      "(Epoch 13 / 20) train loss: 2.305484; val loss: 2.305438\n",
      "(Epoch 14 / 20) train loss: 2.305373; val loss: 2.305329\n",
      "(Epoch 15 / 20) train loss: 2.305265; val loss: 2.305225\n",
      "(Epoch 16 / 20) train loss: 2.305162; val loss: 2.305125\n",
      "(Epoch 17 / 20) train loss: 2.305062; val loss: 2.305027\n",
      "(Epoch 18 / 20) train loss: 2.304966; val loss: 2.304934\n",
      "(Epoch 19 / 20) train loss: 2.304873; val loss: 2.304843\n",
      "(Epoch 20 / 20) train loss: 2.304784; val loss: 2.304757\n",
      "\n",
      "Evaluating Config #100 [of 100]:\n",
      " {'learning_rate': 7.301294138496507e-05, 'reg': 0.000959499585524809, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 150, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 2}\n",
      "(Epoch 1 / 20) train loss: 2.302977; val loss: 2.303016\n",
      "(Epoch 2 / 20) train loss: 2.199112; val loss: 2.020410\n",
      "(Epoch 3 / 20) train loss: 1.981321; val loss: 1.868395\n",
      "(Epoch 4 / 20) train loss: 1.864562; val loss: 1.841908\n",
      "(Epoch 5 / 20) train loss: 1.787193; val loss: 1.814323\n",
      "(Epoch 6 / 20) train loss: 1.723258; val loss: 1.814723\n",
      "(Epoch 7 / 20) train loss: 1.664616; val loss: 1.827883\n",
      "(Epoch 8 / 20) train loss: 1.611835; val loss: 1.789512\n",
      "(Epoch 9 / 20) train loss: 1.560255; val loss: 1.806793\n",
      "(Epoch 10 / 20) train loss: 1.507352; val loss: 1.819181\n",
      "(Epoch 11 / 20) train loss: 1.450374; val loss: 1.838500\n",
      "(Epoch 12 / 20) train loss: 1.403270; val loss: 1.828690\n",
      "(Epoch 13 / 20) train loss: 1.348461; val loss: 1.840958\n",
      "Stopping early at epoch 13!\n",
      "\n",
      "Search done. Best Val Loss = 1.758609107195931\n",
      "Best Config: {'learning_rate': 7.024967337672036e-05, 'reg': 1.4402117092079504e-05, 'loss_func': <exercise_code.networks.loss.CrossEntropyFromLogits object at 0x7ff128f085c0>, 'hidden_size': 205, 'activation': <exercise_code.networks.layer.LeakyRelu object at 0x7ff128f083c8>, 'num_layer': 2}\n"
     ]
    }
   ],
   "source": [
    "from exercise_code.networks import MyOwnNetwork\n",
    "\n",
    "best_model = ClassificationNet()\n",
    "#best_model = MyOwnNetwork()\n",
    "\n",
    "########################################################################\n",
    "# TODO:                                                                #\n",
    "# Implement your own neural network and find suitable hyperparameters  #\n",
    "# Be sure to edit the MyOwnNetwork class in the following code snippet #\n",
    "# to upload the correct model!                                         #\n",
    "########################################################################\n",
    "model_class = ClassificationNet\n",
    "\n",
    "best_model, results = random_search(\n",
    "    dataloaders['train_small'], dataloaders['val_500files'],\n",
    "    random_search_spaces = {\n",
    "        \"learning_rate\": ([1e-2, 1e-6], 'log'),\n",
    "        \"reg\": ([1e-2, 1e-6], \"log\"),\n",
    "        \"loss_func\": ([CrossEntropyFromLogits()], \"item\"),\n",
    "        \"hidden_size\": ([100, 400], \"int\"),\n",
    "        \"activation\": ([LeakyRelu()], \"item\"),\n",
    "        \"num_layer\": ([2,3,4], \"int\")\n",
    "    },\n",
    "    model_class=model_class,\n",
    "    num_search = 100, epochs=20, patience=5)\n",
    "\n",
    "########################################################################\n",
    "#                           END OF YOUR CODE                           #\n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1 / 50) train loss: 2.302607; val loss: 2.302608\n",
      "(Epoch 2 / 50) train loss: 2.124521; val loss: 1.961502\n",
      "(Epoch 3 / 50) train loss: 1.875584; val loss: 1.833637\n",
      "(Epoch 4 / 50) train loss: 1.780414; val loss: 1.764844\n",
      "(Epoch 5 / 50) train loss: 1.719131; val loss: 1.717690\n",
      "(Epoch 6 / 50) train loss: 1.672577; val loss: 1.681440\n",
      "(Epoch 7 / 50) train loss: 1.633655; val loss: 1.651996\n",
      "(Epoch 8 / 50) train loss: 1.600278; val loss: 1.625750\n",
      "(Epoch 9 / 50) train loss: 1.570323; val loss: 1.607129\n",
      "(Epoch 10 / 50) train loss: 1.544914; val loss: 1.589944\n",
      "(Epoch 11 / 50) train loss: 1.521116; val loss: 1.573654\n",
      "(Epoch 12 / 50) train loss: 1.500399; val loss: 1.559966\n",
      "(Epoch 13 / 50) train loss: 1.480239; val loss: 1.549804\n",
      "(Epoch 14 / 50) train loss: 1.463035; val loss: 1.540440\n",
      "(Epoch 15 / 50) train loss: 1.444811; val loss: 1.530252\n",
      "(Epoch 16 / 50) train loss: 1.429476; val loss: 1.522291\n",
      "(Epoch 17 / 50) train loss: 1.413981; val loss: 1.511074\n",
      "(Epoch 18 / 50) train loss: 1.398637; val loss: 1.505332\n",
      "(Epoch 19 / 50) train loss: 1.384695; val loss: 1.499966\n",
      "(Epoch 20 / 50) train loss: 1.370194; val loss: 1.492260\n",
      "(Epoch 21 / 50) train loss: 1.357114; val loss: 1.487710\n",
      "(Epoch 22 / 50) train loss: 1.344337; val loss: 1.481025\n",
      "(Epoch 23 / 50) train loss: 1.332010; val loss: 1.474386\n",
      "(Epoch 24 / 50) train loss: 1.319768; val loss: 1.469264\n",
      "(Epoch 25 / 50) train loss: 1.306578; val loss: 1.466581\n",
      "(Epoch 26 / 50) train loss: 1.294261; val loss: 1.462914\n",
      "(Epoch 27 / 50) train loss: 1.283016; val loss: 1.459331\n",
      "(Epoch 28 / 50) train loss: 1.272903; val loss: 1.452570\n",
      "(Epoch 29 / 50) train loss: 1.260784; val loss: 1.449078\n",
      "(Epoch 30 / 50) train loss: 1.250061; val loss: 1.447423\n",
      "(Epoch 31 / 50) train loss: 1.239124; val loss: 1.444592\n",
      "(Epoch 32 / 50) train loss: 1.229578; val loss: 1.445033\n",
      "(Epoch 33 / 50) train loss: 1.219113; val loss: 1.442501\n",
      "(Epoch 34 / 50) train loss: 1.208737; val loss: 1.436856\n",
      "(Epoch 35 / 50) train loss: 1.198540; val loss: 1.436865\n",
      "(Epoch 36 / 50) train loss: 1.189155; val loss: 1.432309\n",
      "(Epoch 37 / 50) train loss: 1.179125; val loss: 1.433429\n",
      "(Epoch 38 / 50) train loss: 1.171182; val loss: 1.430464\n",
      "(Epoch 39 / 50) train loss: 1.160843; val loss: 1.430993\n",
      "(Epoch 40 / 50) train loss: 1.151992; val loss: 1.428081\n",
      "(Epoch 41 / 50) train loss: 1.142244; val loss: 1.426955\n",
      "(Epoch 42 / 50) train loss: 1.133718; val loss: 1.425370\n",
      "(Epoch 43 / 50) train loss: 1.125047; val loss: 1.427063\n",
      "(Epoch 44 / 50) train loss: 1.117321; val loss: 1.423023\n",
      "(Epoch 45 / 50) train loss: 1.107812; val loss: 1.423580\n",
      "(Epoch 46 / 50) train loss: 1.099134; val loss: 1.421643\n",
      "(Epoch 47 / 50) train loss: 1.089618; val loss: 1.423749\n"
     ]
    }
   ],
   "source": [
    "best_model = ClassificationNet(activation=LeakyRelu(), hidden_size=205, reg=1.4402117092079504e-05)\n",
    "solver = Solver(best_model, dataloaders['train'], dataloaders['val'], \n",
    "                learning_rate=7.024967337672036e-05, loss_func=loss, optimizer=Adam)\n",
    "\n",
    "solver.train(epochs=50)\n",
    "best_model = solver.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to experiment around! The network architecture, optimizer options and activations functions, etc. are hyperparameters that you can change as well!\n",
    "\n",
    "The goal of this exercise is to get your accuracy as high as possible! You'll pass if you reach at least <b>48%</b> accuracy on our test set. There will also be a leaderboard of all students of this course. Will you make it to the top? :-)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check your validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 65.0307158119658%\n",
      "Validation Accuracy: 50.921474358974365%\n"
     ]
    }
   ],
   "source": [
    "labels, pred, acc = best_model.get_dataset_prediction(dataloaders['train'])\n",
    "print(\"Train Accuracy: {}%\".format(acc*100))\n",
    "labels, pred, acc = best_model.get_dataset_prediction(dataloaders['val'])\n",
    "print(\"Validation Accuracy: {}%\".format(acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test your model\n",
    "When you have finished your hyperparameter tuning and are sure you have your final model that performs well on the validation set (**you should at least get 48% accuracy on the validation set!**), it's time to run your  model on the test set.\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "    <h3>Important</h3>\n",
    "    <p>As you have learned in the lecture, you must only use the test set one single time! So only run the next cell if you are really sure your model works well enough and that you want to submit. Your test set is different from the test set on our server, so results may vary. Nevertheless, you will have a reasonable close approximation about your performance if you only do a final evaluation on the test set.</p>\n",
    "    <p>If you are an external student that can't use our submission webpage: this test performance is your final result and if you surpassed the threshold, you have completed this exercise :). Now, train again to aim for a better number!</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 50.54086538461539%\n"
     ]
    }
   ],
   "source": [
    "# comment this part out to see your model's performance on the test set.\n",
    "\n",
    "labels, pred, acc = best_model.get_dataset_prediction(dataloaders['test'])\n",
    "print(\"Test Accuracy: {}%\".format(acc*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The \"real\" test set is actually the dataset we're using for testing your model, which is **different** from the test set you're using here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving your Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "PicklingError",
     "evalue": "Can't pickle <class 'exercise_code.networks.classification_net.ClassificationNet'>: it's not the same object as exercise_code.networks.classification_net.ClassificationNet",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-c0d193da5490>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexercise_code\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtests\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msave_pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msave_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"cifar_fcn\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbest_model\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"cifar_fcn.p\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/TUM/WS20:21/I2DL/i2dl/exercise_06_/exercise_code/tests/eval_utils.py\u001b[0m in \u001b[0;36msave_pickle\u001b[0;34m(data_dict, file_name)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mPicklingError\u001b[0m: Can't pickle <class 'exercise_code.networks.classification_net.ClassificationNet'>: it's not the same object as exercise_code.networks.classification_net.ClassificationNet"
     ]
    }
   ],
   "source": [
    "from exercise_code.tests import save_pickle\n",
    "save_pickle({\"cifar_fcn\": best_model}, \"cifar_fcn.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exercise_code.submit import submit_exercise\n",
    "\n",
    "submit_exercise('exercise06')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission Instructions\n",
    "\n",
    "Congratulations! You've just built your first image classifier! To complete the exercise, submit your final model to our submission portal - you probably know the procedure by now.\n",
    "\n",
    "1. Go on [our submission page](https://dvl.in.tum.de/teaching/submission/), register for an account and login. We use your matriculation number and send an email with the login details to the mail account associated. When in doubt, login into tum online and check your mails there. You will get an id which we need in the next step.\n",
    "2. Execute the cell below to create a zipped folder for upload.\n",
    "3. Log into [our submission page](https://dvl.in.tum.de/teaching/submission/) with your account details and upload the `zip` file. Once successfully uploaded, you should be able to see the submitted \"cifar_fcn.p\" file selectable on the top.\n",
    "4. Click on this file and run the submission script. You will get an email with your score as well as a message if you have surpassed the threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission Goals\n",
    "\n",
    "- Goal: Implementation of activation functions and successfully implement a fully connected NN image classifier, tune hyperparameters.\n",
    "\n",
    "- Overview Tasks:\n",
    "\n",
    "    1. Implementation LeakyReLU Activation function: `forward()` and `backward()` pass\n",
    "    2. Implementation Tanh Activation function: `forward()` and `backward()` pass\n",
    "    3. Hyperparamter Tuning + Training of your own network: Reach at least **48%** accuracy on our test set\n",
    "\n",
    "\n",
    "- Passing Criteria: This time, there are no unit tests that check specific components of your code. The only thing that's required to pass the submission, is your model to reach at least **48% accuracy** on __our__ test dataset. The submission system will show you a number between 0 and 100 which corresponds to your accuracy.\n",
    "\n",
    "- Submission start: __Dec 10, 2020 13.00__\n",
    "- Submission deadline : __Dec 16, 2020 15.59__ \n",
    "- You can make **$\\infty$** submissions until the deadline. Your __best submission__ will be considered for bonus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
